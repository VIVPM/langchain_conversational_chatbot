[
    {
        "id": "7f13f50550bb3152287bcebbb85fe7a56f373baf",
        "content": "both honesty and reliability. The reliability assumption is unfounded. Several other ran-\ndomness protocols, including Algorand and Ouroboros Praos, do not depend on random\ninputs from participants at all, but instead use real-time data on blockchains and crypto-\ngraphic hash functions to generate pseudo-random numbers. This pseudo-randomness\nis not guaranteed to be uniform, even though it is standard to assume its uniformity in\nsecurity analyses. Hence, there is no guarantee that miners get elected with probabilities\nproportional to their stake.\n2\nPreliminaries\n2.1\nGames and Equilibria\nProbability Distributions. Given a finite set X = {x1, . . . , xm}, a probability distribu-\ntion on X is a function δ : X →[0, 1] such that δ(x1) + · · · + δ(xm) = 1. We denote\nthe set of all probability distributions on X by ∆(X).\nOne-shot Games [25]. A one-shot game with n players is a tuple G = (S1, S2, . . . , Sn,\nu1, u2, . . . , un) where:",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "both honesty and reliability. The reliability assumption is unfounded. Several other ran-\ndomness protocols, including Algorand and Ouroboros Praos, do not depend on random\ninputs from participants at all, but instead use real-time data on blockchains and crypto-\ngraphic hash functions to generate pseudo-random numbers. This pseudo-randomness\nis not guaranteed to be uniform, even though it is standard to assume its uniformity in\nsecurity analyses. Hence, there is no guarantee that miners get elected with probabilities\nproportional to their stake.\n2\nPreliminaries\n2.1\nGames and Equilibria\nProbability Distributions. Given a finite set X = {x1, . . . , xm}, a probability distribu-\ntion on X is a function δ : X →[0, 1] such that δ(x1) + · · · + δ(xm) = 1. We denote\nthe set of all probability distributions on X by ∆(X).\nOne-shot Games [25]. A one-shot game with n players is a tuple G = (S1, S2, . . . , Sn,\nu1, u2, . . . , un) where:"
        }
    },
    {
        "id": "9265fecaf66a6f5696f7a48c67ea84103e2cc6e5",
        "content": "tion protocols with ease. In Section 4, we design protocols to implement RIG as a\nGame-theoretic Randomness for Proof-of-Stake\n5\nrandom beacon on general proof-of-stake blockchains. We describe RIG protocols\nbased on commitment schemes and VDFs in Section 4.1 and RIG protocols based\non PVSS in Section 4.2.\n– In Section 5, we discuss how RIG can be deployed with minor changes in particular\nproof-of-stake protocols. We cover Algorand [17] and Ouroboros Praos [13].\nOur protocols are the first to incentivize participants to be reliable and submit uni-\nform random numbers. In comparison, previous distributed randomness protocols using\ncommitment schemes and PVSS assume that there is at least one reliable participant\nwithout incentivizing reliability. In other words, they only reward honesty but assume\nboth honesty and reliability. The reliability assumption is unfounded. Several other ran-\ndomness protocols, including Algorand and Ouroboros Praos, do not depend on random",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "tion protocols with ease. In Section 4, we design protocols to implement RIG as a\nGame-theoretic Randomness for Proof-of-Stake\n5\nrandom beacon on general proof-of-stake blockchains. We describe RIG protocols\nbased on commitment schemes and VDFs in Section 4.1 and RIG protocols based\non PVSS in Section 4.2.\n– In Section 5, we discuss how RIG can be deployed with minor changes in particular\nproof-of-stake protocols. We cover Algorand [17] and Ouroboros Praos [13].\nOur protocols are the first to incentivize participants to be reliable and submit uni-\nform random numbers. In comparison, previous distributed randomness protocols using\ncommitment schemes and PVSS assume that there is at least one reliable participant\nwithout incentivizing reliability. In other words, they only reward honesty but assume\nboth honesty and reliability. The reliability assumption is unfounded. Several other ran-\ndomness protocols, including Algorand and Ouroboros Praos, do not depend on random"
        }
    },
    {
        "id": "e33f1b6323f4f269cfbeca372d30d435ec34e084",
        "content": "the seed generation is synchronized among all participants for the next epoch.\nWe can substitute the random beacon of Ouroboros Praos with our RIG. This is\nfeasible because Ouroboros Praos assumes >50% honesty and suitable synchronous\nnetworks. If we use the commitment scheme approach, we can reuse the VRF selection\nrule and epoch/slot timing system. The major difference is that the execution of RIG\nconsists of two phases of communication. Therefore, it requires Treveal + Twait more\nslots within an epoch to reach a consensus on the result of RIG. Besides, we improve\nthe VRF selection using the split randomness trick to ensure uniformly random sam-\npling, instead of pseudo-random sampling. In Cardano, 1 epoch lasts for 5 days, and\nthe transaction confirmation time is 20 minutes. When using the commitment scheme,\nwe require more time (<= 3× transaction confirmation time) within an epoch for the\nextra reveal phase and VDF computation time to reach a consensus on the result of RIG,",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "the seed generation is synchronized among all participants for the next epoch.\nWe can substitute the random beacon of Ouroboros Praos with our RIG. This is\nfeasible because Ouroboros Praos assumes >50% honesty and suitable synchronous\nnetworks. If we use the commitment scheme approach, we can reuse the VRF selection\nrule and epoch/slot timing system. The major difference is that the execution of RIG\nconsists of two phases of communication. Therefore, it requires Treveal + Twait more\nslots within an epoch to reach a consensus on the result of RIG. Besides, we improve\nthe VRF selection using the split randomness trick to ensure uniformly random sam-\npling, instead of pseudo-random sampling. In Cardano, 1 epoch lasts for 5 days, and\nthe transaction confirmation time is 20 minutes. When using the commitment scheme,\nwe require more time (<= 3× transaction confirmation time) within an epoch for the\nextra reveal phase and VDF computation time to reach a consensus on the result of RIG,"
        }
    },
    {
        "id": "4c8072d00850053b3c59ac60a223fd50c91abfae",
        "content": "ness beacon is that the generated numbers are not guaranteed to be uniform.\nOuroboros and Ouroboros Praos. Ouroboros [19] was the first provably secure proof-\nof-stake blockchain protocol. It uses a publicly verifiable secret sharing scheme to gen-\nerate a fresh random seed for each epoch. However, in this scheme the participants have\nno incentive to submit a uniform random value. In other words, there is no incentive\nto be reliable, but just to be honest. Ouroboros Praos [13] improves over Ouroboros to\nbe provably secure under a semi-synchronous setting. The random seed of Ouroboros\nPraos is updated every epoch by applying a random oracle hash function to a concate-\nnation of VRF outputs in the previous epoch. Similar to Algorand, the random numbers\nare not guaranteed to be uniformly random, despite the fact that they are assumed to be\nuniform in the security analysis.\nOur Contribution. Our main contributions are as follows:",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "ness beacon is that the generated numbers are not guaranteed to be uniform.\nOuroboros and Ouroboros Praos. Ouroboros [19] was the first provably secure proof-\nof-stake blockchain protocol. It uses a publicly verifiable secret sharing scheme to gen-\nerate a fresh random seed for each epoch. However, in this scheme the participants have\nno incentive to submit a uniform random value. In other words, there is no incentive\nto be reliable, but just to be honest. Ouroboros Praos [13] improves over Ouroboros to\nbe provably secure under a semi-synchronous setting. The random seed of Ouroboros\nPraos is updated every epoch by applying a random oracle hash function to a concate-\nnation of VRF outputs in the previous epoch. Similar to Algorand, the random numbers\nare not guaranteed to be uniformly random, despite the fact that they are assumed to be\nuniform in the security analysis.\nOur Contribution. Our main contributions are as follows:"
        }
    },
    {
        "id": "f20d21c9c522f892b3c939a458a4eec37250a122",
        "content": "we require more time (<= 3× transaction confirmation time) within an epoch for the\nextra reveal phase and VDF computation time to reach a consensus on the result of RIG,\nwhich is negligible.\n5.2\nRIG in Algorand\nIt is also feasible to use RIG in Algorand [17]. Algorand executes a byzantine agreement\nprotocol to achieve no-fork and one-minute-liveness properties, which guarantee a one-\nminute-synchronous blockchain for RIG random beacon. Algorand assumes that honest\nusers hold more than two-thirds of the total stake, which satisfies the requirement of the\nPVSS implementation of RIG.\nGame-theoretic Randomness for Proof-of-Stake\n17\nAlgorand requires an evolving random seed for VRF-based selection of miners and\nByzantine agreement protocols. The random seed is updated every R rounds by apply-\ning the VRF of the current miner to the previous random seed and the current epoch\nnumber. This is again pseudo-random and the output might deviate from uniform distri-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "we require more time (<= 3× transaction confirmation time) within an epoch for the\nextra reveal phase and VDF computation time to reach a consensus on the result of RIG,\nwhich is negligible.\n5.2\nRIG in Algorand\nIt is also feasible to use RIG in Algorand [17]. Algorand executes a byzantine agreement\nprotocol to achieve no-fork and one-minute-liveness properties, which guarantee a one-\nminute-synchronous blockchain for RIG random beacon. Algorand assumes that honest\nusers hold more than two-thirds of the total stake, which satisfies the requirement of the\nPVSS implementation of RIG.\nGame-theoretic Randomness for Proof-of-Stake\n17\nAlgorand requires an evolving random seed for VRF-based selection of miners and\nByzantine agreement protocols. The random seed is updated every R rounds by apply-\ning the VRF of the current miner to the previous random seed and the current epoch\nnumber. This is again pseudo-random and the output might deviate from uniform distri-"
        }
    },
    {
        "id": "f73705264175475b7232bf383fb242826b61d3d2",
        "content": "When we require the participants to use blockchain transactions for communication, c\nshould at least cover the transaction fees. The deposit amount d should also be larger\nthan any reward that a participant can possibly obtain in the game in order to discourage\ndishonest behavior.\n4.4\nAssumptions and Limits to Applicability\nNetwork Assumptions. The most important assumptions are the network assumptions.\nOur RIG game relies on a synchronous communication network. All real-world blockchain\nnetworks use the internet and are effectively synchronous.\nδ-synchrony. A broadcast network is δ-synchronous if a message broadcasted by some\nuser at time t will be received by all other users by time t + δ.\nWhen applied to blockchains, blockchain consensus protocols guarantee public ledgers\nwith different levels of synchrony. In this paper, we rely on blockchain consensus\nprotocols to achieve a synchronized view of RIG execution. In detail, we require ∆-\nsynchrony for blockchains:",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "When we require the participants to use blockchain transactions for communication, c\nshould at least cover the transaction fees. The deposit amount d should also be larger\nthan any reward that a participant can possibly obtain in the game in order to discourage\ndishonest behavior.\n4.4\nAssumptions and Limits to Applicability\nNetwork Assumptions. The most important assumptions are the network assumptions.\nOur RIG game relies on a synchronous communication network. All real-world blockchain\nnetworks use the internet and are effectively synchronous.\nδ-synchrony. A broadcast network is δ-synchronous if a message broadcasted by some\nuser at time t will be received by all other users by time t + δ.\nWhen applied to blockchains, blockchain consensus protocols guarantee public ledgers\nwith different levels of synchrony. In this paper, we rely on blockchain consensus\nprotocols to achieve a synchronized view of RIG execution. In detail, we require ∆-\nsynchrony for blockchains:"
        }
    },
    {
        "id": "d56b1d53e27c7a307615d828241f6a5ae1c35971",
        "content": "proofs can be verified by any party, including third parties who are not taking part in\nthe PVSS scheme.\nRANDAO [1] and EVR [30]. RANDAO is a family of smart contracts that produce\nrandom numbers. Anyone can participate and submit a random value to contribute to\nthe output. RANDAO uses a commitment scheme. Compared to general distributed\nrandomness protocols based on distributed networks, RANDAO’s smart contracts run\non a blockchain with consensus and directly interact with the underlying cryptocur-\nrency. Therefore, RANDAO naturally enjoys the decentralized consensus provided by\nthe blockchain protocol. Besides, economic incentives can be designed to promote hon-\nesty. Cheaters who violate the rules are punished economically, e.g. by having their\ndeposits confiscated. On the other hand, honest participants are rewarded by the income\ngenerated from providing the random number generation service to external contracts.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "proofs can be verified by any party, including third parties who are not taking part in\nthe PVSS scheme.\nRANDAO [1] and EVR [30]. RANDAO is a family of smart contracts that produce\nrandom numbers. Anyone can participate and submit a random value to contribute to\nthe output. RANDAO uses a commitment scheme. Compared to general distributed\nrandomness protocols based on distributed networks, RANDAO’s smart contracts run\non a blockchain with consensus and directly interact with the underlying cryptocur-\nrency. Therefore, RANDAO naturally enjoys the decentralized consensus provided by\nthe blockchain protocol. Besides, economic incentives can be designed to promote hon-\nesty. Cheaters who violate the rules are punished economically, e.g. by having their\ndeposits confiscated. On the other hand, honest participants are rewarded by the income\ngenerated from providing the random number generation service to external contracts."
        }
    },
    {
        "id": "32981dcd6f3cbe42048e20febcc50151244f2e92",
        "content": "first place.\n4\nDesigning a Random Beacon Based on RIG\nIn this section, we discuss how to use a single execution of the Random Integer Gen-\neration game in a distributed random number generation beacon. The major challenge\nis to execute the game, in which the parties have to move simultaneously, in a decen-\ntralized environment. We propose two schemes to implement the RIG game: (1) using\ncommitment schemes and verifiable delay functions, and (2) using publicly verifiable\nsecret sharing. We assume that the set of players is already fixed. Usually, only a small\nsubset of users (or blockchain miners) are selected from all the users in the system to\njoin a single execution of the game (generation of a random number). The selection rule\nis determined by the specific application. The design and amount of reward/penalty and\ndeposits are also subject to the specific application. We will address these adjustable",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "first place.\n4\nDesigning a Random Beacon Based on RIG\nIn this section, we discuss how to use a single execution of the Random Integer Gen-\neration game in a distributed random number generation beacon. The major challenge\nis to execute the game, in which the parties have to move simultaneously, in a decen-\ntralized environment. We propose two schemes to implement the RIG game: (1) using\ncommitment schemes and verifiable delay functions, and (2) using publicly verifiable\nsecret sharing. We assume that the set of players is already fixed. Usually, only a small\nsubset of users (or blockchain miners) are selected from all the users in the system to\njoin a single execution of the game (generation of a random number). The selection rule\nis determined by the specific application. The design and amount of reward/penalty and\ndeposits are also subject to the specific application. We will address these adjustable"
        }
    },
    {
        "id": "7f1cafcb7b028c4dc820a3aa63627d37502504a4",
        "content": "reliable, but this is already incentivized by our game.\n4.2\nPVSS approach\nThe drawback of the commitment scheme in random number generators is the possi-\nbility of adversarial manipulation in the reveal phase by not revealing values. We have\nalready seen a solution using VDFs. A publicly verifiable secret sharing scheme solves\nthe same issue differently, i.e. by forcing the opening of commitments, at the cost of\nincreased communication complexity. We follow the PVSS scheme in [27].\nAn execution of the RIG game in a PVSS scheme consists of three phases: prepare,\ndistribute and reconstruct.\nIn the prepare phase, all eligible participants inform each other that they are se-\nlected. Under a synchronous communication setting, all honest participants can reach\na consensus on the list of participants. More specifically, we assume a blockchain pro-\ntocol that a transaction will be added to a finalized block within known bounded time",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "reliable, but this is already incentivized by our game.\n4.2\nPVSS approach\nThe drawback of the commitment scheme in random number generators is the possi-\nbility of adversarial manipulation in the reveal phase by not revealing values. We have\nalready seen a solution using VDFs. A publicly verifiable secret sharing scheme solves\nthe same issue differently, i.e. by forcing the opening of commitments, at the cost of\nincreased communication complexity. We follow the PVSS scheme in [27].\nAn execution of the RIG game in a PVSS scheme consists of three phases: prepare,\ndistribute and reconstruct.\nIn the prepare phase, all eligible participants inform each other that they are se-\nlected. Under a synchronous communication setting, all honest participants can reach\na consensus on the list of participants. More specifically, we assume a blockchain pro-\ntocol that a transaction will be added to a finalized block within known bounded time"
        }
    },
    {
        "id": "3973e26a0695d9ec54acb6b6bc064e02454a7437",
        "content": "reveal scheme to generate randomness. It designs a punishment scheme to discourage\ndeviating from the protocol. However, similar to RANDAO, the incentives of EVR only\ncare about whether the values are revealed faithfully. They do not differentiate between\na reliable participant who submits a fresh uniformly-sampled random number and an\nunreliable honest participant who submits a constant number each time while following\nthe rest of the protocol.\nVDFs. Verifiable delay functions [6] can be used to ensure bias-resistance in distributed\nrandomness protocols. A VDF is a function whose evaluation takes at least some pre-\ndetermined number of sequential steps, even with many parallel processors. Once the\nevaluation is complete, it can provide a publicly verifiable proof for the evaluation re-\nsult, which can also be checked by any third party efficiently.\nVRFs. Verifiable random functions [22, 14] are widely used in PoS blockchain proto-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "reveal scheme to generate randomness. It designs a punishment scheme to discourage\ndeviating from the protocol. However, similar to RANDAO, the incentives of EVR only\ncare about whether the values are revealed faithfully. They do not differentiate between\na reliable participant who submits a fresh uniformly-sampled random number and an\nunreliable honest participant who submits a constant number each time while following\nthe rest of the protocol.\nVDFs. Verifiable delay functions [6] can be used to ensure bias-resistance in distributed\nrandomness protocols. A VDF is a function whose evaluation takes at least some pre-\ndetermined number of sequential steps, even with many parallel processors. Once the\nevaluation is complete, it can provide a publicly verifiable proof for the evaluation re-\nsult, which can also be checked by any third party efficiently.\nVRFs. Verifiable random functions [22, 14] are widely used in PoS blockchain proto-"
        }
    },
    {
        "id": "61cf8a72fab3f5fb8a76a7b3c1dcdbe1b191d379",
        "content": "the values are submitted independently. Therefore, the distributed randomness protocols\nGame-theoretic Randomness for Proof-of-Stake\n3\ntypically assume that at least one of the participants is reliable. We distinguish between\nreliable and honest participants.\nHonest Participants. An honest participant is a participant who correctly follows the\nprotocol, e.g. submits their random number si in time. Distributed randomness proto-\ncols often assume and require that a large proportion of participants are honest and obey\nthe communication rules to complete and produce final values. For example, PBFT\nachieves Byzantine agreement in a partially-synchronous network by requiring that\nmore than two thirds of all participants be honest [8].\nCommitment Schemes. Using the formula above for random number generation, since\nthe participants cannot broadcast their values in a distributed network in a perfectly si-\nmultaneous way, the last participant has an advantage and can dominate the final output.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "the values are submitted independently. Therefore, the distributed randomness protocols\nGame-theoretic Randomness for Proof-of-Stake\n3\ntypically assume that at least one of the participants is reliable. We distinguish between\nreliable and honest participants.\nHonest Participants. An honest participant is a participant who correctly follows the\nprotocol, e.g. submits their random number si in time. Distributed randomness proto-\ncols often assume and require that a large proportion of participants are honest and obey\nthe communication rules to complete and produce final values. For example, PBFT\nachieves Byzantine agreement in a partially-synchronous network by requiring that\nmore than two thirds of all participants be honest [8].\nCommitment Schemes. Using the formula above for random number generation, since\nthe participants cannot broadcast their values in a distributed network in a perfectly si-\nmultaneous way, the last participant has an advantage and can dominate the final output."
        }
    },
    {
        "id": "e0c710b28b3618e556d56f9b8c63a565cf74c197",
        "content": "general, the RIG random beacon, be it implemented by the commitment scheme ap-\nproach or the PVSS approach, is applicable to any PoS protocol that requires an evolv-\ning random seed to select miners. Overall, using our RIG as the source of randomness\nonly introduces negligible overhead in terms of transaction throughput.\nIf we use the RIG random beacon for generating the random seed in a proof-of-stake\nprotocol, a single execution of the RIG game updates the random seed once. Usually,\na single execution spans an epoch, which consists of multiple slots where the same\nrandom seed is repeatedly used. The blockchain protocol is modified to consecutively\nrun the RIG random beacon to update the random seed in every epoch. The participants\nof RIG random beacon of each epoch are randomly selected, e.g. based on the RIG\nresult of the previous epoch. Note that our approach can also be applied for every block,\ninstead of every epoch, but this would require more communication complexity.\n5.1",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "general, the RIG random beacon, be it implemented by the commitment scheme ap-\nproach or the PVSS approach, is applicable to any PoS protocol that requires an evolv-\ning random seed to select miners. Overall, using our RIG as the source of randomness\nonly introduces negligible overhead in terms of transaction throughput.\nIf we use the RIG random beacon for generating the random seed in a proof-of-stake\nprotocol, a single execution of the RIG game updates the random seed once. Usually,\na single execution spans an epoch, which consists of multiple slots where the same\nrandom seed is repeatedly used. The blockchain protocol is modified to consecutively\nrun the RIG random beacon to update the random seed in every epoch. The participants\nof RIG random beacon of each epoch are randomly selected, e.g. based on the RIG\nresult of the previous epoch. Note that our approach can also be applied for every block,\ninstead of every epoch, but this would require more communication complexity.\n5.1"
        }
    },
    {
        "id": "175cb0ed3982bcc92bfc8895b6ca27d91ff4b21a",
        "content": "ing the VRF of the current miner to the previous random seed and the current epoch\nnumber. This is again pseudo-random and the output might deviate from uniform distri-\nbution. We can use our RIG random beacon to generate the random seeds for Algorand.\nIf we use on-chain communication in the PVSS approach, then we have to select the\nparticipants of RIG separately from the committee for each single round, because we\nwant the participants of RIG to be active for multiple slots (Tprepare + Tdistribute +\nTreconstruct slots) without obstructing the growth of blockchain. Each single execu-\ntion of the RIG spans the duration of an epoch, which consists of R rounds. Algo-\nrand reaches consensus within 1 round, and updates the random seed once every 1000\nrounds, which is sufficient for a PVSS execution. Moreover, assuming that R = 1000\nand 100 participants join the RIG game, using RIG decreases the transaction-per-second\nby less than 1%.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "ing the VRF of the current miner to the previous random seed and the current epoch\nnumber. This is again pseudo-random and the output might deviate from uniform distri-\nbution. We can use our RIG random beacon to generate the random seeds for Algorand.\nIf we use on-chain communication in the PVSS approach, then we have to select the\nparticipants of RIG separately from the committee for each single round, because we\nwant the participants of RIG to be active for multiple slots (Tprepare + Tdistribute +\nTreconstruct slots) without obstructing the growth of blockchain. Each single execu-\ntion of the RIG spans the duration of an epoch, which consists of R rounds. Algo-\nrand reaches consensus within 1 round, and updates the random seed once every 1000\nrounds, which is sufficient for a PVSS execution. Moreover, assuming that R = 1000\nand 100 participants join the RIG game, using RIG decreases the transaction-per-second\nby less than 1%."
        }
    },
    {
        "id": "17ef9b99f11a07be0170ce250289f79f62ecb414",
        "content": "result of the previous epoch. Note that our approach can also be applied for every block,\ninstead of every epoch, but this would require more communication complexity.\n5.1\nRIG in Ouroboros Praos\nOuroboros Praos is the second proof-of-stake protocol in the Ouroboros family and\nthe underlying protocol of Cardano cryptocurrency [13]. The selection rule for random\nseed generation participants in Ouroboros Praos is based on a VRF. The random beacon\nconcatenates the VRF output of the participants and applies a random oracle hash func-\ntion on the concatenated output. Each participant is also a miner and announces their\nVRF output along with their new block. The generated random seed is used in the next\nepoch, which consists of a number of slots. The protocol waits for enough slots until\nthe seed generation is synchronized among all participants for the next epoch.\nWe can substitute the random beacon of Ouroboros Praos with our RIG. This is",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "result of the previous epoch. Note that our approach can also be applied for every block,\ninstead of every epoch, but this would require more communication complexity.\n5.1\nRIG in Ouroboros Praos\nOuroboros Praos is the second proof-of-stake protocol in the Ouroboros family and\nthe underlying protocol of Cardano cryptocurrency [13]. The selection rule for random\nseed generation participants in Ouroboros Praos is based on a VRF. The random beacon\nconcatenates the VRF output of the participants and applies a random oracle hash func-\ntion on the concatenated output. Each participant is also a miner and announces their\nVRF output along with their new block. The generated random seed is used in the next\nepoch, which consists of a number of slots. The protocol waits for enough slots until\nthe seed generation is synchronized among all participants for the next epoch.\nWe can substitute the random beacon of Ouroboros Praos with our RIG. This is"
        }
    },
    {
        "id": "5296347e53f3888d40dd7f76d13a6f6856284aa4",
        "content": "The theorem above shows that it is in every player’s best interest to play uniformly\nat random, i.e. choose each pure strategy in Si with probability exactly 1/m. Moreover,\nthis equilibrium is self-enforcing even in the presence of alliances. Hence, we can plug\nthis game into a distributed random number generation protocol and give participants\nrewards that are based on their payoffs in this game. This ensures that every participant\nis incentivized to provide a uniformly random si. As mentioned before, even if one\nparticipant is reliable and submits a uniformly random si, then the entire result v =\nP si of the random number generation protocol is guaranteed to be unbiased. Hence,\ninstead of assuming that a reliable party exists, we incentivize every party to be reliable.\n3.3\nDense RIG bimatrix game\nIf the strategy size m is large, which is the case when we aim to generate integers from\na large range, then the matrix A would be sparse. If the number of players is much",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "The theorem above shows that it is in every player’s best interest to play uniformly\nat random, i.e. choose each pure strategy in Si with probability exactly 1/m. Moreover,\nthis equilibrium is self-enforcing even in the presence of alliances. Hence, we can plug\nthis game into a distributed random number generation protocol and give participants\nrewards that are based on their payoffs in this game. This ensures that every participant\nis incentivized to provide a uniformly random si. As mentioned before, even if one\nparticipant is reliable and submits a uniformly random si, then the entire result v =\nP si of the random number generation protocol is guaranteed to be unbiased. Hence,\ninstead of assuming that a reliable party exists, we incentivize every party to be reliable.\n3.3\nDense RIG bimatrix game\nIf the strategy size m is large, which is the case when we aim to generate integers from\na large range, then the matrix A would be sparse. If the number of players is much"
        }
    },
    {
        "id": "cf0a72e14fdce266460c243cafb8676ea44e5bf6",
        "content": "rounds, which is sufficient for a PVSS execution. Moreover, assuming that R = 1000\nand 100 participants join the RIG game, using RIG decreases the transaction-per-second\nby less than 1%.\nThe execution of the RIG random beacon is parallel to other parts of Algorand.\nWhile other parts require an evolving random seed from the RIG random beacon, they\ncan reuse the previous random seed until a new seed is computed and synchronized.\nCompared to the random seed updating procedure in Algorand, our RIG random beacon\nis bias-resistant and rules out the possibility of adversarial manipulation on the random\nseed assuming the selected participants satisfy the assumption of honest majority.\n6\nConclusion\nWe presented a game-theoretic beacon for distributed random number generation. We\nshowed that our approach is bias-resistant, unpredictable, available, verifiable and in-\ncentivizes every participant to be reliable, i.e. faithfully provide a uniform random input.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "rounds, which is sufficient for a PVSS execution. Moreover, assuming that R = 1000\nand 100 participants join the RIG game, using RIG decreases the transaction-per-second\nby less than 1%.\nThe execution of the RIG random beacon is parallel to other parts of Algorand.\nWhile other parts require an evolving random seed from the RIG random beacon, they\ncan reuse the previous random seed until a new seed is computed and synchronized.\nCompared to the random seed updating procedure in Algorand, our RIG random beacon\nis bias-resistant and rules out the possibility of adversarial manipulation on the random\nseed assuming the selected participants satisfy the assumption of honest majority.\n6\nConclusion\nWe presented a game-theoretic beacon for distributed random number generation. We\nshowed that our approach is bias-resistant, unpredictable, available, verifiable and in-\ncentivizes every participant to be reliable, i.e. faithfully provide a uniform random input."
        }
    },
    {
        "id": "12e5ad79b4a96ff40f4d81cd1f6f3f84b78fae97",
        "content": "In the reveal phase, each participant pi broadcasts a signed reveal message: (ss_id, si, ri)i.\nA reveal message is valid if (1) the message is received during the reveal phase, (2) pi\nhas exactly one valid commit message, and (3) hash(si|ri) matches the commitment\nhi of the participant pi.\nAfter the reveal phase completes, we can compute the payoffs of the RIG game.\nWe describe in subsection 4.3 the details of computing results. Assume the outcome of\nthe game is (s1, . . . , sn), where si ∈{0, 1, . . . , m −1} is the strategy played by each\nplayer. We set v := P si (mod m) and output it as the result of the random number\ngeneration protocol.\nThe value of v can be biased by malicious participants who might choose not to\nreveal their values/strategies. If a participant does not reveal the values after completing\nthe commit phase correctly, they will lose their deposit. However, the participant might\nbenefit from a biased output of the random beacon in the downstream application, for",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "In the reveal phase, each participant pi broadcasts a signed reveal message: (ss_id, si, ri)i.\nA reveal message is valid if (1) the message is received during the reveal phase, (2) pi\nhas exactly one valid commit message, and (3) hash(si|ri) matches the commitment\nhi of the participant pi.\nAfter the reveal phase completes, we can compute the payoffs of the RIG game.\nWe describe in subsection 4.3 the details of computing results. Assume the outcome of\nthe game is (s1, . . . , sn), where si ∈{0, 1, . . . , m −1} is the strategy played by each\nplayer. We set v := P si (mod m) and output it as the result of the random number\ngeneration protocol.\nThe value of v can be biased by malicious participants who might choose not to\nreveal their values/strategies. If a participant does not reveal the values after completing\nthe commit phase correctly, they will lose their deposit. However, the participant might\nbenefit from a biased output of the random beacon in the downstream application, for"
        }
    },
    {
        "id": "2ea38d2789220cd9b92a9cce8a1503e87fe76788",
        "content": "threshold secret shares to others along with a proof of commitment and consistency.\nThe shares are publicly verifiable so that a dishonest participant who distributes invalid\nshares can be discovered and excluded from the game. Hence, by the end of distribute\nphase, all honest participants release their correct shares and receive correct shares from\nother honest participants. If a dishonest participant distributes some invalid shares or\ndoes not distribute part of the shares, they will be reported and deleted from the list of\nparticipants. As long as the number of dishonest participants is less than t, they cannot\ndecrypt any secret from honest participants in the distribute phase.\nIn the reconstruct phase, each participant can reveal their value and share the de-\ncrypted secret shares they received. If the number of honest participants is at least t,\nthen the pooling mechanism is successful and anyone can find all the secrets from valid\n14\nZ. Cai and A. Goharshady",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "threshold secret shares to others along with a proof of commitment and consistency.\nThe shares are publicly verifiable so that a dishonest participant who distributes invalid\nshares can be discovered and excluded from the game. Hence, by the end of distribute\nphase, all honest participants release their correct shares and receive correct shares from\nother honest participants. If a dishonest participant distributes some invalid shares or\ndoes not distribute part of the shares, they will be reported and deleted from the list of\nparticipants. As long as the number of dishonest participants is less than t, they cannot\ndecrypt any secret from honest participants in the distribute phase.\nIn the reconstruct phase, each participant can reveal their value and share the de-\ncrypted secret shares they received. If the number of honest participants is at least t,\nthen the pooling mechanism is successful and anyone can find all the secrets from valid\n14\nZ. Cai and A. Goharshady"
        }
    },
    {
        "id": "3951b3d5bdfedd51f49e2fc1a443373a624094cd",
        "content": "person with a large stake is likely to control multiple players in the randomly selected\ncommittee and only care about the overall revenue.\n2.2\nPublicly-Verifiable Secret Sharing\nWe follow [27] in our description of PVSS. In a PVSS scheme, a dealer D wants to\nshare a secret s with a group of n participants P1, P2, . . . , Pn. The goal is to have a\n(t, n)-threshold scheme, i.e. any subset of t participants can collaborate to recover the\nsecret s, while any smaller subset of participants cannot recover the secret or obtain\nany information about it. Moreover, anyone on the network, even those who are not\nparticipating, should be able to verify that the dealer is acting honestly and following\nthe protocol.\nInitialization. We assume that a multiplicative group Z∗\nq and two generators g, G of\nthis group are selected using an appropriate public procedure. Here, q is a large prime\nnumber and all calculations are done modulo q. Each participant Pi generates a non-\nzero private key xi ∈Z∗",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "person with a large stake is likely to control multiple players in the randomly selected\ncommittee and only care about the overall revenue.\n2.2\nPublicly-Verifiable Secret Sharing\nWe follow [27] in our description of PVSS. In a PVSS scheme, a dealer D wants to\nshare a secret s with a group of n participants P1, P2, . . . , Pn. The goal is to have a\n(t, n)-threshold scheme, i.e. any subset of t participants can collaborate to recover the\nsecret s, while any smaller subset of participants cannot recover the secret or obtain\nany information about it. Moreover, anyone on the network, even those who are not\nparticipating, should be able to verify that the dealer is acting honestly and following\nthe protocol.\nInitialization. We assume that a multiplicative group Z∗\nq and two generators g, G of\nthis group are selected using an appropriate public procedure. Here, q is a large prime\nnumber and all calculations are done modulo q. Each participant Pi generates a non-\nzero private key xi ∈Z∗"
        }
    },
    {
        "id": "5dcc6d7ddc9d3d058d20833ccb78051ee1a923dd",
        "content": "Computation and Communication Complexity. Our RIG can be implemented using a\ncommit-reveal scheme or a PVSS scheme, which is typical in random number genera-\ntion protocols and other components of distributed protocols. Depending on the assump-\ntions such as availability of reliable communication channels, the complexity might be\ndifferent. Overall, we claim that the computation and communication complexity of\nRIG is better or comparable with existing efficient random number generation proto-\ncols and imposes negligible overhead in applications under reasonable assumptions, as\nexemplified in Section 5.\n16\nZ. Cai and A. Goharshady\n5\nRIG in Proof of Stake Protocols\nWe now show how our RIG random beacon can supplant standard PoS protocols. In\ngeneral, the RIG random beacon, be it implemented by the commitment scheme ap-\nproach or the PVSS approach, is applicable to any PoS protocol that requires an evolv-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "Computation and Communication Complexity. Our RIG can be implemented using a\ncommit-reveal scheme or a PVSS scheme, which is typical in random number genera-\ntion protocols and other components of distributed protocols. Depending on the assump-\ntions such as availability of reliable communication channels, the complexity might be\ndifferent. Overall, we claim that the computation and communication complexity of\nRIG is better or comparable with existing efficient random number generation proto-\ncols and imposes negligible overhead in applications under reasonable assumptions, as\nexemplified in Section 5.\n16\nZ. Cai and A. Goharshady\n5\nRIG in Proof of Stake Protocols\nWe now show how our RIG random beacon can supplant standard PoS protocols. In\ngeneral, the RIG random beacon, be it implemented by the commitment scheme ap-\nproach or the PVSS approach, is applicable to any PoS protocol that requires an evolv-"
        }
    },
    {
        "id": "ae7fcc4c90bbe9de7ea42fa33779948ed77ec0ff",
        "content": "(2021)\n19. Kiayias, A., Russell, A., David, B., Oliynykov, R.: Ouroboros: A provably secure proof-of-\nstake blockchain protocol. In: CRYPTO. pp. 357–388 (2017)\n20. Krasnoselskii, M., Melnikov, G., Yanovich, Y.: No-dealer: Byzantine fault-tolerant random\nnumber generator. In: INFOCOM. pp. 568–573 (2020)\n21. Meybodi, M.A., Goharshady, A.K., Hooshmandasl, M.R., Shakiba, A.: Optimal mining:\nMaximizing Bitcoin miners’ revenues from transaction fees. In: IEEE Blockchain. pp. 266–\n273 (2022)\n22. Micali, S., Rabin, M.O., Vadhan, S.P.: Verifiable random functions. In: FOCS. pp. 120–130\n(1999)\n23. Nakamoto, S.: Bitcoin: A peer-to-peer electronic cash system. Decentralized Business Re-\nview p. 21260 (2008)\n24. Nash, J.: Non-cooperative games. Annals of Mathematics pp. 286–95 (1951)\n25. Roughgarden, T., Nisan, N.: Algorithmic Game Theory. Cambridge University Press (2007)\n26. Schindler, P., Judmayer, A., Stifter, N., Weippl, E.R.: Hydrand: Efficient continuous dis-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "(2021)\n19. Kiayias, A., Russell, A., David, B., Oliynykov, R.: Ouroboros: A provably secure proof-of-\nstake blockchain protocol. In: CRYPTO. pp. 357–388 (2017)\n20. Krasnoselskii, M., Melnikov, G., Yanovich, Y.: No-dealer: Byzantine fault-tolerant random\nnumber generator. In: INFOCOM. pp. 568–573 (2020)\n21. Meybodi, M.A., Goharshady, A.K., Hooshmandasl, M.R., Shakiba, A.: Optimal mining:\nMaximizing Bitcoin miners’ revenues from transaction fees. In: IEEE Blockchain. pp. 266–\n273 (2022)\n22. Micali, S., Rabin, M.O., Vadhan, S.P.: Verifiable random functions. In: FOCS. pp. 120–130\n(1999)\n23. Nakamoto, S.: Bitcoin: A peer-to-peer electronic cash system. Decentralized Business Re-\nview p. 21260 (2008)\n24. Nash, J.: Non-cooperative games. Annals of Mathematics pp. 286–95 (1951)\n25. Roughgarden, T., Nisan, N.: Algorithmic Game Theory. Cambridge University Press (2007)\n26. Schindler, P., Judmayer, A., Stifter, N., Weippl, E.R.: Hydrand: Efficient continuous dis-"
        }
    },
    {
        "id": "35525143014c84bca34435d0b5f84d99fe119154",
        "content": "– Verification of the shares. Anyone on the network, be it a player Pi or a non-\nparticipant third-party, can verify the proof and encrypted shares provided by the\ndealer to ensure that the dealer is acting honestly, i.e. following the protocol above,\nand not giving out invalid shares.\nReconstruction. This step consists of:\n– Decryption of the shares. Each party Pi knows Yi = yp(i)\ni\nand their secret key\nxi. Recall that yi = Gxi. Hence, the i-th party can compute Y 1/xi\ni\n= yp(i)/xi\ni\n=\nGp(i). They publish Gpi along with a non-interactive zero-knowledge proof of its\ncorrectness.\n– Pooling the shares. Any t participants Pi1, Pi2, . . . , Pit can compute the Gr by La-\ngrange interpolation. More specifically, they know t points (ij, p(ij)) of the poly-\nnomial p that is of degree t −1. So, they can find the unique polynomial that goes\nthrough these points. Note that after all the shares are decrypted, anyone on the",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "– Verification of the shares. Anyone on the network, be it a player Pi or a non-\nparticipant third-party, can verify the proof and encrypted shares provided by the\ndealer to ensure that the dealer is acting honestly, i.e. following the protocol above,\nand not giving out invalid shares.\nReconstruction. This step consists of:\n– Decryption of the shares. Each party Pi knows Yi = yp(i)\ni\nand their secret key\nxi. Recall that yi = Gxi. Hence, the i-th party can compute Y 1/xi\ni\n= yp(i)/xi\ni\n=\nGp(i). They publish Gpi along with a non-interactive zero-knowledge proof of its\ncorrectness.\n– Pooling the shares. Any t participants Pi1, Pi2, . . . , Pit can compute the Gr by La-\ngrange interpolation. More specifically, they know t points (ij, p(ij)) of the poly-\nnomial p that is of degree t −1. So, they can find the unique polynomial that goes\nthrough these points. Note that after all the shares are decrypted, anyone on the"
        }
    },
    {
        "id": "8c5236598e8c4cb5ec847264472ef8d0318f3569",
        "content": "a consensus on the list of participants. More specifically, we assume a blockchain pro-\ntocol that a transaction will be added to a finalized block within known bounded time\nslots after it is broadcasted. Each participant firstly broadcasts a signed prepare message\n(ss_id, proofi)i to announce its identity along with eligibility proof for the current ses-\nsion of execution. By the end of prepare phase, all prepare messages should be included\nin the blockchain and are synchronized across all nodes. Suppose the list of participants\nis {Pi}n\ni=1.\nIn the distribute and reconstruct phases, each participant Pi runs a PVSS scheme\nto share their value si to the other n −1 participants. This is exactly as described\nin Section 2.2. In the distribute phase, every participant should send valid (n −1, t)-\nthreshold secret shares to others along with a proof of commitment and consistency.\nThe shares are publicly verifiable so that a dishonest participant who distributes invalid",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "a consensus on the list of participants. More specifically, we assume a blockchain pro-\ntocol that a transaction will be added to a finalized block within known bounded time\nslots after it is broadcasted. Each participant firstly broadcasts a signed prepare message\n(ss_id, proofi)i to announce its identity along with eligibility proof for the current ses-\nsion of execution. By the end of prepare phase, all prepare messages should be included\nin the blockchain and are synchronized across all nodes. Suppose the list of participants\nis {Pi}n\ni=1.\nIn the distribute and reconstruct phases, each participant Pi runs a PVSS scheme\nto share their value si to the other n −1 participants. This is exactly as described\nin Section 2.2. In the distribute phase, every participant should send valid (n −1, t)-\nthreshold secret shares to others along with a proof of commitment and consistency.\nThe shares are publicly verifiable so that a dishonest participant who distributes invalid"
        }
    },
    {
        "id": "346f7409cc20311320d662546ff9a358d58ecaaa",
        "content": "for each player. The output random number is computed as s = Pn\ni=1 si (mod m).\nNote that all dishonest players are excluded from the sum. If a player does not release\ntheir value or otherwise cheats, then they will be punished by confiscating their deposit\nand they will not be included in the game. Each honest participant Pi receives a payoff\nof the form rwi = ui(s1, . . . , sn) + c. Recall that ui(s1, . . . , sn) is the payoff of Pi\nGame-theoretic Randomness for Proof-of-Stake\n15\ndefined by the game matrix, which sums up to 0 among valid participants. The number\nc is a constant defined by the specific application. Generally, c should be positive and\nhigh enough to motivate honest participants to join the RIG game and perform its steps.\nWhen we require the participants to use blockchain transactions for communication, c\nshould at least cover the transaction fees. The deposit amount d should also be larger",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "for each player. The output random number is computed as s = Pn\ni=1 si (mod m).\nNote that all dishonest players are excluded from the sum. If a player does not release\ntheir value or otherwise cheats, then they will be punished by confiscating their deposit\nand they will not be included in the game. Each honest participant Pi receives a payoff\nof the form rwi = ui(s1, . . . , sn) + c. Recall that ui(s1, . . . , sn) is the payoff of Pi\nGame-theoretic Randomness for Proof-of-Stake\n15\ndefined by the game matrix, which sums up to 0 among valid participants. The number\nc is a constant defined by the specific application. Generally, c should be positive and\nhigh enough to motivate honest participants to join the RIG game and perform its steps.\nWhen we require the participants to use blockchain transactions for communication, c\nshould at least cover the transaction fees. The deposit amount d should also be larger"
        }
    },
    {
        "id": "e8bdefe6f396696091584c07772e261ae3bbf689",
        "content": "Algorand, have two fundamental limitations: Either (i) they rely on pseudoran-\ndomness, e.g. assuming that the output of a hash function is uniform, which is\na widely used but unproven assumption, or (ii) they generate their randomness\nusing a distributed protocol in which several participants are required to submit\nrandom numbers which are then used in the generation of a final random result.\nHowever, in this case, there is no guarantee that the numbers provided by the\nparties are uniformly random and there is no incentive for the parties to honestly\ngenerate uniform randomness. Most random beacons have both limitations.\nIn this work, we provide a protocol for distributed generation of randomness.\nOur protocol does not rely on pseudorandomness at all. Similar to some of the\nprevious approaches, it uses random inputs by different participants to generate\na final random result. However, the crucial difference is that we provide a game-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "Algorand, have two fundamental limitations: Either (i) they rely on pseudoran-\ndomness, e.g. assuming that the output of a hash function is uniform, which is\na widely used but unproven assumption, or (ii) they generate their randomness\nusing a distributed protocol in which several participants are required to submit\nrandom numbers which are then used in the generation of a final random result.\nHowever, in this case, there is no guarantee that the numbers provided by the\nparties are uniformly random and there is no incentive for the parties to honestly\ngenerate uniform randomness. Most random beacons have both limitations.\nIn this work, we provide a protocol for distributed generation of randomness.\nOur protocol does not rely on pseudorandomness at all. Similar to some of the\nprevious approaches, it uses random inputs by different participants to generate\na final random result. However, the crucial difference is that we provide a game-"
        }
    },
    {
        "id": "3559eb7256efd2a06bd9638000d30abfe694744b",
        "content": "then the dishonest stake ratio should be strictly smaller than 1/2 and preferably much\nsmaller. Moreover, n should not be too small. In other words, this approach assumes\nthat most of the stake, strictly more than half and preferably much more than half, is\nowned by honest participants. This is in contrast to the previous approach that only\nneeded more than 1/n.\n4.3\nFurther Details of the Approach\nParticipant Selection rules. Proof-of-stake blockchain protocols are important appli-\ncations of random beacons. To prevent Sybil attacks and enforce proof-of-stake, it is\ncommon to sample a small subset of participants based on stake ratios for the random\nbeacon. Verifiable random functions (VRF)[22] are popular for the purpose of selecting\nparticipants. VRF requires a random seed, which can be the output of RIG game in the\nprevious round. Similar to the treatment for VDF outputs to ensure uniformly random",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "then the dishonest stake ratio should be strictly smaller than 1/2 and preferably much\nsmaller. Moreover, n should not be too small. In other words, this approach assumes\nthat most of the stake, strictly more than half and preferably much more than half, is\nowned by honest participants. This is in contrast to the previous approach that only\nneeded more than 1/n.\n4.3\nFurther Details of the Approach\nParticipant Selection rules. Proof-of-stake blockchain protocols are important appli-\ncations of random beacons. To prevent Sybil attacks and enforce proof-of-stake, it is\ncommon to sample a small subset of participants based on stake ratios for the random\nbeacon. Verifiable random functions (VRF)[22] are popular for the purpose of selecting\nparticipants. VRF requires a random seed, which can be the output of RIG game in the\nprevious round. Similar to the treatment for VDF outputs to ensure uniformly random"
        }
    },
    {
        "id": "e2e944ed235df9efbb097cd487b1a36e2ea5715b",
        "content": "\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n0\n0 −1 −1 −1\n−1 1\n1\n1\n0\n0 −1 −1\n−1 −1 1\n1\n1\n0\n0 −1\n−1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n\n\n\n\n\n\n\n\n\n\n\n\nGame-theoretic Randomness for Proof-of-Stake\n11\nWhen f = 1, the matrix B(m,1) is the same as A(m). We can check that the mixed\nstrategy profile ¯σ under B(m,f) is also an alliance-resistant Nash equilibrium. To show\nthat it is the only Nash equilibrium, we follow the analysis we did for A(m). Suppose\nthere is another Nash equilibrium (˜σi, ˜σj) between two adjacent players i and j(= i+1)\nand ˜ui ≤0. Let ˜σj be (p0, p1, . . . , pm−1), then every element of r = B · ˜σj is at most\n˜ui, which is non-positive. However, Pm−1\nk=0 rk = 1T · r = 1T · B · ˜σj = 0T · ˜σj = 0,\nwhich requires r = 0 and ˜ui = 0. By r = 0, we have\nrs =\nX\n0≤l≤f−1\npl+s −\nX\nm−f≤l≤m−1\npl+s = 0\nfor every s ∈{0, 1, . . . , m −1}. With a slight misuse of notation, assume that pm+t =",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n0\n0 −1 −1 −1\n−1 1\n1\n1\n0\n0 −1 −1\n−1 −1 1\n1\n1\n0\n0 −1\n−1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n\n\n\n\n\n\n\n\n\n\n\n\nGame-theoretic Randomness for Proof-of-Stake\n11\nWhen f = 1, the matrix B(m,1) is the same as A(m). We can check that the mixed\nstrategy profile ¯σ under B(m,f) is also an alliance-resistant Nash equilibrium. To show\nthat it is the only Nash equilibrium, we follow the analysis we did for A(m). Suppose\nthere is another Nash equilibrium (˜σi, ˜σj) between two adjacent players i and j(= i+1)\nand ˜ui ≤0. Let ˜σj be (p0, p1, . . . , pm−1), then every element of r = B · ˜σj is at most\n˜ui, which is non-positive. However, Pm−1\nk=0 rk = 1T · r = 1T · B · ˜σj = 0T · ˜σj = 0,\nwhich requires r = 0 and ˜ui = 0. By r = 0, we have\nrs =\nX\n0≤l≤f−1\npl+s −\nX\nm−f≤l≤m−1\npl+s = 0\nfor every s ∈{0, 1, . . . , m −1}. With a slight misuse of notation, assume that pm+t ="
        }
    },
    {
        "id": "6f093c18bd021d9b8c316bcda603e65b6e94d0bf",
        "content": "the set of all probability distributions on X by ∆(X).\nOne-shot Games [25]. A one-shot game with n players is a tuple G = (S1, S2, . . . , Sn,\nu1, u2, . . . , un) where:\n– Each Si is a finite set of pure strategies for players i and S = S1 × S2 × · · · × Sn\nis the set of all outcomes; and\n– Each ui is a utility function of the form ui : S →R.\nIn a play, each player i chooses one strategy si ∈Si. The choices are simultaneous\nand independent. Then each player i is paid a utility of ui(s1, s2, . . . , sn) units.\nMixed Strategies [25]. A mixed strategy σi ∈∆(Si) for player i is a probability dis-\ntribution over Si, that characterizes the probability of playing each pure strategy in Si.\nA mixed strategy profile is a tuple σ = (σ1, σ2, . . . , σn) consisting of one mixed strat-\negy for each player. The expected utility ui(σ) of player i in a mixed strategy profile\nσ is defined as ui(σ) = Esi∼σi[ui(s1, s2, . . . , sn)]. Intuitively, in a mixed strategy, the",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "the set of all probability distributions on X by ∆(X).\nOne-shot Games [25]. A one-shot game with n players is a tuple G = (S1, S2, . . . , Sn,\nu1, u2, . . . , un) where:\n– Each Si is a finite set of pure strategies for players i and S = S1 × S2 × · · · × Sn\nis the set of all outcomes; and\n– Each ui is a utility function of the form ui : S →R.\nIn a play, each player i chooses one strategy si ∈Si. The choices are simultaneous\nand independent. Then each player i is paid a utility of ui(s1, s2, . . . , sn) units.\nMixed Strategies [25]. A mixed strategy σi ∈∆(Si) for player i is a probability dis-\ntribution over Si, that characterizes the probability of playing each pure strategy in Si.\nA mixed strategy profile is a tuple σ = (σ1, σ2, . . . , σn) consisting of one mixed strat-\negy for each player. The expected utility ui(σ) of player i in a mixed strategy profile\nσ is defined as ui(σ) = Esi∼σi[ui(s1, s2, . . . , sn)]. Intuitively, in a mixed strategy, the"
        }
    },
    {
        "id": "14e31a394f42fc8671abb2ddde0c015b9cffa776",
        "content": "deposits confiscated. On the other hand, honest participants are rewarded by the income\ngenerated from providing the random number generation service to external contracts.\nHowever, there is no way to ensure bias-resistance and availability. A malicious party\nmight choose not to reveal their value si as it might be beneficial to them to bias the out-\nput. So, if a party does not reveal values, the whole random number generation process\nshould be repeated, but even this biases the output as a malicious party can choose not to\nreveal only when the final result is not to their benefit in an external smart contract. Fi-\n4\nZ. Cai and A. Goharshady\nnally, RANDAO does not incentivize reliability and assumes that a reliable party exists,\nwithout arguing why. Economically Viable Randomness (EVR) also uses a commit-\nreveal scheme to generate randomness. It designs a punishment scheme to discourage\ndeviating from the protocol. However, similar to RANDAO, the incentives of EVR only",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "deposits confiscated. On the other hand, honest participants are rewarded by the income\ngenerated from providing the random number generation service to external contracts.\nHowever, there is no way to ensure bias-resistance and availability. A malicious party\nmight choose not to reveal their value si as it might be beneficial to them to bias the out-\nput. So, if a party does not reveal values, the whole random number generation process\nshould be repeated, but even this biases the output as a malicious party can choose not to\nreveal only when the final result is not to their benefit in an external smart contract. Fi-\n4\nZ. Cai and A. Goharshady\nnally, RANDAO does not incentivize reliability and assumes that a reliable party exists,\nwithout arguing why. Economically Viable Randomness (EVR) also uses a commit-\nreveal scheme to generate randomness. It designs a punishment scheme to discourage\ndeviating from the protocol. However, similar to RANDAO, the incentives of EVR only"
        }
    },
    {
        "id": "2e7336fa9c37e8758dd250e56e89957ca6aa6428",
        "content": "Treveal time slots. The RIG game is executed when the reveal phase completes.\nIn the commit phase, each participant pi broadcasts a signed commit message:\n(ss_id, hi, proofi)i, where ss_id is the session id of the execution and hi = hash(si|ri)\nis the commitment. si is the value the participant chooses and ri is a random nonce.\nproofi is a publicly verifiable proof that pi is an eligible participant, applicable when\nonly a subset of selected users are allowed to join the game. A commit message is valid\nif: (1) the message is properly signed by pi, (2) the message has a valid proof of par-\nticipation, (3) there is no other different valid commit message from pi in the network,\nand (4) the message is received during the commit phase.\nIn the reveal phase, each participant pi broadcasts a signed reveal message: (ss_id, si, ri)i.\nA reveal message is valid if (1) the message is received during the reveal phase, (2) pi",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "Treveal time slots. The RIG game is executed when the reveal phase completes.\nIn the commit phase, each participant pi broadcasts a signed commit message:\n(ss_id, hi, proofi)i, where ss_id is the session id of the execution and hi = hash(si|ri)\nis the commitment. si is the value the participant chooses and ri is a random nonce.\nproofi is a publicly verifiable proof that pi is an eligible participant, applicable when\nonly a subset of selected users are allowed to join the game. A commit message is valid\nif: (1) the message is properly signed by pi, (2) the message has a valid proof of par-\nticipation, (3) there is no other different valid commit message from pi in the network,\nand (4) the message is received during the commit phase.\nIn the reveal phase, each participant pi broadcasts a signed reveal message: (ss_id, si, ri)i.\nA reveal message is valid if (1) the message is received during the reveal phase, (2) pi"
        }
    },
    {
        "id": "4ba9a59ae973a3846afef09b1032162cfa44f80b",
        "content": "the commit phase correctly, they will lose their deposit. However, the participant might\nbenefit from a biased output of the random beacon in the downstream application, for\nwhich they might be willing to cheat even at the cost of losing the deposit. In order to\nprevent this possibility of adversarial manipulation on the game result, we make use of\na verifiable delay function VDF(·) as in [6] and require the participant to provide all\nnecessary parameters for the evaluation of the VDF as part of the commit message. We\nthen check that the provided VDF really evaluates to the strategy si of the player and\notherwise, simply remove player i from the game. Of course, the VDF evaluation time\nshould be long enough to ensure it cannot be evaluated until the reveal phase is over.\nUsing this technique, any adversary cannot have any information about the final\noutput by the end of the reveal phase. Therefore, revealing values honestly is always a",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "the commit phase correctly, they will lose their deposit. However, the participant might\nbenefit from a biased output of the random beacon in the downstream application, for\nwhich they might be willing to cheat even at the cost of losing the deposit. In order to\nprevent this possibility of adversarial manipulation on the game result, we make use of\na verifiable delay function VDF(·) as in [6] and require the participant to provide all\nnecessary parameters for the evaluation of the VDF as part of the commit message. We\nthen check that the provided VDF really evaluates to the strategy si of the player and\notherwise, simply remove player i from the game. Of course, the VDF evaluation time\nshould be long enough to ensure it cannot be evaluated until the reveal phase is over.\nUsing this technique, any adversary cannot have any information about the final\noutput by the end of the reveal phase. Therefore, revealing values honestly is always a"
        }
    },
    {
        "id": "4db723c37dc95de7cbf3c43ce1e4701bf1545639",
        "content": "n −2 players. For each pair (2 · k −1, 2 · k) of players, this is a zero-sum matrix game\nwith the following payoff matrix A(m) for player 2 · k −1:\nA(m) =\n\n\n\n\n\n\n\n1 −1 0 · · · 0\n0\n1 −1 · · · 0\n0\n0\n1 · · · 0\n...\n...\n...\n...\n−1 0\n0 · · · 1\n\n\n\n\n\n\n\n3.2\nAnalysis of Alliance-Resistant Nash Equilibria\nTheorem 1. (Alliance-Resistant Nash Equilibrium of an RIG.) Let G be an RIG game\nwith n players and m strategies, where n is an even number and m ≥3. Let ¯σ be a\nmixed strategy profile defined by ¯σi = (1/m, 1/m, . . . , 1/m) for all i, i.e. the mixed\nstrategy profile in which each player i chooses a strategy in Si uniformly at random.\nThen, ¯σ is the only Nash equilibrium of G. Further, it is also alliance-resistant.\nProof. First, we prove that ¯σ is an alliance-resistant Nash Equilibrium. Under the mixed\nstrategy profile, the expected payoff of each one of the players is 0. Let G be a sub-\nset of players, then the overall utility of all players in G is P\ni∈G ui(¯σ−G, σ) if play-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "n −2 players. For each pair (2 · k −1, 2 · k) of players, this is a zero-sum matrix game\nwith the following payoff matrix A(m) for player 2 · k −1:\nA(m) =\n\n\n\n\n\n\n\n1 −1 0 · · · 0\n0\n1 −1 · · · 0\n0\n0\n1 · · · 0\n...\n...\n...\n...\n−1 0\n0 · · · 1\n\n\n\n\n\n\n\n3.2\nAnalysis of Alliance-Resistant Nash Equilibria\nTheorem 1. (Alliance-Resistant Nash Equilibrium of an RIG.) Let G be an RIG game\nwith n players and m strategies, where n is an even number and m ≥3. Let ¯σ be a\nmixed strategy profile defined by ¯σi = (1/m, 1/m, . . . , 1/m) for all i, i.e. the mixed\nstrategy profile in which each player i chooses a strategy in Si uniformly at random.\nThen, ¯σ is the only Nash equilibrium of G. Further, it is also alliance-resistant.\nProof. First, we prove that ¯σ is an alliance-resistant Nash Equilibrium. Under the mixed\nstrategy profile, the expected payoff of each one of the players is 0. Let G be a sub-\nset of players, then the overall utility of all players in G is P\ni∈G ui(¯σ−G, σ) if play-"
        }
    },
    {
        "id": "840b831701f8e5fd3023fbc6fd724029e52c98e8",
        "content": "is determined by the specific application. The design and amount of reward/penalty and\ndeposits are also subject to the specific application. We will address these adjustable\nconfigurations in Section 4.3. Finally, we focus on the case where our protocol is used\n12\nZ. Cai and A. Goharshady\nin conjunction with a blockchain protocol, including gossip protocols for announce-\nments.\n4.1\nCommitment Scheme and VDF approach\nAs mentioned above, commitment schemes are already widely used in random number\ngeneration in distributed systems. As expected, our approach has two phases: commit\nand reveal. The execution starts with the commit phase, which lasts for Tcommit time\nslots. In a blockchain ecosystem, we can use the block number to keep track of time.\nAfter the commit phase ends, the execution enters the reveal phase, which lasts for\nTreveal time slots. The RIG game is executed when the reveal phase completes.\nIn the commit phase, each participant pi broadcasts a signed commit message:",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "is determined by the specific application. The design and amount of reward/penalty and\ndeposits are also subject to the specific application. We will address these adjustable\nconfigurations in Section 4.3. Finally, we focus on the case where our protocol is used\n12\nZ. Cai and A. Goharshady\nin conjunction with a blockchain protocol, including gossip protocols for announce-\nments.\n4.1\nCommitment Scheme and VDF approach\nAs mentioned above, commitment schemes are already widely used in random number\ngeneration in distributed systems. As expected, our approach has two phases: commit\nand reveal. The execution starts with the commit phase, which lasts for Tcommit time\nslots. In a blockchain ecosystem, we can use the block number to keep track of time.\nAfter the commit phase ends, the execution enters the reveal phase, which lasts for\nTreveal time slots. The RIG game is executed when the reveal phase completes.\nIn the commit phase, each participant pi broadcasts a signed commit message:"
        }
    },
    {
        "id": "d8d7d06e64f378234e257ad1e82bf9aab29eef52",
        "content": "25. Roughgarden, T., Nisan, N.: Algorithmic Game Theory. Cambridge University Press (2007)\n26. Schindler, P., Judmayer, A., Stifter, N., Weippl, E.R.: Hydrand: Efficient continuous dis-\ntributed randomness. In: SP. pp. 73–89 (2020)\n27. Schoenmakers, B.: A simple publicly verifiable secret sharing scheme and its application to\nelectronic voting. In: CRYPTO. pp. 148–164 (1999)\n28. Syta, E., Jovanovic, P., Kokoris-Kogias, E., Gailly, N., Gasser, L., Khoffi, I., Fischer, M.J.,\nFord, B.: Scalable bias-resistant distributed randomness. In: SP. pp. 444–460 (2017)\n29. Wang, G., Nixon, M.: Randchain: Practical scalable decentralized randomness attested by\nblockchain. In: IEEE Blockchain. pp. 442–449 (2020)\n30. Yakira, D., Asayag, A., Grayevsky, I., Keidar, I.: Economically viable randomness. arXiv\npreprint arXiv:2007.03531 (2020)",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "25. Roughgarden, T., Nisan, N.: Algorithmic Game Theory. Cambridge University Press (2007)\n26. Schindler, P., Judmayer, A., Stifter, N., Weippl, E.R.: Hydrand: Efficient continuous dis-\ntributed randomness. In: SP. pp. 73–89 (2020)\n27. Schoenmakers, B.: A simple publicly verifiable secret sharing scheme and its application to\nelectronic voting. In: CRYPTO. pp. 148–164 (1999)\n28. Syta, E., Jovanovic, P., Kokoris-Kogias, E., Gailly, N., Gasser, L., Khoffi, I., Fischer, M.J.,\nFord, B.: Scalable bias-resistant distributed randomness. In: SP. pp. 444–460 (2017)\n29. Wang, G., Nixon, M.: Randchain: Practical scalable decentralized randomness attested by\nblockchain. In: IEEE Blockchain. pp. 442–449 (2020)\n30. Yakira, D., Asayag, A., Grayevsky, I., Keidar, I.: Economically viable randomness. arXiv\npreprint arXiv:2007.03531 (2020)"
        }
    },
    {
        "id": "fb30b95ab30dafc449f63a594f3ddd3481a89db1",
        "content": "showed that our approach is bias-resistant, unpredictable, available, verifiable and in-\ncentivizes every participant to be reliable, i.e. faithfully provide a uniform random input.\nEven if only one of the participants is rational and thus reliable, the output number is\nguaranteed to be unbiased. Additionally, our approach does not use pseudo-randomness\nat any point and instead only relies on well-incentivized game-theoretic randomness. Fi-\nnally, even though the approach is general and not limited to blockchain use cases, we\nshowed that one can easily augment common proof-of-stake protocols to include our\nrandomness beacon for the task of selecting their miners. This ensures that proof-of-\nstake protocols choose the miners fairly, i.e. exactly in proportion to their stake.\nReferences\n1. RANDAO: A DAO working as RNG of Ethereum (2019), https://github.com/randao/randao\n2. Cambridge bitcoin electricity consumption index (2022), https://ccaf.io/cbeci/index",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "showed that our approach is bias-resistant, unpredictable, available, verifiable and in-\ncentivizes every participant to be reliable, i.e. faithfully provide a uniform random input.\nEven if only one of the participants is rational and thus reliable, the output number is\nguaranteed to be unbiased. Additionally, our approach does not use pseudo-randomness\nat any point and instead only relies on well-incentivized game-theoretic randomness. Fi-\nnally, even though the approach is general and not limited to blockchain use cases, we\nshowed that one can easily augment common proof-of-stake protocols to include our\nrandomness beacon for the task of selecting their miners. This ensures that proof-of-\nstake protocols choose the miners fairly, i.e. exactly in proportion to their stake.\nReferences\n1. RANDAO: A DAO working as RNG of Ethereum (2019), https://github.com/randao/randao\n2. Cambridge bitcoin electricity consumption index (2022), https://ccaf.io/cbeci/index"
        }
    },
    {
        "id": "029491abcd221d5264690fdafbc2969daa7682eb",
        "content": "3.3\nDense RIG bimatrix game\nIf the strategy size m is large, which is the case when we aim to generate integers from\na large range, then the matrix A would be sparse. If the number of players is much\nsmaller than m, then the probability that one party really receives a non-zero payoff is\nnegligible. Therefore, it is desirable to design a matrix B that is dense and also provides\nthe same unique alliance-resistant equilibrium property as A.\nFor any integer f such that 1 ≤f ≤m/2 and gcd(f, m) = 1, we can construct\na new matrix B(m,f) with dense parameter 2f/m such that B(m,f)\ni,j\n= g(j −i), where\ng(·) is defined as:\ng(l) :=\n\n\n\n\n\n1\nif 0 ≤l ≤f −1\n(mod m)\n−1\nif m −f ≤l ≤m −1\n(mod m)\n0\notherwise.\nFor m = 8 and f = 3, the new payoff matrix B(8,3) is the following:\nB =\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n0\n0 −1 −1 −1\n−1 1\n1\n1\n0\n0 −1 −1\n−1 −1 1\n1\n1\n0\n0 −1\n−1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n\n\n\n\n\n\n\n\n\n\n\n",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "3.3\nDense RIG bimatrix game\nIf the strategy size m is large, which is the case when we aim to generate integers from\na large range, then the matrix A would be sparse. If the number of players is much\nsmaller than m, then the probability that one party really receives a non-zero payoff is\nnegligible. Therefore, it is desirable to design a matrix B that is dense and also provides\nthe same unique alliance-resistant equilibrium property as A.\nFor any integer f such that 1 ≤f ≤m/2 and gcd(f, m) = 1, we can construct\na new matrix B(m,f) with dense parameter 2f/m such that B(m,f)\ni,j\n= g(j −i), where\ng(·) is defined as:\ng(l) :=\n\n\n\n\n\n1\nif 0 ≤l ≤f −1\n(mod m)\n−1\nif m −f ≤l ≤m −1\n(mod m)\n0\notherwise.\nFor m = 8 and f = 3, the new payoff matrix B(8,3) is the following:\nB =\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n0\n0 −1 −1 −1\n−1 1\n1\n1\n0\n0 −1 −1\n−1 −1 1\n1\n1\n0\n0 −1\n−1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n0\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n1\n1\n1\n0\n0 −1 −1 −1 1\n\n\n\n\n\n\n\n\n\n\n\n"
        }
    },
    {
        "id": "3a67a5aac4220f9f8716b60950c53da983f7fe07",
        "content": "is Y. V is a triplet of algorithms as follows:\n– Setup(λ, t) →(ek, vk). This function generates an evaluation key ek and a veri-\nfication key vk in polynomial time with respect to λ.\n– Eval(ek, x) →(y, π) takes an input x ∈X and produces an output y ∈Y and\na proof π. Eval may use randomness in computing the proof π but not in the\ncomputation of y. It must run in parallel time t with poly(log(t), λ) processors.\n– Verify(vk, x, y, π) →{Yes, No} is a deterministic algorithm that verifies the cor-\nrectness of evaluation in sequential time poly(log(t), λ).\nSee [6] for more details and desired properties. Intuitively, anyone can evaluate the VDF\nusing the evaluation key. However, this takes a long time, i.e. at least t steps, even when\nusing parallelization. Even if a malicious participant has strong parallel computational\npower, they cannot evaluate the VDF significantly faster than an ordinary participant",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "is Y. V is a triplet of algorithms as follows:\n– Setup(λ, t) →(ek, vk). This function generates an evaluation key ek and a veri-\nfication key vk in polynomial time with respect to λ.\n– Eval(ek, x) →(y, π) takes an input x ∈X and produces an output y ∈Y and\na proof π. Eval may use randomness in computing the proof π but not in the\ncomputation of y. It must run in parallel time t with poly(log(t), λ) processors.\n– Verify(vk, x, y, π) →{Yes, No} is a deterministic algorithm that verifies the cor-\nrectness of evaluation in sequential time poly(log(t), λ).\nSee [6] for more details and desired properties. Intuitively, anyone can evaluate the VDF\nusing the evaluation key. However, this takes a long time, i.e. at least t steps, even when\nusing parallelization. Even if a malicious participant has strong parallel computational\npower, they cannot evaluate the VDF significantly faster than an ordinary participant"
        }
    },
    {
        "id": "6360d44cb4452828082ac3bad87720952a3c5cdb",
        "content": "most t −1 with coefficients in Zq of the form p(x) = Pt−1\nj=0 αj · xj. Here, we have\nα0 = Gr, i.e. the number r is encoded in the first coefficient of the polynomial\nand every other αj is a random number from Zq. The dealer then publishes the\nfollowing:\n• Commitment: Cj = gαj, for 0 ≤j < t. This ensures that the dealer is com-\nmitting to the polynomial and cannot change it later.\n• Encrypted shares: For each player Pi, the dealer computes and publishes Yi =\nyp(i)\ni\n, for 1 ≤i < n. Intuitively, the dealer is taking the value p(i) of the\npolynomial p at point i and encrypting it using yi so that only the i-th player\ncan decrypt it. This encrypted value is then published.\n• Proof of correctness: The dealer provides a non-interactive zero-knowledge\nproof ensuring that the encrypted shares above are valid. See [27] for details.\n– Verification of the shares. Anyone on the network, be it a player Pi or a non-\nparticipant third-party, can verify the proof and encrypted shares provided by the",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "most t −1 with coefficients in Zq of the form p(x) = Pt−1\nj=0 αj · xj. Here, we have\nα0 = Gr, i.e. the number r is encoded in the first coefficient of the polynomial\nand every other αj is a random number from Zq. The dealer then publishes the\nfollowing:\n• Commitment: Cj = gαj, for 0 ≤j < t. This ensures that the dealer is com-\nmitting to the polynomial and cannot change it later.\n• Encrypted shares: For each player Pi, the dealer computes and publishes Yi =\nyp(i)\ni\n, for 1 ≤i < n. Intuitively, the dealer is taking the value p(i) of the\npolynomial p at point i and encrypting it using yi so that only the i-th player\ncan decrypt it. This encrypted value is then published.\n• Proof of correctness: The dealer provides a non-interactive zero-knowledge\nproof ensuring that the encrypted shares above are valid. See [27] for details.\n– Verification of the shares. Anyone on the network, be it a player Pi or a non-\nparticipant third-party, can verify the proof and encrypted shares provided by the"
        }
    },
    {
        "id": "3371541df0dfebe261e18477f6cef1b0c0e116b4",
        "content": "By the definition of Nash equilibrium, player i cannot increase its utility by changing\nits strategy ˜σi to any other strategy σi, while player j keeps playing the same strategy\n˜σj. This indicates that every coordinate of the vector A(m) · ˜σj is no more than ˜ui,\nwhich is at most 0. Let ˜σj = (p0, p1, . . . , pm−1), then the utility of the player i playing\npure strategy k is δk = pk −pk+1 (mod m) ≤˜ui ≤0, for each k in {0, 1, . . . , m −\n1}. However, Pm−1\nk=0 δk = Pm−1\nk=0 pk −Pm−1\nk=0 pk = 0. So it must hold that ˜σj =\n(1/m, 1/m, . . . , 1/m) and ˜ui = ˜uj = 0. Since ˜uj = 0 ≤0, a similar analysis can\nshow that ˜σi = (1/m, 1/m, . . . , 1/m). This proves that ¯σ is the only Nash equilibrium\nfor the Random Integer Generation game.\nThe theorem above shows that it is in every player’s best interest to play uniformly\nat random, i.e. choose each pure strategy in Si with probability exactly 1/m. Moreover,",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "By the definition of Nash equilibrium, player i cannot increase its utility by changing\nits strategy ˜σi to any other strategy σi, while player j keeps playing the same strategy\n˜σj. This indicates that every coordinate of the vector A(m) · ˜σj is no more than ˜ui,\nwhich is at most 0. Let ˜σj = (p0, p1, . . . , pm−1), then the utility of the player i playing\npure strategy k is δk = pk −pk+1 (mod m) ≤˜ui ≤0, for each k in {0, 1, . . . , m −\n1}. However, Pm−1\nk=0 δk = Pm−1\nk=0 pk −Pm−1\nk=0 pk = 0. So it must hold that ˜σj =\n(1/m, 1/m, . . . , 1/m) and ˜ui = ˜uj = 0. Since ˜uj = 0 ≤0, a similar analysis can\nshow that ˜σi = (1/m, 1/m, . . . , 1/m). This proves that ¯σ is the only Nash equilibrium\nfor the Random Integer Generation game.\nThe theorem above shows that it is in every player’s best interest to play uniformly\nat random, i.e. choose each pure strategy in Si with probability exactly 1/m. Moreover,"
        }
    },
    {
        "id": "41cfce94db9034aa9d508864e4da95965cd146cc",
        "content": "ipants and create a random output based on random numbers submitted by participants\nof the protocol. Usually, the final value is simply defined by the modular sum of all input\nvalues by participants modulo some large number p, i.e. v := Pn\ni=1 si (mod p). If the\nprotocol generates only a single random bit, then p = 2 and the modular sum is equiv-\nalent to the xor operation. Using the summation formula above, if the input values of\ndifferent participants are chosen independently and if at least one of the participants\nsubmits a uniform random value in the range [0, p −1], then the final output is also a\nuniform random value. We call such a participant reliable. Note that it is enough to have\nonly one reliable participant for the final output to have the uniform distribution when\nthe values are submitted independently. Therefore, the distributed randomness protocols\nGame-theoretic Randomness for Proof-of-Stake\n3",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "ipants and create a random output based on random numbers submitted by participants\nof the protocol. Usually, the final value is simply defined by the modular sum of all input\nvalues by participants modulo some large number p, i.e. v := Pn\ni=1 si (mod p). If the\nprotocol generates only a single random bit, then p = 2 and the modular sum is equiv-\nalent to the xor operation. Using the summation formula above, if the input values of\ndifferent participants are chosen independently and if at least one of the participants\nsubmits a uniform random value in the range [0, p −1], then the final output is also a\nuniform random value. We call such a participant reliable. Note that it is enough to have\nonly one reliable participant for the final output to have the uniform distribution when\nthe values are submitted independently. Therefore, the distributed randomness protocols\nGame-theoretic Randomness for Proof-of-Stake\n3"
        }
    },
    {
        "id": "230c78f6808cd63d42603a49748a65dfdac15895",
        "content": "– For every player i ∈{1, . . . , n}, we have m pure strategies Si = {0, 1, . . . , m−1};\n– We pair the players such that every even player is paired with the previous odd\nplayer and every odd player is paired with the next even player. In other words,\npair(2 · k) = 2 · k −1 and pair(2 · k −1) = 2 · k.\nGame-theoretic Randomness for Proof-of-Stake\n9\n– At an outcome s = (s1, s2, . . . , sn) of the game, the payoff of player i = 2 · k −1\nis defined as ui(s) := f(s2k−1, s2k) and the payoff of player j = 2 · k is defined as\nuj(s) := −ui(s) = −f(s2k−1, s2k), where\nf(s2k−1, s2k) :=\n\n\n\n\n\n1\nif s2k−1 −s2k ≡0\n(mod m)\n−1\nif s2k−1 −s2k ≡−1\n(mod m)\n0\notherwise\nEssentially, we assume that any adjacent pair of even player and odd player play\na zero-sum one-shot game with each other. Their payoffs are independent of the other\nn −2 players. For each pair (2 · k −1, 2 · k) of players, this is a zero-sum matrix game\nwith the following payoff matrix A(m) for player 2 · k −1:\nA(m) =\n\n\n\n\n\n\n",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "– For every player i ∈{1, . . . , n}, we have m pure strategies Si = {0, 1, . . . , m−1};\n– We pair the players such that every even player is paired with the previous odd\nplayer and every odd player is paired with the next even player. In other words,\npair(2 · k) = 2 · k −1 and pair(2 · k −1) = 2 · k.\nGame-theoretic Randomness for Proof-of-Stake\n9\n– At an outcome s = (s1, s2, . . . , sn) of the game, the payoff of player i = 2 · k −1\nis defined as ui(s) := f(s2k−1, s2k) and the payoff of player j = 2 · k is defined as\nuj(s) := −ui(s) = −f(s2k−1, s2k), where\nf(s2k−1, s2k) :=\n\n\n\n\n\n1\nif s2k−1 −s2k ≡0\n(mod m)\n−1\nif s2k−1 −s2k ≡−1\n(mod m)\n0\notherwise\nEssentially, we assume that any adjacent pair of even player and odd player play\na zero-sum one-shot game with each other. Their payoffs are independent of the other\nn −2 players. For each pair (2 · k −1, 2 · k) of players, this is a zero-sum matrix game\nwith the following payoff matrix A(m) for player 2 · k −1:\nA(m) =\n\n\n\n\n\n\n"
        }
    },
    {
        "id": "235e1bd219d4c31264d4504b14fb46cf9e7a59cf",
        "content": "agrees to a change of strategies in the alliance P if and only if their own utility is\nstrictly increased. However, if the players can share and redistribute utilities, or if they\nare indeed controlled by the same person, then a group is willing to defect as long as\ntheir total utility increases, which leads to an even stronger notion of equilibrium:\nAlliance-resistant Nash Equilibria [11]. An alliance-resistant Nash equilibrium is a\nmixed strategy profile σ such that for any non-empty set P of players and any strategy\nprofile ˜σP , it holds that uP (σ) ≥uP (˜σP , σ−P ), where uP is the sum of utilities of all\nmember of P. In our setting, especially in PoS blockchain protocols, alliance-resistant\nequilibria are the suitable notion to justify stability and self-enforceability, because a\nperson with a large stake is likely to control multiple players in the randomly selected\ncommittee and only care about the overall revenue.\n2.2\nPublicly-Verifiable Secret Sharing",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "agrees to a change of strategies in the alliance P if and only if their own utility is\nstrictly increased. However, if the players can share and redistribute utilities, or if they\nare indeed controlled by the same person, then a group is willing to defect as long as\ntheir total utility increases, which leads to an even stronger notion of equilibrium:\nAlliance-resistant Nash Equilibria [11]. An alliance-resistant Nash equilibrium is a\nmixed strategy profile σ such that for any non-empty set P of players and any strategy\nprofile ˜σP , it holds that uP (σ) ≥uP (˜σP , σ−P ), where uP is the sum of utilities of all\nmember of P. In our setting, especially in PoS blockchain protocols, alliance-resistant\nequilibria are the suitable notion to justify stability and self-enforceability, because a\nperson with a large stake is likely to control multiple players in the randomly selected\ncommittee and only care about the overall revenue.\n2.2\nPublicly-Verifiable Secret Sharing"
        }
    },
    {
        "id": "d20d04dc1f22b4a6dafb119dd2c88d01eb1a7c58",
        "content": "sult, which can also be checked by any third party efficiently.\nVRFs. Verifiable random functions [22, 14] are widely used in PoS blockchain proto-\ncols [17, 13]. A party can run a VRF locally, producing a pseudo-random output value\nbased on their secret key and random seed. The VRF also outputs a proof of the output\nthat can be verified by anyone with access to the party’s public key and random seed.\nWith the use of VRFs, malicious parties cannot predict who the future miners are before\nthe miners announce their identities themselves.\nAlgorand. Algorand [17] is a proof-of-stake blockchain protocol based on Byzantine\nagreement. The random seed for its VRF is based on the VRF of the previous round.\nWhile this guarantees most of the desired properties, a major drawback of this random-\nness beacon is that the generated numbers are not guaranteed to be uniform.\nOuroboros and Ouroboros Praos. Ouroboros [19] was the first provably secure proof-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "sult, which can also be checked by any third party efficiently.\nVRFs. Verifiable random functions [22, 14] are widely used in PoS blockchain proto-\ncols [17, 13]. A party can run a VRF locally, producing a pseudo-random output value\nbased on their secret key and random seed. The VRF also outputs a proof of the output\nthat can be verified by anyone with access to the party’s public key and random seed.\nWith the use of VRFs, malicious parties cannot predict who the future miners are before\nthe miners announce their identities themselves.\nAlgorand. Algorand [17] is a proof-of-stake blockchain protocol based on Byzantine\nagreement. The random seed for its VRF is based on the VRF of the previous round.\nWhile this guarantees most of the desired properties, a major drawback of this random-\nness beacon is that the generated numbers are not guaranteed to be uniform.\nOuroboros and Ouroboros Praos. Ouroboros [19] was the first provably secure proof-"
        }
    },
    {
        "id": "2897fd7606b4a73f814c5e37e2ea4605f89a15f2",
        "content": "random bits. Instead, we must set m = 2k and have a single non-parallel RIG game. As\nan example, consider the simplified case of two players and k bits. If each player only\nuniformly randomly set their first bit, and then copied the same bit to all other bits, this\nwould also form a Nash equilibrium. However, this Nash equilibrium does not produce\na uniform random output. Instead, the output is 0 with 50% probability and 2k −1 with\n50% probability. More generally, any σ = (σ1, σ2) such that σi(j-th bit is 0) = 1/2 for\nall 1 ≤j ≤m is a Nash equilibrium. The existence of these undesirable Nash equilibria\nbreaks the guarantee of uniform random distribution over the final output value. Hence,\nparallelization should not be used and a game on 2k strategies should be played in the\nfirst place.\n4\nDesigning a Random Beacon Based on RIG\nIn this section, we discuss how to use a single execution of the Random Integer Gen-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "random bits. Instead, we must set m = 2k and have a single non-parallel RIG game. As\nan example, consider the simplified case of two players and k bits. If each player only\nuniformly randomly set their first bit, and then copied the same bit to all other bits, this\nwould also form a Nash equilibrium. However, this Nash equilibrium does not produce\na uniform random output. Instead, the output is 0 with 50% probability and 2k −1 with\n50% probability. More generally, any σ = (σ1, σ2) such that σi(j-th bit is 0) = 1/2 for\nall 1 ≤j ≤m is a Nash equilibrium. The existence of these undesirable Nash equilibria\nbreaks the guarantee of uniform random distribution over the final output value. Hence,\nparallelization should not be used and a game on 2k strategies should be played in the\nfirst place.\n4\nDesigning a Random Beacon Based on RIG\nIn this section, we discuss how to use a single execution of the Random Integer Gen-"
        }
    },
    {
        "id": "35f16fb8c13a0c6c5d2dc6f747faf61cc34ec167",
        "content": "previous approaches, it uses random inputs by different participants to generate\na final random result. However, the crucial difference is that we provide a game-\ntheoretic guarantee showing that it is in everyone’s best interest to submit uniform\nrandom numbers. Hence, our approach is the first to incentivize honest behavior\ninstead of just assuming it. Moreover, the approach is trustless and generates\nunbiased random numbers. It is also tamper-proof and no party can change the\noutput or affect its distribution. Finally, it is designed with modularity in mind\nand can be easily plugged into existing distributed protocols such as proof-of-\nstake blockchains.\nKeywords: Distributed Randomness · Proof-of-stake · Mechanism Design\n1\nIntroduction\nProof of Work. Bitcoin, the first blockchain protocol, was proposed by Satoshi Nakamoto\nto achieve consensus in a decentralized peer-to-peer electronic payment system [23]. In",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "previous approaches, it uses random inputs by different participants to generate\na final random result. However, the crucial difference is that we provide a game-\ntheoretic guarantee showing that it is in everyone’s best interest to submit uniform\nrandom numbers. Hence, our approach is the first to incentivize honest behavior\ninstead of just assuming it. Moreover, the approach is trustless and generates\nunbiased random numbers. It is also tamper-proof and no party can change the\noutput or affect its distribution. Finally, it is designed with modularity in mind\nand can be easily plugged into existing distributed protocols such as proof-of-\nstake blockchains.\nKeywords: Distributed Randomness · Proof-of-stake · Mechanism Design\n1\nIntroduction\nProof of Work. Bitcoin, the first blockchain protocol, was proposed by Satoshi Nakamoto\nto achieve consensus in a decentralized peer-to-peer electronic payment system [23]. In"
        }
    },
    {
        "id": "8d8b05fabdf428b6b2f15cc5031e34c839d85d62",
        "content": "nomial p that is of degree t −1. So, they can find the unique polynomial that goes\nthrough these points. Note that after all the shares are decrypted, anyone on the\nnetwork can use t of the shares to compute the polynomial p and then Gr is sim-\nply p(0). However, before the decryption of the shares in the reconstruction step,\nfinding Gr requires the collaboration of at least t participants and no set of t −1\nparticipants can obtain any information about Gr. Finally, knowing Gr and U, it is\neasy to find the secret s, i.e. s = U −h(Gr).\nA PVSS scheme can be used to generate random numbers. To do so, we use a separate\nPVSS scheme for each participant Pi. All n PVSS schemes run in parallel. In the i-\nth scheme, Pi is the dealer and everyone else is a normal participant. Pi first chooses\na random number si and then performs the protocol above as the dealer. At the end\nof the process, all the si’s are revealed by pooling the shares and we can use v =\nPn",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "nomial p that is of degree t −1. So, they can find the unique polynomial that goes\nthrough these points. Note that after all the shares are decrypted, anyone on the\nnetwork can use t of the shares to compute the polynomial p and then Gr is sim-\nply p(0). However, before the decryption of the shares in the reconstruction step,\nfinding Gr requires the collaboration of at least t participants and no set of t −1\nparticipants can obtain any information about Gr. Finally, knowing Gr and U, it is\neasy to find the secret s, i.e. s = U −h(Gr).\nA PVSS scheme can be used to generate random numbers. To do so, we use a separate\nPVSS scheme for each participant Pi. All n PVSS schemes run in parallel. In the i-\nth scheme, Pi is the dealer and everyone else is a normal participant. Pi first chooses\na random number si and then performs the protocol above as the dealer. At the end\nof the process, all the si’s are revealed by pooling the shares and we can use v =\nPn"
        }
    },
    {
        "id": "22569f3b6a09d8cc62f6c12ae615d9955d7e8cf1",
        "content": "for our random number generation protocol. However, evaluation takes a long time, and\nhence the choice will not be revealed while in the commit phase.\nNote that both PVSS and VDF methods above can be used to ensure availability\nand defend against dishonest parties who do not reveal their choices. However, they do\nnot incentivize the parties to be reliable and choose their si uniformly at random. This\nis our main contribution in the next section.\n3\nRandom Integer Generation Game (RIG)\nWe now provide the main component of our approach, i.e. a game to incentivize relia-\nbility in random number generation.\n3.1\nOverview of RIG\nRIG. Suppose that we have n players and n is even. A Random Integer Generation\ngame (RIG) with n players and m ≥2 strategies is a game G in which:\n– For every player i ∈{1, . . . , n}, we have m pure strategies Si = {0, 1, . . . , m−1};\n– We pair the players such that every even player is paired with the previous odd",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "for our random number generation protocol. However, evaluation takes a long time, and\nhence the choice will not be revealed while in the commit phase.\nNote that both PVSS and VDF methods above can be used to ensure availability\nand defend against dishonest parties who do not reveal their choices. However, they do\nnot incentivize the parties to be reliable and choose their si uniformly at random. This\nis our main contribution in the next section.\n3\nRandom Integer Generation Game (RIG)\nWe now provide the main component of our approach, i.e. a game to incentivize relia-\nbility in random number generation.\n3.1\nOverview of RIG\nRIG. Suppose that we have n players and n is even. A Random Integer Generation\ngame (RIG) with n players and m ≥2 strategies is a game G in which:\n– For every player i ∈{1, . . . , n}, we have m pure strategies Si = {0, 1, . . . , m−1};\n– We pair the players such that every even player is paired with the previous odd"
        }
    },
    {
        "id": "3244ffc79ab70a92290f7dee478afb6a18e803bb",
        "content": "participants. VRF requires a random seed, which can be the output of RIG game in the\nprevious round. Similar to the treatment for VDF outputs to ensure uniformly random\ndistribution, we can also use the trick of separating the bits of the random seed v to two\nparts v1 and v2 and using v1 + V RF(v2) instead of V RF(v).\nSorting rules. In contrast to RANDAO and many other random number generators, our\nRIG game is sensitive to the order of participants. The result of the RIG game is not only\nthe output value, which is the sum of all valid values submitted by the participants, but\nalso the payoffs. The honest participants who reveal their values faithfully might receive\ndifferent rewards/penalties depending on the ordering of participants. As before, we can\nuse the output of the previous RIG round to generate a random ordering for the current\nround. Finally, the RIG game requires an even number of participants, so if the number",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "participants. VRF requires a random seed, which can be the output of RIG game in the\nprevious round. Similar to the treatment for VDF outputs to ensure uniformly random\ndistribution, we can also use the trick of separating the bits of the random seed v to two\nparts v1 and v2 and using v1 + V RF(v2) instead of V RF(v).\nSorting rules. In contrast to RANDAO and many other random number generators, our\nRIG game is sensitive to the order of participants. The result of the RIG game is not only\nthe output value, which is the sum of all valid values submitted by the participants, but\nalso the payoffs. The honest participants who reveal their values faithfully might receive\ndifferent rewards/penalties depending on the ordering of participants. As before, we can\nuse the output of the previous RIG round to generate a random ordering for the current\nround. Finally, the RIG game requires an even number of participants, so if the number"
        }
    },
    {
        "id": "68691b32de4f37836f918cd38dba6091c98ff546",
        "content": "that a majority of the computational power on the network is owned by honest par-\nticipants, the security claims of proof-of-stake protocols rely on the assumption that a\nmajority, or a high percentage, of the stake is owned by honest participants. Despite\ntheir differences, all proof-of-stake protocols require a random beacon to randomly se-\nlect the next miners in an unpredictable manner.\nDistributed Randomness. A random beacon is an ideal oracle used in a distributed\nprotocol, e.g. a proof-of-stake blockchain, that emits a fresh random number in prede-\ntermined intervals. Designing random beacons is an active research topic in the context\nof distributed and decentralized computation [28, 26, 29, 20, 7]. The desired properties\nof a random beacon are as follows:\n– Bias-resistance: The output should always be sampled according to a fixed under-\nlying distribution δ, which is usually the uniform distribution. No party should be\nable to bias the output or change the distribution δ.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "that a majority of the computational power on the network is owned by honest par-\nticipants, the security claims of proof-of-stake protocols rely on the assumption that a\nmajority, or a high percentage, of the stake is owned by honest participants. Despite\ntheir differences, all proof-of-stake protocols require a random beacon to randomly se-\nlect the next miners in an unpredictable manner.\nDistributed Randomness. A random beacon is an ideal oracle used in a distributed\nprotocol, e.g. a proof-of-stake blockchain, that emits a fresh random number in prede-\ntermined intervals. Designing random beacons is an active research topic in the context\nof distributed and decentralized computation [28, 26, 29, 20, 7]. The desired properties\nof a random beacon are as follows:\n– Bias-resistance: The output should always be sampled according to a fixed under-\nlying distribution δ, which is usually the uniform distribution. No party should be\nable to bias the output or change the distribution δ."
        }
    },
    {
        "id": "f5692f298e7f0e384429a1deae26a3819677757e",
        "content": "then the pooling mechanism is successful and anyone can find all the secrets from valid\n14\nZ. Cai and A. Goharshady\nsecret shares, without the help of any dishonest participant. The dishonest participants\ncannot mislead honest participants by providing wrong decryption of secret shares in\nreconstruction, because the decryption for reconstruction also requires a publicly veri-\nfiable proof.\nSuppose the number of dishonest participants is f. PVSS requires f < t ≤n −f.\nTherefore, we can assume that n ≥2·f +1 and let t = ⌈n/2⌉. In other words, we need\nto assume that more than half of the participants are honest. In a proof-of-stake use case,\nthe set of participants for each execution are sampled from the population of miners\nbased on their stake. If we want n ≥2 · f + 1 to hold with overwhelming probability,\nthen the dishonest stake ratio should be strictly smaller than 1/2 and preferably much\nsmaller. Moreover, n should not be too small. In other words, this approach assumes",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "then the pooling mechanism is successful and anyone can find all the secrets from valid\n14\nZ. Cai and A. Goharshady\nsecret shares, without the help of any dishonest participant. The dishonest participants\ncannot mislead honest participants by providing wrong decryption of secret shares in\nreconstruction, because the decryption for reconstruction also requires a publicly veri-\nfiable proof.\nSuppose the number of dishonest participants is f. PVSS requires f < t ≤n −f.\nTherefore, we can assume that n ≥2·f +1 and let t = ⌈n/2⌉. In other words, we need\nto assume that more than half of the participants are honest. In a proof-of-stake use case,\nthe set of participants for each execution are sampled from the population of miners\nbased on their stake. If we want n ≥2 · f + 1 to hold with overwhelming probability,\nthen the dishonest stake ratio should be strictly smaller than 1/2 and preferably much\nsmaller. Moreover, n should not be too small. In other words, this approach assumes"
        }
    },
    {
        "id": "f8e9b99f7e197415ed732cc59d6e11c1a94a8584",
        "content": "are not guaranteed to be uniformly random, despite the fact that they are assumed to be\nuniform in the security analysis.\nOur Contribution. Our main contributions are as follows:\n– First, we design a novel game-theoretic approach for randomness generation. We\ncall this an RIG (Random Integer Generation) game. RIG efficiently produces a\nuniform random integer from an arbitrarily large interval. Moreover, we show that\nthe only equilibrium in an RIG is for all participants to choose their si uniformly at\nrandom. In other words, our RIG ensures that the participants are incentivized not\nonly to be honest, but also to be reliable. This will alleviate the problems with the\nprevious approaches and ensure that all desired properties of distributed random-\nness are attained.\n– We show that our RIG approach can be plugged into common randomness genera-\ntion protocols with ease. In Section 4, we design protocols to implement RIG as a\nGame-theoretic Randomness for Proof-of-Stake\n5",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "are not guaranteed to be uniformly random, despite the fact that they are assumed to be\nuniform in the security analysis.\nOur Contribution. Our main contributions are as follows:\n– First, we design a novel game-theoretic approach for randomness generation. We\ncall this an RIG (Random Integer Generation) game. RIG efficiently produces a\nuniform random integer from an arbitrarily large interval. Moreover, we show that\nthe only equilibrium in an RIG is for all participants to choose their si uniformly at\nrandom. In other words, our RIG ensures that the participants are incentivized not\nonly to be honest, but also to be reliable. This will alleviate the problems with the\nprevious approaches and ensure that all desired properties of distributed random-\nness are attained.\n– We show that our RIG approach can be plugged into common randomness genera-\ntion protocols with ease. In Section 4, we design protocols to implement RIG as a\nGame-theoretic Randomness for Proof-of-Stake\n5"
        }
    },
    {
        "id": "94e158373f6c67b4b1ee549c7d85efab1ff94b40",
        "content": "can guarantee the delivery of uniformly random output values, if we define the final\noutput as ˜v = v1 + VDF(v2), where v1 is the higher half bits of v and v2 is the lower\nhalf bits of v. Since v is uniformly random, then v1 and v2 are independent uniformly\nrandom integers. Whatever distribution VDF(v2) has, the sum ˜v is a uniformly random\ninteger.\nFinally, note that this approach works as long as at least one of the participants is\nhonest. So, in a proof-of-stake scenario, if we choose n participants for each random\nnumber generation, we need to ensure that honest miners have much more than 1/n\nfraction of the stake in the cryptocurrency, so as to ensure that at least one honest par-\nticipant is chosen with high probability. We also assume that at least one participant is\nreliable, but this is already incentivized by our game.\n4.2\nPVSS approach\nThe drawback of the commitment scheme in random number generators is the possi-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "can guarantee the delivery of uniformly random output values, if we define the final\noutput as ˜v = v1 + VDF(v2), where v1 is the higher half bits of v and v2 is the lower\nhalf bits of v. Since v is uniformly random, then v1 and v2 are independent uniformly\nrandom integers. Whatever distribution VDF(v2) has, the sum ˜v is a uniformly random\ninteger.\nFinally, note that this approach works as long as at least one of the participants is\nhonest. So, in a proof-of-stake scenario, if we choose n participants for each random\nnumber generation, we need to ensure that honest miners have much more than 1/n\nfraction of the stake in the cryptocurrency, so as to ensure that at least one honest par-\nticipant is chosen with high probability. We also assume that at least one participant is\nreliable, but this is already incentivized by our game.\n4.2\nPVSS approach\nThe drawback of the commitment scheme in random number generators is the possi-"
        }
    },
    {
        "id": "575d26a75b6f4692b6288916a12b36ee9dc1d144",
        "content": "Game-theoretic Randomness for Proof-of-Stake⋆\nZhuo Cai[0000−0001−9673−6888] and Amir Goharshady[0000−0003−1702−6584]\nDepartment of Computer Science and Engineering\nHong Kong University of Science and Technology (HKUST)\nClear Water Bay, Hong Kong SAR, China\nzcaiam@connect.ust.hk, goharshady@cse.ust.hk\nAbstract. Many protocols in distributed computing rely on a source of random-\nness, usually called a random beacon, both for their applicability and security.\nThis is especially true for proof-of-stake blockchain protocols in which the next\nminer or set of miners have to be chosen randomly and each party’s likelihood to\nbe selected is in proportion to their stake in the cryptocurrency. The chosen miner\nis then allowed to add a block to the chain.\nCurrent random beacons used in proof-of-stake protocols, such as Ouroboros and\nAlgorand, have two fundamental limitations: Either (i) they rely on pseudoran-\ndomness, e.g. assuming that the output of a hash function is uniform, which is",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "Game-theoretic Randomness for Proof-of-Stake⋆\nZhuo Cai[0000−0001−9673−6888] and Amir Goharshady[0000−0003−1702−6584]\nDepartment of Computer Science and Engineering\nHong Kong University of Science and Technology (HKUST)\nClear Water Bay, Hong Kong SAR, China\nzcaiam@connect.ust.hk, goharshady@cse.ust.hk\nAbstract. Many protocols in distributed computing rely on a source of random-\nness, usually called a random beacon, both for their applicability and security.\nThis is especially true for proof-of-stake blockchain protocols in which the next\nminer or set of miners have to be chosen randomly and each party’s likelihood to\nbe selected is in proportion to their stake in the cryptocurrency. The chosen miner\nis then allowed to add a block to the chain.\nCurrent random beacons used in proof-of-stake protocols, such as Ouroboros and\nAlgorand, have two fundamental limitations: Either (i) they rely on pseudoran-\ndomness, e.g. assuming that the output of a hash function is uniform, which is"
        }
    },
    {
        "id": "c42256c6d46f51dc451d88c3ab49ccda385f0937",
        "content": "a random number si and then performs the protocol above as the dealer. At the end\nof the process, all the si’s are revealed by pooling the shares and we can use v =\nPn\ni=1 si as our random number. The upside is that no party can avoid revealing their si\nand hence the protocol satisfies availability. The downside is that every set of t parties\ncan unmask everyone else’s choices and hence bias the result. Therefore, in random\nnumber generation using PVSS we have to assume that there are at most t−1 dishonest\nparticipants.\n8\nZ. Cai and A. Goharshady\n2.3\nVerifiable Delay Functions\nWe follow [6] in our treatment of verifiable delay functions. A verifiable delay function\n(VDF) is a tuple V = (Setup, Eval, Verify) parameterized by a security parameter\nλ and a desired puzzle difficulty t. Suppose our input space is X and our output space\nis Y. V is a triplet of algorithms as follows:\n– Setup(λ, t) →(ek, vk). This function generates an evaluation key ek and a veri-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "a random number si and then performs the protocol above as the dealer. At the end\nof the process, all the si’s are revealed by pooling the shares and we can use v =\nPn\ni=1 si as our random number. The upside is that no party can avoid revealing their si\nand hence the protocol satisfies availability. The downside is that every set of t parties\ncan unmask everyone else’s choices and hence bias the result. Therefore, in random\nnumber generation using PVSS we have to assume that there are at most t−1 dishonest\nparticipants.\n8\nZ. Cai and A. Goharshady\n2.3\nVerifiable Delay Functions\nWe follow [6] in our treatment of verifiable delay functions. A verifiable delay function\n(VDF) is a tuple V = (Setup, Eval, Verify) parameterized by a security parameter\nλ and a desired puzzle difficulty t. Suppose our input space is X and our output space\nis Y. V is a triplet of algorithms as follows:\n– Setup(λ, t) →(ek, vk). This function generates an evaluation key ek and a veri-"
        }
    },
    {
        "id": "c8f955d22e83c97c8bd2bf63264a5d8df0bc530e",
        "content": "tually controlled by the same person or are in an alliance. Therefore, we need a stronger\nconcept of equilibrium that does not assume a lack of cooperation between any pair of\nplayers. Thus, we rely on strong and alliance-resistant equilibria as defined below.\nStrong Nash Equilibria [4, 5]. A strong Nash equilibrium is a mixed strategy profile in\nwhich no group of players have a way to cooperate and change their mixed strategies\nsuch that the utility of every member of the group is increased. Formally, σ is a strong\nNash equilibrium if for any non-empty set P of players and any strategy profile ˜σP over\nP, there exists a player p ∈P such that up(σ) ≥up(˜σP , σ−P ). In strong equilibria,\nthe assumption is that the players cannot share or transfer their utilities, so a player\nagrees to a change of strategies in the alliance P if and only if their own utility is\nstrictly increased. However, if the players can share and redistribute utilities, or if they",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "tually controlled by the same person or are in an alliance. Therefore, we need a stronger\nconcept of equilibrium that does not assume a lack of cooperation between any pair of\nplayers. Thus, we rely on strong and alliance-resistant equilibria as defined below.\nStrong Nash Equilibria [4, 5]. A strong Nash equilibrium is a mixed strategy profile in\nwhich no group of players have a way to cooperate and change their mixed strategies\nsuch that the utility of every member of the group is increased. Formally, σ is a strong\nNash equilibrium if for any non-empty set P of players and any strategy profile ˜σP over\nP, there exists a player p ∈P such that up(σ) ≥up(˜σP , σ−P ). In strong equilibria,\nthe assumption is that the players cannot share or transfer their utilities, so a player\nagrees to a change of strategies in the alliance P if and only if their own utility is\nstrictly increased. However, if the players can share and redistribute utilities, or if they"
        }
    },
    {
        "id": "ccc31251be6204da2a1a644a63607f457be5c32b",
        "content": "6\nZ. Cai and A. Goharshady\nthe central concept of stability and self-enforceability for non-cooperative games [25],\nespecially in the context of game-theoretic analysis of blockchain protocols [18, 9, 12,\n16, 15], in which each player maximizes their own utility, i.e. when a game is in a Nash\nequilibrium, no party has an incentive to change their strategy and hence the game\nremains in the Nash equilibrium.\nIn distributed randomness generation, especially for proof-of-stake protocols, we\naim to have a committee that plays a game whose output is our random number. Since\nthe players/parties are pseudonymous on a blockchain network and only participate\nusing their public keys, we might have multiple accounts in our committee that are ac-\ntually controlled by the same person or are in an alliance. Therefore, we need a stronger\nconcept of equilibrium that does not assume a lack of cooperation between any pair of",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "6\nZ. Cai and A. Goharshady\nthe central concept of stability and self-enforceability for non-cooperative games [25],\nespecially in the context of game-theoretic analysis of blockchain protocols [18, 9, 12,\n16, 15], in which each player maximizes their own utility, i.e. when a game is in a Nash\nequilibrium, no party has an incentive to change their strategy and hence the game\nremains in the Nash equilibrium.\nIn distributed randomness generation, especially for proof-of-stake protocols, we\naim to have a committee that plays a game whose output is our random number. Since\nthe players/parties are pseudonymous on a blockchain network and only participate\nusing their public keys, we might have multiple accounts in our committee that are ac-\ntually controlled by the same person or are in an alliance. Therefore, we need a stronger\nconcept of equilibrium that does not assume a lack of cooperation between any pair of"
        }
    },
    {
        "id": "4ea83447a488b98ea81c1e6c2aceb1de3c9134d6",
        "content": "block is proportional to their computational (hash) power. Security guarantees are then\nproven with the assumption that more than half of computational power is in the hands\nof honest miners. Proof of work is known to be highly energy-inefficient [2, 10] and\nalso prone to centralization due to large mining pools [3]. Currently, the three largest\nmining pools have more than half of the entire Bitcoin mining power.\nProof of Stake [19]. Proof of Stake (PoS) is the main alternative consensus mechanism\nproposed to replace PoW in blockchain protocols. In a PoS protocol, miners are chosen\nrandomly and each miner’s chance of being allowed to add the next block is normally\nproportional to their stake in the currency. Hence, instead of relying on the assumption\nthat a majority of the computational power on the network is owned by honest par-\nticipants, the security claims of proof-of-stake protocols rely on the assumption that a",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "block is proportional to their computational (hash) power. Security guarantees are then\nproven with the assumption that more than half of computational power is in the hands\nof honest miners. Proof of work is known to be highly energy-inefficient [2, 10] and\nalso prone to centralization due to large mining pools [3]. Currently, the three largest\nmining pools have more than half of the entire Bitcoin mining power.\nProof of Stake [19]. Proof of Stake (PoS) is the main alternative consensus mechanism\nproposed to replace PoW in blockchain protocols. In a PoS protocol, miners are chosen\nrandomly and each miner’s chance of being allowed to add the next block is normally\nproportional to their stake in the currency. Hence, instead of relying on the assumption\nthat a majority of the computational power on the network is owned by honest par-\nticipants, the security claims of proof-of-stake protocols rely on the assumption that a"
        }
    },
    {
        "id": "c740dba67af6acd9304af17ab37dc4664eaed84e",
        "content": "synchronous proof-of-stake blockchain. In: CRYPTO. pp. 66–98 (2018)\n14. Dodis, Y., Yampolskiy, A.: A verifiable random function with short proofs and keys. In: PKC.\npp. 416–431 (2005)\n15. Farokhnia, S., Goharshady, A.: Alleviating high gas costs by secure and trustless off-chain\nexecution of smart contracts. In: SAC (2023)\n16. Farokhnia, S., Goharshady, A.: Reducing the gas usage of Ethereum smart contracts without\na sidechain. In: IEEE ICBC (2023)\n17. Gilad, Y., Hemo, R., Micali, S., Vlachos, G., Zeldovich, N.: Algorand: Scaling Byzantine\nagreements for cryptocurrencies. In: SOSP. pp. 51–68 (2017)\n18. Goharshady, A.K.: Irrationality, extortion, or trusted third-parties: Why it is impossible to\nbuy and sell physical goods securely on the blockchain. In: IEEE Blockchain. pp. 73–81\n(2021)\n19. Kiayias, A., Russell, A., David, B., Oliynykov, R.: Ouroboros: A provably secure proof-of-\nstake blockchain protocol. In: CRYPTO. pp. 357–388 (2017)",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "synchronous proof-of-stake blockchain. In: CRYPTO. pp. 66–98 (2018)\n14. Dodis, Y., Yampolskiy, A.: A verifiable random function with short proofs and keys. In: PKC.\npp. 416–431 (2005)\n15. Farokhnia, S., Goharshady, A.: Alleviating high gas costs by secure and trustless off-chain\nexecution of smart contracts. In: SAC (2023)\n16. Farokhnia, S., Goharshady, A.: Reducing the gas usage of Ethereum smart contracts without\na sidechain. In: IEEE ICBC (2023)\n17. Gilad, Y., Hemo, R., Micali, S., Vlachos, G., Zeldovich, N.: Algorand: Scaling Byzantine\nagreements for cryptocurrencies. In: SOSP. pp. 51–68 (2017)\n18. Goharshady, A.K.: Irrationality, extortion, or trusted third-parties: Why it is impossible to\nbuy and sell physical goods securely on the blockchain. In: IEEE Blockchain. pp. 73–81\n(2021)\n19. Kiayias, A., Russell, A., David, B., Oliynykov, R.: Ouroboros: A provably secure proof-of-\nstake blockchain protocol. In: CRYPTO. pp. 357–388 (2017)"
        }
    },
    {
        "id": "edd9eafe65c593ba0329c72b6d67d4c761735c8f",
        "content": "strategy profile, the expected payoff of each one of the players is 0. Let G be a sub-\nset of players, then the overall utility of all players in G is P\ni∈G ui(¯σ−G, σ) if play-\ners in G play another strategy profile σ. Each player i is effectively playing against\nits adjacent player. If both player i and player pair(i) are in G, then ui(¯σ−G, σ) =\n−upair(i)(¯σ−G, σ). The utilities of these two players always sum up to zero, so that\nthe overall utility of G is not influenced by them. Similarly, if both player i and player\npair(i) are not in G, they do not influence the overall utility of G either. The only\nnon-trivial part consists of those players in G who play against players outside G. For\neach such player i, since the player pair(i) plays mixed strategy ¯σpair(i), the utility is\nui = σT\ni · A(m) · ¯σpair(i) = σT\ni · (0, 0, . . . , 0) = 0. Therefore, the overall utility of G is\n0 and changing the strategy has no benefit.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "strategy profile, the expected payoff of each one of the players is 0. Let G be a sub-\nset of players, then the overall utility of all players in G is P\ni∈G ui(¯σ−G, σ) if play-\ners in G play another strategy profile σ. Each player i is effectively playing against\nits adjacent player. If both player i and player pair(i) are in G, then ui(¯σ−G, σ) =\n−upair(i)(¯σ−G, σ). The utilities of these two players always sum up to zero, so that\nthe overall utility of G is not influenced by them. Similarly, if both player i and player\npair(i) are not in G, they do not influence the overall utility of G either. The only\nnon-trivial part consists of those players in G who play against players outside G. For\neach such player i, since the player pair(i) plays mixed strategy ¯σpair(i), the utility is\nui = σT\ni · A(m) · ¯σpair(i) = σT\ni · (0, 0, . . . , 0) = 0. Therefore, the overall utility of G is\n0 and changing the strategy has no benefit."
        }
    },
    {
        "id": "b0d49ec7495b49fed02294446d8548a09683d79d",
        "content": "Using this technique, any adversary cannot have any information about the final\noutput by the end of the reveal phase. Therefore, revealing values honestly is always a\nstrictly better strategy than not revealing values for all participants. The game is then\nexecuted when all the values are revealed and all the VDFs are evaluated and it is\nensured that cheating participants are excluded.\nNote that, even if v conforms to a uniformly random distribution, the output of VDF\non v is not guaranteed to be uniformly random. In existing constructions of random\nGame-theoretic Randomness for Proof-of-Stake\n13\nbeacons that rely on VDF, a hash function is applied to the output of VDF to get random\nnumbers, under the random oracle assumption. However, with the novel RIG game, we\ncan guarantee the delivery of uniformly random output values, if we define the final\noutput as ˜v = v1 + VDF(v2), where v1 is the higher half bits of v and v2 is the lower",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "Using this technique, any adversary cannot have any information about the final\noutput by the end of the reveal phase. Therefore, revealing values honestly is always a\nstrictly better strategy than not revealing values for all participants. The game is then\nexecuted when all the values are revealed and all the VDFs are evaluated and it is\nensured that cheating participants are excluded.\nNote that, even if v conforms to a uniformly random distribution, the output of VDF\non v is not guaranteed to be uniformly random. In existing constructions of random\nGame-theoretic Randomness for Proof-of-Stake\n13\nbeacons that rely on VDF, a hash function is applied to the output of VDF to get random\nnumbers, under the random oracle assumption. However, with the novel RIG game, we\ncan guarantee the delivery of uniformly random output values, if we define the final\noutput as ˜v = v1 + VDF(v2), where v1 is the higher half bits of v and v2 is the lower"
        }
    },
    {
        "id": "c1f9bdee163d5d53b8230cb33444ecef3bce3e4b",
        "content": "1\nIntroduction\nProof of Work. Bitcoin, the first blockchain protocol, was proposed by Satoshi Nakamoto\nto achieve consensus in a decentralized peer-to-peer electronic payment system [23]. In\n⋆The research was partially supported by the Hong Kong Research Grants Council ECS Project\nNumber 26208122, the HKUST-Kaisa Joint Research Institute Project Grant HKJRI3A-055\nand the HKUST Startup Grant R9272. Z. Cai was supported by the Hong Kong PhD Fellow-\nship Scheme (HKPFS). Authors are ordered alphabetically.\n2\nZ. Cai and A. Goharshady\nBitcoin and many other cryptocurrencies, the miners are selected by a proof-of-work\n(PoW) mechanism to add blocks of transactions to the public ledger [21], i.e. they have\nto compete in solving a mathematical puzzle and each miner’s chance of adding the next\nblock is proportional to their computational (hash) power. Security guarantees are then\nproven with the assumption that more than half of computational power is in the hands",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "1\nIntroduction\nProof of Work. Bitcoin, the first blockchain protocol, was proposed by Satoshi Nakamoto\nto achieve consensus in a decentralized peer-to-peer electronic payment system [23]. In\n⋆The research was partially supported by the Hong Kong Research Grants Council ECS Project\nNumber 26208122, the HKUST-Kaisa Joint Research Institute Project Grant HKJRI3A-055\nand the HKUST Startup Grant R9272. Z. Cai was supported by the Hong Kong PhD Fellow-\nship Scheme (HKPFS). Authors are ordered alphabetically.\n2\nZ. Cai and A. Goharshady\nBitcoin and many other cryptocurrencies, the miners are selected by a proof-of-work\n(PoW) mechanism to add blocks of transactions to the public ledger [21], i.e. they have\nto compete in solving a mathematical puzzle and each miner’s chance of adding the next\nblock is proportional to their computational (hash) power. Security guarantees are then\nproven with the assumption that more than half of computational power is in the hands"
        }
    },
    {
        "id": "578aaf4a7c6275728cc22b18a5264e811a3a5eee",
        "content": "with different levels of synchrony. In this paper, we rely on blockchain consensus\nprotocols to achieve a synchronized view of RIG execution. In detail, we require ∆-\nsynchrony for blockchains:\n∆-synchronous blockchains. A blockchain is ∆-synchronous if any valid transaction\nbroadcasted by some user at time t will become part of the stable chain at time t + ∆\nin the view of all honest nodes.\nWe assume that ∆is known to all nodes and use it to design the duration of com-\nmitment scheme approach and PVSS approach. If the PVSS approach is implemented\nusing off-chain communication, then we can design durations in terms of δ. In any case,\nthe approach will not work if the network/blockchain is not synchronous or if the time\nlimits are too tight and messages are not guaranteed to be delivered before the beginning\nof the next phase.\nRationality Assumption. We proved that any rational party or parties, i.e. a party/parties",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "with different levels of synchrony. In this paper, we rely on blockchain consensus\nprotocols to achieve a synchronized view of RIG execution. In detail, we require ∆-\nsynchrony for blockchains:\n∆-synchronous blockchains. A blockchain is ∆-synchronous if any valid transaction\nbroadcasted by some user at time t will become part of the stable chain at time t + ∆\nin the view of all honest nodes.\nWe assume that ∆is known to all nodes and use it to design the duration of com-\nmitment scheme approach and PVSS approach. If the PVSS approach is implemented\nusing off-chain communication, then we can design durations in terms of δ. In any case,\nthe approach will not work if the network/blockchain is not synchronous or if the time\nlimits are too tight and messages are not guaranteed to be delivered before the beginning\nof the next phase.\nRationality Assumption. We proved that any rational party or parties, i.e. a party/parties"
        }
    },
    {
        "id": "5b8a9ca0ab0ae8718d255d91597fb80932c0d0d1",
        "content": "this group are selected using an appropriate public procedure. Here, q is a large prime\nnumber and all calculations are done modulo q. Each participant Pi generates a non-\nzero private key xi ∈Z∗\nq and announces yi = Gxi as their public key. Suppose the\nsecret to be shared is s, the dealer first chooses a random number r and publishes U =\ns + h(Gr), where h is a pre-selected cryptographic hash function. The dealer then runs\nGame-theoretic Randomness for Proof-of-Stake\n7\nthe main protocol below to distribute the shares that can reveal Gr. The main protocol\nconsists of two steps: (1) distribution, and (2) reconstruction, each of which has two\nsubsteps.\nDistribution. This consists of the following:\n– Distribution of the shares. The dealer picks a random polynomial p of degree at\nmost t −1 with coefficients in Zq of the form p(x) = Pt−1\nj=0 αj · xj. Here, we have\nα0 = Gr, i.e. the number r is encoded in the first coefficient of the polynomial",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "this group are selected using an appropriate public procedure. Here, q is a large prime\nnumber and all calculations are done modulo q. Each participant Pi generates a non-\nzero private key xi ∈Z∗\nq and announces yi = Gxi as their public key. Suppose the\nsecret to be shared is s, the dealer first chooses a random number r and publishes U =\ns + h(Gr), where h is a pre-selected cryptographic hash function. The dealer then runs\nGame-theoretic Randomness for Proof-of-Stake\n7\nthe main protocol below to distribute the shares that can reveal Gr. The main protocol\nconsists of two steps: (1) distribution, and (2) reconstruction, each of which has two\nsubsteps.\nDistribution. This consists of the following:\n– Distribution of the shares. The dealer picks a random polynomial p of degree at\nmost t −1 with coefficients in Zq of the form p(x) = Pt−1\nj=0 αj · xj. Here, we have\nα0 = Gr, i.e. the number r is encoded in the first coefficient of the polynomial"
        }
    },
    {
        "id": "686f954a896f52cfb10e2bb708cc7b0acaa86b0a",
        "content": "ui = σT\ni · A(m) · ¯σpair(i) = σT\ni · (0, 0, . . . , 0) = 0. Therefore, the overall utility of G is\n0 and changing the strategy has no benefit.\nWe now prove that ¯σ is the unique Nash equilibrium of this game. Suppose there is\nanother strategy profile ˜σ that is also a Nash equilibrium. Then for any player i, since it\nis effectively only playing with its adjacent player j = pair(i), it follows that (˜σi, ˜σj)\nforms a Nash equilibrium for the zero-sum bimatrix game defined by A(m).\nNow consider the bimatrix game between player i and player j. Let their utility\nat Nash equilibrium mixed strategy profile (˜σi, ˜σj) be (˜ui, ˜uj). Since it is a zero-sum\nmatrix game, ˜ui+˜uj = 0. Without loss of generality, assume that ˜ui ≤˜uj, then ˜ui ≤0.\n10\nZ. Cai and A. Goharshady\nBy the definition of Nash equilibrium, player i cannot increase its utility by changing\nits strategy ˜σi to any other strategy σi, while player j keeps playing the same strategy",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "ui = σT\ni · A(m) · ¯σpair(i) = σT\ni · (0, 0, . . . , 0) = 0. Therefore, the overall utility of G is\n0 and changing the strategy has no benefit.\nWe now prove that ¯σ is the unique Nash equilibrium of this game. Suppose there is\nanother strategy profile ˜σ that is also a Nash equilibrium. Then for any player i, since it\nis effectively only playing with its adjacent player j = pair(i), it follows that (˜σi, ˜σj)\nforms a Nash equilibrium for the zero-sum bimatrix game defined by A(m).\nNow consider the bimatrix game between player i and player j. Let their utility\nat Nash equilibrium mixed strategy profile (˜σi, ˜σj) be (˜ui, ˜uj). Since it is a zero-sum\nmatrix game, ˜ui+˜uj = 0. Without loss of generality, assume that ˜ui ≤˜uj, then ˜ui ≤0.\n10\nZ. Cai and A. Goharshady\nBy the definition of Nash equilibrium, player i cannot increase its utility by changing\nits strategy ˜σi to any other strategy σi, while player j keeps playing the same strategy"
        }
    },
    {
        "id": "7cff9858a02870b8186ce8ee10b27cc69d49e808",
        "content": "scheme does not ensure availability, since malicious parties might only commit but not\nreveal their values.\nPVSS. Publicly verifiable secret sharing (PVSS) is a powerful cryptographic tool to\nensure the revelation of values si even if a number of malicious parties stop participat-\ning in the reveal phase of a commitment scheme [27]. PVSS adds a protection layer to\ntraditional secret sharing schemes in the presence of malicious participants. In a PVSS\nscheme, a dealer is required to provide a non-interactive zero-knowledge proof (NIZK)\nalong with encrypted secret shares Ei(si) to guarantee the validity of secret shares.\nDuring the reconstruction phase, a participant sends their secret share to other partici-\npants along with an NIZK proof to guarantee the correctness of secret share. The NIZK\nproofs can be verified by any party, including third parties who are not taking part in\nthe PVSS scheme.\nRANDAO [1] and EVR [30]. RANDAO is a family of smart contracts that produce",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "scheme does not ensure availability, since malicious parties might only commit but not\nreveal their values.\nPVSS. Publicly verifiable secret sharing (PVSS) is a powerful cryptographic tool to\nensure the revelation of values si even if a number of malicious parties stop participat-\ning in the reveal phase of a commitment scheme [27]. PVSS adds a protection layer to\ntraditional secret sharing schemes in the presence of malicious participants. In a PVSS\nscheme, a dealer is required to provide a non-interactive zero-knowledge proof (NIZK)\nalong with encrypted secret shares Ei(si) to guarantee the validity of secret shares.\nDuring the reconstruction phase, a participant sends their secret share to other partici-\npants along with an NIZK proof to guarantee the correctness of secret share. The NIZK\nproofs can be verified by any party, including third parties who are not taking part in\nthe PVSS scheme.\nRANDAO [1] and EVR [30]. RANDAO is a family of smart contracts that produce"
        }
    },
    {
        "id": "291df093c88e65ac1eb5151c6a17ea459f5278c9",
        "content": "lying distribution δ, which is usually the uniform distribution. No party should be\nable to bias the output or change the distribution δ.\n– Unpredictability: No party should be able to predict the output before it is publi-\ncized. Moreover, no party should even be able to have any extra information about\nthe output, other than the fact that it will be sampled from δ.\n– Availability: Each execution of the beacon must successfully terminate and produce\na random value.\n– Verifiability: Each execution of the beacon should provide a “proof” such that any\nthird party, even if not involved in the random beacon, is able to verify both the\noutput and the fact that the random beacon executed successfully.\nReliable Participants. Almost all distributed randomness protocols have several partic-\nipants and create a random output based on random numbers submitted by participants\nof the protocol. Usually, the final value is simply defined by the modular sum of all input",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "lying distribution δ, which is usually the uniform distribution. No party should be\nable to bias the output or change the distribution δ.\n– Unpredictability: No party should be able to predict the output before it is publi-\ncized. Moreover, no party should even be able to have any extra information about\nthe output, other than the fact that it will be sampled from δ.\n– Availability: Each execution of the beacon must successfully terminate and produce\na random value.\n– Verifiability: Each execution of the beacon should provide a “proof” such that any\nthird party, even if not involved in the random beacon, is able to verify both the\noutput and the fact that the random beacon executed successfully.\nReliable Participants. Almost all distributed randomness protocols have several partic-\nipants and create a random output based on random numbers submitted by participants\nof the protocol. Usually, the final value is simply defined by the modular sum of all input"
        }
    },
    {
        "id": "4be04bce953bc298488bdf6d9cff5f7ded1e02d1",
        "content": "using parallelization. Even if a malicious participant has strong parallel computational\npower, they cannot evaluate the VDF significantly faster than an ordinary participant\nthat owns only a single processor. However, after the evaluation is done, verifying the\nresult is easy and much faster and anyone can do the verification using the verification\nkey vk.\nThe use-case of verifiable delay functions in random number generation is to again\ndefend against dishonest participants who do not reveal their choice in a commitment\nscheme. We can require every participant to provide a VDF whose evaluation is their\nchoice si. Then, even if the participant is dishonest and does not reveal their own choice,\nother participants can evaluate the VDF and obtain the si, hence ensuring availability\nfor our random number generation protocol. However, evaluation takes a long time, and\nhence the choice will not be revealed while in the commit phase.",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "using parallelization. Even if a malicious participant has strong parallel computational\npower, they cannot evaluate the VDF significantly faster than an ordinary participant\nthat owns only a single processor. However, after the evaluation is done, verifying the\nresult is easy and much faster and anyone can do the verification using the verification\nkey vk.\nThe use-case of verifiable delay functions in random number generation is to again\ndefend against dishonest participants who do not reveal their choice in a commitment\nscheme. We can require every participant to provide a VDF whose evaluation is their\nchoice si. Then, even if the participant is dishonest and does not reveal their own choice,\nother participants can evaluate the VDF and obtain the si, hence ensuring availability\nfor our random number generation protocol. However, evaluation takes a long time, and\nhence the choice will not be revealed while in the commit phase."
        }
    },
    {
        "id": "37f7b529a02313f38f9229abca42a3083414872d",
        "content": "use the output of the previous RIG round to generate a random ordering for the current\nround. Finally, the RIG game requires an even number of participants, so if the number\nof valid participants is odd, we will remove one participant arbitrarily. To make sure\nthis does not have an effect on consensus, we can remove the participant for whom\nh(commit message) has the largest value.\nDesign of the Incentives. Every participant puts down a deposit d at the same time they\nsend their commit message. The value of d is fixed by the protocol. After collecting\nall the valid values si and ordering of the participants Pi, i ∈[1, n], the result has\nthe format (s, {Pi, ui}), where ui is the payoff of participant Pi. The values s, si are\nin {0, 1, . . . , m}, where m is a parameter of RIG game, i.e. the number of strategies\nfor each player. The output random number is computed as s = Pn\ni=1 si (mod m).\nNote that all dishonest players are excluded from the sum. If a player does not release",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "use the output of the previous RIG round to generate a random ordering for the current\nround. Finally, the RIG game requires an even number of participants, so if the number\nof valid participants is odd, we will remove one participant arbitrarily. To make sure\nthis does not have an effect on consensus, we can remove the participant for whom\nh(commit message) has the largest value.\nDesign of the Incentives. Every participant puts down a deposit d at the same time they\nsend their commit message. The value of d is fixed by the protocol. After collecting\nall the valid values si and ordering of the participants Pi, i ∈[1, n], the result has\nthe format (s, {Pi, ui}), where ui is the payoff of participant Pi. The values s, si are\nin {0, 1, . . . , m}, where m is a parameter of RIG game, i.e. the number of strategies\nfor each player. The output random number is computed as s = Pn\ni=1 si (mod m).\nNote that all dishonest players are excluded from the sum. If a player does not release"
        }
    },
    {
        "id": "4e34683b6fc7031c6df80dc7905b55f76b88908c",
        "content": "which requires r = 0 and ˜ui = 0. By r = 0, we have\nrs =\nX\n0≤l≤f−1\npl+s −\nX\nm−f≤l≤m−1\npl+s = 0\nfor every s ∈{0, 1, . . . , m −1}. With a slight misuse of notation, assume that pm+t =\npt for any integer t. If we subtract rs+1 by rs, we get\nps+f −ps = ps+m −ps+m−f = ps −ps−f\nLet q(s) = ps −ps−f, then q(s + f) = q(s). We also have q(s + m) = q(s). Since\ngcd(f, m) = 1, the function q(·) is constant on integers, from which we can infer that\np0 = p1 = · · · = pm−1 = 1/m. Therefore, ¯σ is still the only Nash equilibrium.\nImportant Remark on Parallelization. Note that the parallelization of RIG loses the\nuniqueness property of Nash equilibrium, hence we cannot simply have an RIG game\nfor m = 2, use it to generate a random bit, and parallelize it k times to generate k\nrandom bits. Instead, we must set m = 2k and have a single non-parallel RIG game. As\nan example, consider the simplified case of two players and k bits. If each player only",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "which requires r = 0 and ˜ui = 0. By r = 0, we have\nrs =\nX\n0≤l≤f−1\npl+s −\nX\nm−f≤l≤m−1\npl+s = 0\nfor every s ∈{0, 1, . . . , m −1}. With a slight misuse of notation, assume that pm+t =\npt for any integer t. If we subtract rs+1 by rs, we get\nps+f −ps = ps+m −ps+m−f = ps −ps−f\nLet q(s) = ps −ps−f, then q(s + f) = q(s). We also have q(s + m) = q(s). Since\ngcd(f, m) = 1, the function q(·) is constant on integers, from which we can infer that\np0 = p1 = · · · = pm−1 = 1/m. Therefore, ¯σ is still the only Nash equilibrium.\nImportant Remark on Parallelization. Note that the parallelization of RIG loses the\nuniqueness property of Nash equilibrium, hence we cannot simply have an RIG game\nfor m = 2, use it to generate a random bit, and parallelize it k times to generate k\nrandom bits. Instead, we must set m = 2k and have a single non-parallel RIG game. As\nan example, consider the simplified case of two players and k bits. If each player only"
        }
    },
    {
        "id": "e4b8112fee4b656e2245a7ef9a0fb6d27636d08b",
        "content": "8. Castro, M., Liskov, B.: Practical byzantine fault tolerance. In: OSDI. pp. 173–186 (1999)\n9. Chatterjee, K., Goharshady, A.K., Ibsen-Jensen, R., Velner, Y.: Ergodic mean-payoff games\nfor the analysis of attacks in crypto-currencies. In: CONCUR (2018)\n10. Chatterjee,\nK.,\nGoharshady,\nA.K.,\nPourdamghani,\nA.:\nHybrid\nmining:\nExploiting\nblockchain’s computational power for distributed problem solving. In: SAC. pp. 374–381\n(2019)\n11. Chatterjee, K., Goharshady, A.K., Pourdamghani, A.: Probabilistic smart contracts: Secure\nrandomness on the blockchain. In: IEEE ICBC. pp. 403–412 (2019)\n12. Chatterjee, K., Goharshady, A.K., Velner, Y.: Quantitative analysis of smart contracts. In:\nESOP. pp. 739–767 (2018)\n13. David, B., Gaži, P., Kiayias, A., Russell, A.: Ouroboros praos: An adaptively-secure, semi-\nsynchronous proof-of-stake blockchain. In: CRYPTO. pp. 66–98 (2018)\n14. Dodis, Y., Yampolskiy, A.: A verifiable random function with short proofs and keys. In: PKC.\npp. 416–431 (2005)",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "8. Castro, M., Liskov, B.: Practical byzantine fault tolerance. In: OSDI. pp. 173–186 (1999)\n9. Chatterjee, K., Goharshady, A.K., Ibsen-Jensen, R., Velner, Y.: Ergodic mean-payoff games\nfor the analysis of attacks in crypto-currencies. In: CONCUR (2018)\n10. Chatterjee,\nK.,\nGoharshady,\nA.K.,\nPourdamghani,\nA.:\nHybrid\nmining:\nExploiting\nblockchain’s computational power for distributed problem solving. In: SAC. pp. 374–381\n(2019)\n11. Chatterjee, K., Goharshady, A.K., Pourdamghani, A.: Probabilistic smart contracts: Secure\nrandomness on the blockchain. In: IEEE ICBC. pp. 403–412 (2019)\n12. Chatterjee, K., Goharshady, A.K., Velner, Y.: Quantitative analysis of smart contracts. In:\nESOP. pp. 739–767 (2018)\n13. David, B., Gaži, P., Kiayias, A., Russell, A.: Ouroboros praos: An adaptively-secure, semi-\nsynchronous proof-of-stake blockchain. In: CRYPTO. pp. 66–98 (2018)\n14. Dodis, Y., Yampolskiy, A.: A verifiable random function with short proofs and keys. In: PKC.\npp. 416–431 (2005)"
        }
    },
    {
        "id": "408011e2f87468643deb26249ede0cd43cb3d3de",
        "content": "egy for each player. The expected utility ui(σ) of player i in a mixed strategy profile\nσ is defined as ui(σ) = Esi∼σi[ui(s1, s2, . . . , sn)]. Intuitively, in a mixed strategy, the\nplayer is not committing to a single pure strategy, but only to the probability of playing\neach pure strategy.\nNash Equilibria [24]. A Nash Equilibrium of a game G is a mixed strategy profile σ,\nsuch that no player has an incentive to change their mixed strategy σi, assuming they\nare aware of the mixed strategies played by all the other players. Let σ−i be a tuple\nconsisting of all the mixed strategies in σ except σi. Formally, σ is a Nash equilibrium\nif and only if for all ˜σi ∈∆(Si) we have ui(σ) ≥ui(˜σi, σ−i). A seminal result\nby Nash is that every finite game G has a Nash equilibrium [24]. Nash equilibria are\n6\nZ. Cai and A. Goharshady\nthe central concept of stability and self-enforceability for non-cooperative games [25],",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "egy for each player. The expected utility ui(σ) of player i in a mixed strategy profile\nσ is defined as ui(σ) = Esi∼σi[ui(s1, s2, . . . , sn)]. Intuitively, in a mixed strategy, the\nplayer is not committing to a single pure strategy, but only to the probability of playing\neach pure strategy.\nNash Equilibria [24]. A Nash Equilibrium of a game G is a mixed strategy profile σ,\nsuch that no player has an incentive to change their mixed strategy σi, assuming they\nare aware of the mixed strategies played by all the other players. Let σ−i be a tuple\nconsisting of all the mixed strategies in σ except σi. Formally, σ is a Nash equilibrium\nif and only if for all ˜σi ∈∆(Si) we have ui(σ) ≥ui(˜σi, σ−i). A seminal result\nby Nash is that every finite game G has a Nash equilibrium [24]. Nash equilibria are\n6\nZ. Cai and A. Goharshady\nthe central concept of stability and self-enforceability for non-cooperative games [25],"
        }
    },
    {
        "id": "3f76108c3f859d5e20d95516184c4d27188f6082",
        "content": "References\n1. RANDAO: A DAO working as RNG of Ethereum (2019), https://github.com/randao/randao\n2. Cambridge bitcoin electricity consumption index (2022), https://ccaf.io/cbeci/index\n3. Arnosti, N., Weinberg, S.M.: Bitcoin: A natural oligopoly. Management Science pp. 4755–\n4771 (2022)\n4. Aumann, R.J.: 16. Acceptable Points in General Cooperative n-Person Games, pp. 287–324.\nPrinceton University Press (2016)\n5. Bernheim, B., Peleg, B., Whinston, M.D.: Coalition-proof nash equilibria i. concepts. Journal\nof Economic Theory pp. 1–12 (1987)\n6. Boneh, D., Bonneau, J., Bünz, B., Fisch, B.: Verifiable delay functions. In: CRYPTO. pp.\n757–788 (2018)\n18\nZ. Cai and A. Goharshady\n7. Cai, Z., Goharshady, A.: Trustless and bias-resistant game-theoretic distributed randomness.\nIn: IEEE ICBC (2023)\n8. Castro, M., Liskov, B.: Practical byzantine fault tolerance. In: OSDI. pp. 173–186 (1999)\n9. Chatterjee, K., Goharshady, A.K., Ibsen-Jensen, R., Velner, Y.: Ergodic mean-payoff games",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "References\n1. RANDAO: A DAO working as RNG of Ethereum (2019), https://github.com/randao/randao\n2. Cambridge bitcoin electricity consumption index (2022), https://ccaf.io/cbeci/index\n3. Arnosti, N., Weinberg, S.M.: Bitcoin: A natural oligopoly. Management Science pp. 4755–\n4771 (2022)\n4. Aumann, R.J.: 16. Acceptable Points in General Cooperative n-Person Games, pp. 287–324.\nPrinceton University Press (2016)\n5. Bernheim, B., Peleg, B., Whinston, M.D.: Coalition-proof nash equilibria i. concepts. Journal\nof Economic Theory pp. 1–12 (1987)\n6. Boneh, D., Bonneau, J., Bünz, B., Fisch, B.: Verifiable delay functions. In: CRYPTO. pp.\n757–788 (2018)\n18\nZ. Cai and A. Goharshady\n7. Cai, Z., Goharshady, A.: Trustless and bias-resistant game-theoretic distributed randomness.\nIn: IEEE ICBC (2023)\n8. Castro, M., Liskov, B.: Practical byzantine fault tolerance. In: OSDI. pp. 173–186 (1999)\n9. Chatterjee, K., Goharshady, A.K., Ibsen-Jensen, R., Velner, Y.: Ergodic mean-payoff games"
        }
    },
    {
        "id": "c8941ea0889205172f5c391c6ab076b571bcf47f",
        "content": "the participants cannot broadcast their values in a distributed network in a perfectly si-\nmultaneous way, the last participant has an advantage and can dominate the final output.\nThe classical cryptographic primitive used to avoid such a scenario is a commitment\nscheme. A commitment scheme runs in two phases: a commit phase and a reveal phase.\nIn the commit phase, instead of broadcasting the value si directly, each party pi broad-\ncasts h(si, ri), where h is a cryptographic hash function and ri is a randomly chosen\nnonce. In the reveal phase, each party broadcasts the values of si and ri and everyone\non the network can verify that the broadcast values have the right hash and thus the\nparty has not changed their choice si since the commit phase. However, a commitment\nscheme does not ensure availability, since malicious parties might only commit but not\nreveal their values.\nPVSS. Publicly verifiable secret sharing (PVSS) is a powerful cryptographic tool to",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "the participants cannot broadcast their values in a distributed network in a perfectly si-\nmultaneous way, the last participant has an advantage and can dominate the final output.\nThe classical cryptographic primitive used to avoid such a scenario is a commitment\nscheme. A commitment scheme runs in two phases: a commit phase and a reveal phase.\nIn the commit phase, instead of broadcasting the value si directly, each party pi broad-\ncasts h(si, ri), where h is a cryptographic hash function and ri is a randomly chosen\nnonce. In the reveal phase, each party broadcasts the values of si and ri and everyone\non the network can verify that the broadcast values have the right hash and thus the\nparty has not changed their choice si since the commit phase. However, a commitment\nscheme does not ensure availability, since malicious parties might only commit but not\nreveal their values.\nPVSS. Publicly verifiable secret sharing (PVSS) is a powerful cryptographic tool to"
        }
    },
    {
        "id": "57b1bb11558a219a6d48208fec78ae8f9856d464",
        "content": "the model to gain general syntactic and semantic understanding of the text corpus and then be trained\non task-specific objectives to adapt to various tasks. The final and most recent stage of LM is the Large\nLanguage Models (LLMs), and will be the focus of this paper. Motivated by the observation that scaling\n5",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the model to gain general syntactic and semantic understanding of the text corpus and then be trained\non task-specific objectives to adapt to various tasks. The final and most recent stage of LM is the Large\nLanguage Models (LLMs), and will be the focus of this paper. Motivated by the observation that scaling\n5"
        }
    },
    {
        "id": "2878778a4643977f80c1d9c6c2f2fe291d4e8b55",
        "content": "This model does not require fine-tuning LLM on specific datasets. Model accuracy and ranking are better\nthan XGBoost when the number of samples is small. The model with target augmentation performs notice-\nably better than the model without augmentation. It does not perform well when there are too many columns\nor fewer representative features. GTL (Zhang et al., 2023a) fine-tunes LLaMA to predict the next token",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "This model does not require fine-tuning LLM on specific datasets. Model accuracy and ranking are better\nthan XGBoost when the number of samples is small. The model with target augmentation performs notice-\nably better than the model without augmentation. It does not perform well when there are too many columns\nor fewer representative features. GTL (Zhang et al., 2023a) fine-tunes LLaMA to predict the next token"
        }
    },
    {
        "id": "c8a74c83b8cf8047e0e8ed44d3f83eced7ae16cc",
        "content": "the top-n most relevant documents) consistently enhances the final accuracy of LLMs in numerical QA. Sui\net al. (2023c) explored multiple table sampling methods (of rows and columns) and table packing (based\non a token-limit parameter). The best technique was the query-based sampling, which retrieves rows with\nthe highest semantic similarity to the question, surpassing methods involving no sampling, or clustering,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the top-n most relevant documents) consistently enhances the final accuracy of LLMs in numerical QA. Sui\net al. (2023c) explored multiple table sampling methods (of rows and columns) and table packing (based\non a token-limit parameter). The best technique was the query-based sampling, which retrieves rows with\nthe highest semantic similarity to the question, surpassing methods involving no sampling, or clustering,"
        }
    },
    {
        "id": "7fc41d1b76f456709327517fd174eac995b183f6",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\neven basic mathematical operations require memorization rather than algorithmic processing. The devel-\nopment of new tokenizers, like those used in LLaMA (Touvron et al., 2023b), which outperformed GPT-4\nin arithmetic tasks, involves rethinking tokenizer design to handle mixed textual and numerical data more\neffectively, such as by splitting each digit into individual tokens for consistent number tokenization (Gruver",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\neven basic mathematical operations require memorization rather than algorithmic processing. The devel-\nopment of new tokenizers, like those used in LLaMA (Touvron et al., 2023b), which outperformed GPT-4\nin arithmetic tasks, involves rethinking tokenizer design to handle mixed textual and numerical data more\neffectively, such as by splitting each digit into individual tokens for consistent number tokenization (Gruver"
        }
    },
    {
        "id": "db5c39ac1f000f4a08c16e5e6e92a00e3607d4c5",
        "content": "et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018; Choi et al., 2018; Baowaly et al.,\n2019; Xu et al., 2019), diffusion models(Kotelnikov et al., 2022; Xu et al., 2023a; Kim et al., 2022b;a; Lee\net al., 2023; Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning\napproaches have demonstrated superior performance over classical methods such as Bayesian networks ((Xu",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018; Choi et al., 2018; Baowaly et al.,\n2019; Xu et al., 2019), diffusion models(Kotelnikov et al., 2022; Xu et al., 2023a; Kim et al., 2022b;a; Lee\net al., 2023; Zhang et al., 2023c), and LLMs, opened up many new opportunities. These deep learning\napproaches have demonstrated superior performance over classical methods such as Bayesian networks ((Xu"
        }
    },
    {
        "id": "d7598177a690d1ccf02cbbd9ebcc6815d4d87705",
        "content": "language model. CoRR, abs/2308.11891, 2023d. doi: 10.48550/ARXIV.2308.11891. URL https://\ndoi.org/10.48550/arXiv.2308.11891.\nJun Zhang, Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, and Xiaokui Xiao. Privbayes:\nPrivate data release via bayesian networks. ACM Trans. Database Syst., 42(4), oct 2017. ISSN 0362-5915.\ndoi: 10.1145/3134428. URL https://doi.org/10.1145/3134428.\nTianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, and Qian Liu. Generative table pre-training",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "language model. CoRR, abs/2308.11891, 2023d. doi: 10.48550/ARXIV.2308.11891. URL https://\ndoi.org/10.48550/arXiv.2308.11891.\nJun Zhang, Graham Cormode, Cecilia M. Procopiuc, Divesh Srivastava, and Xiaokui Xiao. Privbayes:\nPrivate data release via bayesian networks. ACM Trans. Database Syst., 42(4), oct 2017. ISSN 0362-5915.\ndoi: 10.1145/3134428. URL https://doi.org/10.1145/3134428.\nTianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, and Qian Liu. Generative table pre-training"
        }
    },
    {
        "id": "8e55446c6ccfc96d5cdc88abd7d5b1ecb0f207fb",
        "content": "the slow training of LLMs by using a randomly initialized model as the starting point; the method achieves\ncontinuous refinement through iterative fine-tuning on successive tabular data tasks 13. It introduces a\ntoken sequence compression method and a middle padding strategy to simplify training data representation\nand enhance performance, achieving a significant reduction in training time while maintaining or improving\nsynthetic data quality.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the slow training of LLMs by using a randomly initialized model as the starting point; the method achieves\ncontinuous refinement through iterative fine-tuning on successive tabular data tasks 13. It introduces a\ntoken sequence compression method and a middle padding strategy to simplify training data representation\nand enhance performance, achieving a significant reduction in training time while maintaining or improving\nsynthetic data quality."
        }
    },
    {
        "id": "02ffcc5564d7ed62552ba0cb77e7bb9c308a4b64",
        "content": "trained on these samples. Additionally, CLLM distinguishes itself by not requiring any fine-tuning of LLMs.\nThe generation process. After fine-tuning the model or using a standard LLM, there are three primary\npreconditioning methods (Borisov et al., 2023b) for designing prompts to generate new data samples for\nCLM-based methods, as depicted in Figure 5: 1) feature name preconditioning: This method involves",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "trained on these samples. Additionally, CLLM distinguishes itself by not requiring any fine-tuning of LLMs.\nThe generation process. After fine-tuning the model or using a standard LLM, there are three primary\npreconditioning methods (Borisov et al., 2023b) for designing prompts to generate new data samples for\nCLM-based methods, as depicted in Figure 5: 1) feature name preconditioning: This method involves"
        }
    },
    {
        "id": "826867b8523658182a3ffe0e746c4a4fc512f46f",
        "content": "these encoders. For LLMs with >1B parameters, there are UniTabPT (Sarkar & Lausen, 2023) with 3B\nparameters (based on T5 and Flan-T5 models)), TableGPT (Gong et al., 2020) with 1.5B parameters (based\non GPT2), TableGPT3 (Zha et al., 2023) with 7B parameters (based on Phoenix (Chen et al., 2023c)),\nTableLlama (Zhang et al., 2023f) with 7B parameters (based on Llama 2 (Touvron et al., 2023b)), and\nTable-GPT with 350M, 3B, 13B or 175B parameters (based on various versions of OpenAI’s GPT models).",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "these encoders. For LLMs with >1B parameters, there are UniTabPT (Sarkar & Lausen, 2023) with 3B\nparameters (based on T5 and Flan-T5 models)), TableGPT (Gong et al., 2020) with 1.5B parameters (based\non GPT2), TableGPT3 (Zha et al., 2023) with 7B parameters (based on Phoenix (Chen et al., 2023c)),\nTableLlama (Zhang et al., 2023f) with 7B parameters (based on Llama 2 (Touvron et al., 2023b)), and\nTable-GPT with 350M, 3B, 13B or 175B parameters (based on various versions of OpenAI’s GPT models)."
        }
    },
    {
        "id": "63331860853cc5f637d81ece57ee0671f46024ac",
        "content": "thesis and table understanding like question answering before LLMs.\n1.3 Overview of large language models (LLMs)\nA language model (LM) is a probabilistic model that predicts the generative likelihood of future or missing\ntokens in a word sequence. Zhao et al. (2023b) thoroughly reviewed the development of LMs, and charac-\nterized the it into four different stages: The first stage is Statistical Language Models (SLM), which",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "thesis and table understanding like question answering before LLMs.\n1.3 Overview of large language models (LLMs)\nA language model (LM) is a probabilistic model that predicts the generative likelihood of future or missing\ntokens in a word sequence. Zhao et al. (2023b) thoroughly reviewed the development of LMs, and charac-\nterized the it into four different stages: The first stage is Statistical Language Models (SLM), which"
        }
    },
    {
        "id": "81c26e5ce99cb760121a7b35aefda38f36f9650b",
        "content": "Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models\nare realistic tabular data generators. In The Eleventh International Conference on Learning Representa-\ntions, 2023b. URL https://openreview.net/forum?id=cEygmQNOeI.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models\nare realistic tabular data generators. In The Eleventh International Conference on Learning Representa-\ntions, 2023b. URL https://openreview.net/forum?id=cEygmQNOeI.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,"
        }
    },
    {
        "id": "beb95b0669f137e4aa5efd086be5b50e3e6c5c10",
        "content": "advantage for LLM based method compared to traditional machine learning methods.\nStandard benchmark LLMs for tabular data could greatly benefit from standardized benchmark datasets\nto enable fair and transparent comparisons between models. In this survey, we strive to summarize commonly\nused datasets/metrics and provide recommendations for dataset selection to researchers and practitioners.\nHowever, for the same dataset, the same method, and the same task, different papers report different perfor-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "advantage for LLM based method compared to traditional machine learning methods.\nStandard benchmark LLMs for tabular data could greatly benefit from standardized benchmark datasets\nto enable fair and transparent comparisons between models. In this survey, we strive to summarize commonly\nused datasets/metrics and provide recommendations for dataset selection to researchers and practitioners.\nHowever, for the same dataset, the same method, and the same task, different papers report different perfor-"
        }
    },
    {
        "id": "4fe9570a6a95cb7f16cb295272da33635e504d6d",
        "content": "synthetic data quality.\nIn contrast to above methods that fine-tune an LLM on the corresponding table samples, Curated LLM\n(CLLM) (Seedat et al., 2023) utilizes the rich world knowledge from GPT4 to augment and enhance training\ndata in scenarios with limited data, without the need for fine-tuning. CLLM is a framework that leverages\nlearning dynamics and two novel curation metrics, namely confidence and uncertainty. These metrics help",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "synthetic data quality.\nIn contrast to above methods that fine-tune an LLM on the corresponding table samples, Curated LLM\n(CLLM) (Seedat et al., 2023) utilizes the rich world knowledge from GPT4 to augment and enhance training\ndata in scenarios with limited data, without the need for fine-tuning. CLLM is a framework that leverages\nlearning dynamics and two novel curation metrics, namely confidence and uncertainty. These metrics help"
        }
    },
    {
        "id": "853adf268982c9d51306c3d2f242beb34825d5ea",
        "content": "schlos. Tapas: Weakly supervised table parsing via pre-training. In Dan Jurafsky, Joyce Chai, Natalie\nSchluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 4320–4333. Association for Computa-\ntional Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.398. URL https://doi.org/10.18653/v1/\n2020.acl-main.398.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "schlos. Tapas: Weakly supervised table parsing via pre-training. In Dan Jurafsky, Joyce Chai, Natalie\nSchluter, and Joel R. Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 4320–4333. Association for Computa-\ntional Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.398. URL https://doi.org/10.18653/v1/\n2020.acl-main.398."
        }
    },
    {
        "id": "9b153745be9009e0f4624f1e061ae6a5b7bd5b90",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nThomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization,\n2021.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf-\nfin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma\nSharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nThomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization,\n2021.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf-\nfin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma\nSharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan"
        }
    },
    {
        "id": "2f8376755762bcadbac9cf654f6e8cc1302fdbb8",
        "content": "method of ZeroTS is easy to implement 8.\nInference-Only Prediction Similar to feature-based tabular prediction, researchers explored LLMs’ per-\nformance for time series forecasting without fine-tuning. ZeroTS (Gruver et al., 2023) examines the use\nof LLMs like GPT-3 (Brown et al., 2020) and LLaMA-70B (Touvron et al., 2023a) directly for time series\nforecasting. It evaluates models using mean absolute error (MAE), Scale MAE, and continuous ranked\n8The code is in https://Github.com/ngruver/llmtime\n16",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "method of ZeroTS is easy to implement 8.\nInference-Only Prediction Similar to feature-based tabular prediction, researchers explored LLMs’ per-\nformance for time series forecasting without fine-tuning. ZeroTS (Gruver et al., 2023) examines the use\nof LLMs like GPT-3 (Brown et al., 2020) and LLaMA-70B (Touvron et al., 2023a) directly for time series\nforecasting. It evaluates models using mean absolute error (MAE), Scale MAE, and continuous ranked\n8The code is in https://Github.com/ngruver/llmtime\n16"
        }
    },
    {
        "id": "84c5a7f6a465219d48260bfe85aba2b009797b92",
        "content": "features into relation graphs promotes heterogeneous feature interaction, 2023.\nJiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, and Jian Wu. Serval: Synergy\nlearning between vertical models and llms towards oracle-level zero-shot medical prediction. arXiv preprint\narXiv:2403.01570, 2024a.\nJiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "features into relation graphs promotes heterogeneous feature interaction, 2023.\nJiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, and Jian Wu. Serval: Synergy\nlearning between vertical models and llms towards oracle-level zero-shot medical prediction. arXiv preprint\narXiv:2403.01570, 2024a.\nJiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen."
        }
    },
    {
        "id": "90682f2762ca32a9d0f06a259eb1499aa9c00097",
        "content": "text or image, tabular data are relatively order-invariant. Therefore, position-based methodologies\n(e.g., spatial correlation, impeding inductive bias, convolutional neural networks (CNN)) are less\napplicable for tabular data modeling (Borisov et al., 2022a).\n6. Lack of prior knowledge: In image or audio data, there is often prior knowledge about the spatial or\ntemporal structure of the data, which can be leveraged by the model during training. However, in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "text or image, tabular data are relatively order-invariant. Therefore, position-based methodologies\n(e.g., spatial correlation, impeding inductive bias, convolutional neural networks (CNN)) are less\napplicable for tabular data modeling (Borisov et al., 2022a).\n6. Lack of prior knowledge: In image or audio data, there is often prior knowledge about the spatial or\ntemporal structure of the data, which can be leveraged by the model during training. However, in"
        }
    },
    {
        "id": "f74a69cb985f70325fc4702c42a8499350393735",
        "content": "Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, and Yarin Gal. Self-attention\nbetween datapoints: Going beyond individual input-output pairs in deep learning, 2022.\nAkim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular\ndata with diffusion models, 2022.\n36",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, and Yarin Gal. Self-attention\nbetween datapoints: Going beyond individual input-output pairs in deep learning, 2022.\nAkim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular\ndata with diffusion models, 2022.\n36"
        }
    },
    {
        "id": "5cd302bbae38dbbc730f68823f11cfd18468a63d",
        "content": "better performance than LIFT with 3 different base models.\nFine-tuning For studies involving fine-tuning, they typically employ one of two distinct approaches. The\nfirst involves training an LLM model on large datasets to learn fundamental features before adapting it to\nspecific prediction tasks. The second takes a pre-trained LLM and further trains it on a smaller, specific pre-\ndiction dataset to specialize its knowledge and improve its performance on the prediction. LIFT (Dinh et al.,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "better performance than LIFT with 3 different base models.\nFine-tuning For studies involving fine-tuning, they typically employ one of two distinct approaches. The\nfirst involves training an LLM model on large datasets to learn fundamental features before adapting it to\nspecific prediction tasks. The second takes a pre-trained LLM and further trains it on a smaller, specific pre-\ndiction dataset to specialize its knowledge and improve its performance on the prediction. LIFT (Dinh et al.,"
        }
    },
    {
        "id": "86d7a9546e9e5b3e5ca8fee79198d5b58d6635ec",
        "content": "Algorithm Type Method Resource Metric Used Model\nTabletSlack & Singh (2023) Tabular No Finetune Low F1 GPTJ/Tk-Instruct/Flan T5\nSummaryBoostManikandan et al. (2023) Tabular No Finetune High RMSE GPT3\nLIFTDinh et al. (2022) Tabular Finetune High MAE/RMSE GPT3/GPTJ\nTabLLMHegselmann et al. (2023) Tabular Finetune High AUC GPT3/T0\nUnipredictWang et al. (2023a) Tabular Finetune Low ACC GPT2\nGTLZhang et al. (2023a) Tabular Finetune Low ACC LLaMA\nSerializeLLMJaitly et al. (2023) Tabular Finetune High AUC T0",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Algorithm Type Method Resource Metric Used Model\nTabletSlack & Singh (2023) Tabular No Finetune Low F1 GPTJ/Tk-Instruct/Flan T5\nSummaryBoostManikandan et al. (2023) Tabular No Finetune High RMSE GPT3\nLIFTDinh et al. (2022) Tabular Finetune High MAE/RMSE GPT3/GPTJ\nTabLLMHegselmann et al. (2023) Tabular Finetune High AUC GPT3/T0\nUnipredictWang et al. (2023a) Tabular Finetune Low ACC GPT2\nGTLZhang et al. (2023a) Tabular Finetune Low ACC LLaMA\nSerializeLLMJaitly et al. (2023) Tabular Finetune High AUC T0"
        }
    },
    {
        "id": "a31ae675c94463991eb7b66f41365918c3276baa",
        "content": "decompose the task into manageable sub-tasks. For example, to improve downstream tabular reasoning, Sui\net al. (2023b) proposed a two-step self-augmented prompting approach: first using prompts to ask the LLM\nto generate additional knowledge (intermediate output) about the table, then incorporating the response\ninto the second prompt to request the final answer for a downstream task. Ye et al. (2023b) also guided",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "decompose the task into manageable sub-tasks. For example, to improve downstream tabular reasoning, Sui\net al. (2023b) proposed a two-step self-augmented prompting approach: first using prompts to ask the LLM\nto generate additional knowledge (intermediate output) about the table, then incorporating the response\ninto the second prompt to request the final answer for a downstream task. Ye et al. (2023b) also guided"
        }
    },
    {
        "id": "ae18d48ac06d5e322061c211164892ccf47f62e2",
        "content": "pertml: Two-dimensional word embedding for the precognition on structured tabular data, 2019.\nChenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate\nllm’s ability for time series, 2023a.\nRuoxi Sun, Sercan Ö. Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas\nPfister. Sql-palm: Improved large language model adaptation for text-to-sql. CoRR, abs/2306.00739,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "pertml: Two-dimensional word embedding for the precognition on structured tabular data, 2019.\nChenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate\nllm’s ability for time series, 2023a.\nRuoxi Sun, Sercan Ö. Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, and Tomas\nPfister. Sql-palm: Improved large language model adaptation for text-to-sql. CoRR, abs/2306.00739,"
        }
    },
    {
        "id": "c1dc1d8d4bc830fa8fc213a904fbde535deb6752",
        "content": "final query (e.g. SQLizer (Yaghmazadeh et al., 2017)). Another example is SQLNet (Xu et al., 2017) which\nuses column attention mechanism to synthesize the query based on a dependency graph-dependent sketch.\nA derivative of SQLNet is TYPESQL (Yu et al., 2018a) which is also a sketch-based and slot-filling method\nentails extracting essential features to populate their respective slots. Unlike the previous supervised end-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "final query (e.g. SQLizer (Yaghmazadeh et al., 2017)). Another example is SQLNet (Xu et al., 2017) which\nuses column attention mechanism to synthesize the query based on a dependency graph-dependent sketch.\nA derivative of SQLNet is TYPESQL (Yu et al., 2018a) which is also a sketch-based and slot-filling method\nentails extracting essential features to populate their respective slots. Unlike the previous supervised end-"
        }
    },
    {
        "id": "106c2de9b18e1bb253ad12aba1e79aabf0c72134",
        "content": "Sos: Score-based oversampling for tabular data. In Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pp. 762–772, 2022b.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models,\n2015.\nSerkan Kiranyaz, Onur Avci, Osama Abdeljaber, Turker Ince, Moncef Gabbouj, and Daniel J. Inman. 1d\nconvolutional neural networks and applications: A survey, 2019.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Sos: Score-based oversampling for tabular data. In Proceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pp. 762–772, 2022b.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models,\n2015.\nSerkan Kiranyaz, Onur Avci, Osama Abdeljaber, Turker Ince, Moncef Gabbouj, and Daniel J. Inman. 1d\nconvolutional neural networks and applications: A survey, 2019."
        }
    },
    {
        "id": "659f7a3c227c67c055f8fd24dd687759af0a57ee",
        "content": "we provide examples of main techniques. Practitioners and researchers can look at the section and\nunderstand the difference of each technique. We only recommend benchmark methods and provide\nGitHub link of these techniques for reference and benchmark.\n5. An overview of future research directions. Future research could focus on how to solve bias\nproblem in tabular data modeling, how to mitigate hallucinations, how to find better representations",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "we provide examples of main techniques. Practitioners and researchers can look at the section and\nunderstand the difference of each technique. We only recommend benchmark methods and provide\nGitHub link of these techniques for reference and benchmark.\n5. An overview of future research directions. Future research could focus on how to solve bias\nproblem in tabular data modeling, how to mitigate hallucinations, how to find better representations"
        }
    },
    {
        "id": "839b0f2a5085942902ede895dc2685496dc5933a",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nPaper Task Models Explored\nDOCMATH-EVAL NumQA GPT4, GPT3.5, WizardLM, Llama-2 7, 13, 70B,\n(Zhao et al., 2023d)\nCodeLlama 34B, Baichuan, Qwen, WizardMath, Vi-\ncuna, Mistral, etc.\nAkhtar et al. (2023) NumQA TAPAS, DeBERTa, TAPEX, NT5, LUNA, PASTA,\nReasTAP, FlanT5, GPT3.5, PaLM\nTableGPT NumQA GPT2\n(Gong et al., 2020)\nDATER QA GPT3 Codex\n(Ye et al., 2023b)\nChen (2023) QA GPT3\ncTBLS QA Custom: Dense Table Retrieval based on RoBERTa",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nPaper Task Models Explored\nDOCMATH-EVAL NumQA GPT4, GPT3.5, WizardLM, Llama-2 7, 13, 70B,\n(Zhao et al., 2023d)\nCodeLlama 34B, Baichuan, Qwen, WizardMath, Vi-\ncuna, Mistral, etc.\nAkhtar et al. (2023) NumQA TAPAS, DeBERTa, TAPEX, NT5, LUNA, PASTA,\nReasTAP, FlanT5, GPT3.5, PaLM\nTableGPT NumQA GPT2\n(Gong et al., 2020)\nDATER QA GPT3 Codex\n(Ye et al., 2023b)\nChen (2023) QA GPT3\ncTBLS QA Custom: Dense Table Retrieval based on RoBERTa"
        }
    },
    {
        "id": "adf8374a640b4432bbe3909b617a2c44aa43af78",
        "content": "to compare the performance of different methods. Among those metrics, MAE and RMSE are used by at\nleast half of our surveyed methods in time series.\nMethod Used Paper Example\nAdding Special Token be- Dinh et al. (2022) ### {Category} @@@\nfore and after the answer\nVerbalizer Hegselmann et al. (2023) Output -> {category1: probability1, .}\nSpecific Prefix Manikandan et al. (2023); Please answer with category 1, category 2, ...\nSlack & Singh (2023)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to compare the performance of different methods. Among those metrics, MAE and RMSE are used by at\nleast half of our surveyed methods in time series.\nMethod Used Paper Example\nAdding Special Token be- Dinh et al. (2022) ### {Category} @@@\nfore and after the answer\nVerbalizer Hegselmann et al. (2023) Output -> {category1: probability1, .}\nSpecific Prefix Manikandan et al. (2023); Please answer with category 1, category 2, ...\nSlack & Singh (2023)"
        }
    },
    {
        "id": "b8ab3907e0ae657031911956b3a0cfc49660a16d",
        "content": "the question; (4) query similarity selection: select k examples similar to target SQL query s∗, which relies\non another model to generate SQL query s′ based on the target question and database, and so s′ is an\n26",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the question; (4) query similarity selection: select k examples similar to target SQL query s∗, which relies\non another model to generate SQL query s′ based on the target question and database, and so s′ is an\n26"
        }
    },
    {
        "id": "cdaf19db19e6379b687caaeec26d15e8bbfa2e24",
        "content": "Many real-world QA applications (E.g. working with financial documents, annual reports, etc.) involve such\nmathematical reasoning tasks. So far, Akhtar et al. (2023) conclude that LLMs like FlanT5 and GPT3.5\nperform better than other models on various numerical reasoning tasks. On the DOCMATH-EVAL (Zhao\net al., 2023d) dataset, GPT-4 with CoT significantly outperforms other LLMs, while open-source LLMs\n(LLaMa-2, Vicuna, Mistral, Starcoder, MPT, Qwen, AquilaChat2, etc.) lag behind.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Many real-world QA applications (E.g. working with financial documents, annual reports, etc.) involve such\nmathematical reasoning tasks. So far, Akhtar et al. (2023) conclude that LLMs like FlanT5 and GPT3.5\nperform better than other models on various numerical reasoning tasks. On the DOCMATH-EVAL (Zhao\net al., 2023d) dataset, GPT-4 with CoT significantly outperforms other LLMs, while open-source LLMs\n(LLaMa-2, Vicuna, Mistral, Starcoder, MPT, Qwen, AquilaChat2, etc.) lag behind."
        }
    },
    {
        "id": "e5d2fe8b5e7adc1010facb38a5bc580bbe480762",
        "content": "effectively, such as by splitting each digit into individual tokens for consistent number tokenization (Gruver\net al., 2023). This method has shown promise in improving the understanding of symbolic and numerical\ndata. However, it hugely increases the dimension of the input, which makes the method not practical for\nlarge datasets and many features. For future direction, it is worth to explore new tokenizer that can better\nrepresent numerical token while not increase the dimension of the input.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "effectively, such as by splitting each digit into individual tokens for consistent number tokenization (Gruver\net al., 2023). This method has shown promise in improving the understanding of symbolic and numerical\ndata. However, it hugely increases the dimension of the input, which makes the method not practical for\nlarge datasets and many features. For future direction, it is worth to explore new tokenizer that can better\nrepresent numerical token while not increase the dimension of the input."
        }
    },
    {
        "id": "1548819665f989eee0b05f49da5b3b663623dcc5",
        "content": "for tabular data. Advances in Neural Information Processing Systems, 34:18932–18943, 2021.\nLéo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep\nlearning on tabular data?, 2022.\nNate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot\ntime series forecasters. arXiv preprint arXiv:2310.07820, 2023.\nZihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: table-operations",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "for tabular data. Advances in Neural Information Processing Systems, 34:18932–18943, 2021.\nLéo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform deep\nlearning on tabular data?, 2022.\nNate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot\ntime series forecasters. arXiv preprint arXiv:2310.07820, 2023.\nZihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: table-operations"
        }
    },
    {
        "id": "265cd1967247527af5ee38cc434226c3d9dfe81c",
        "content": "based few-shot tuning, 2022.\nSajad Darabi and Yotam Elor. Synthesising multi-modal minority samples for tabular data, 2021.\nSanjib Das, AnHai Doan, Paul Suganthan G. C., Chaitanya Gokhale, Pradap Konda, Yash Govind, and Derek\nPaulsen. The magellan data repository. https://sites.google.com/site/anhaidgroup/projects/data,\n2015.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. TURL: table understanding through\nrepresentation learning. SIGMOD Rec., 51(1):33–40, 2022a. doi: 10.1145/3542700.3542709. URL",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "based few-shot tuning, 2022.\nSajad Darabi and Yotam Elor. Synthesising multi-modal minority samples for tabular data, 2021.\nSanjib Das, AnHai Doan, Paul Suganthan G. C., Chaitanya Gokhale, Pradap Konda, Yash Govind, and Derek\nPaulsen. The magellan data repository. https://sites.google.com/site/anhaidgroup/projects/data,\n2015.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and Cong Yu. TURL: table understanding through\nrepresentation learning. SIGMOD Rec., 51(1):33–40, 2022a. doi: 10.1145/3542700.3542709. URL"
        }
    },
    {
        "id": "3cc78e0b2571f2204aa2a29e4cd14954aeda845e",
        "content": "For additional information Some papers explore techniques to curate samples for in-context learning.\nGao et al. (2024) explored the a few methods: (1) random: randomly selecting k examples; (2) question\nsimilarity selection: choosing k examples based on semantic similarity with question Q, based on a predefined\ndistance metric (E.g. Euclidean or negative cosine similarity) of the question and example embedding, and",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "For additional information Some papers explore techniques to curate samples for in-context learning.\nGao et al. (2024) explored the a few methods: (1) random: randomly selecting k examples; (2) question\nsimilarity selection: choosing k examples based on semantic similarity with question Q, based on a predefined\ndistance metric (E.g. Euclidean or negative cosine similarity) of the question and example embedding, and"
        }
    },
    {
        "id": "4cbaadcd7114ce31962da2ecdebc7ed8c6c4c5ce",
        "content": "doi: 10.18653/v1/2022.naacl-industry.34. URL https://aclanthology.org/2022.naacl-industry.34.\nLiran Katzir, Gal Elidan, and Ran El-Yaniv. Net-dnf: Effective deep modeling of tabular data. In Interna-\ntional conference on learning representations, 2020.\nGuolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. Deepgbm: A deep learning framework\ndistilled by gbdt for online prediction tasks. In Proceedings of the 25th ACM SIGKDD International",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "doi: 10.18653/v1/2022.naacl-industry.34. URL https://aclanthology.org/2022.naacl-industry.34.\nLiran Katzir, Gal Elidan, and Ran El-Yaniv. Net-dnf: Effective deep modeling of tabular data. In Interna-\ntional conference on learning representations, 2020.\nGuolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu. Deepgbm: A deep learning framework\ndistilled by gbdt for online prediction tasks. In Proceedings of the 25th ACM SIGKDD International"
        }
    },
    {
        "id": "73002386477c72a90f62cad9ba79ba9cfe6ac1da",
        "content": "three challenges:\nFirstly, some models have short context lengths (E.g. Flan-UL2 (Tay et al., 2023b) supports 2048 tokens,\nLlama 2 (Touvron et al., 2023b) supports 4096 context tokens) and even models that support large context\nlengths might still be insufficient for extremely large tables with over 200K rows (Claude 2.1 supports up to\n200K tokens).\nSecondly, even if the table could fit the context length, most LLMs are slow to process long sentences due to",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "three challenges:\nFirstly, some models have short context lengths (E.g. Flan-UL2 (Tay et al., 2023b) supports 2048 tokens,\nLlama 2 (Touvron et al., 2023b) supports 4096 context tokens) and even models that support large context\nlengths might still be insufficient for extremely large tables with over 200K rows (Claude 2.1 supports up to\n200K tokens).\nSecondly, even if the table could fit the context length, most LLMs are slow to process long sentences due to"
        }
    },
    {
        "id": "80f5e9b4b7cd3615c85baba7676852e915f97d84",
        "content": "LLM achieve higher accuracy across all six datasets explored. “Statistics features” improved performance for\ntasks and datasets that include a higher proportion of statistical cell contents, like FEVEROUS (Aly et al.,\n2021). Meanwhile, “document references” and “term explanations” add context and semantic meaning to\nthe tables. “Table size” had minimal improvements, while “header hierarchy” added unnecessary complexity,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "LLM achieve higher accuracy across all six datasets explored. “Statistics features” improved performance for\ntasks and datasets that include a higher proportion of statistical cell contents, like FEVEROUS (Aly et al.,\n2021). Meanwhile, “document references” and “term explanations” add context and semantic meaning to\nthe tables. “Table size” had minimal improvements, while “header hierarchy” added unnecessary complexity,"
        }
    },
    {
        "id": "825e8a0ef369c6dd0cff8be97792172bb5dd5d68",
        "content": "https://doi.org/10.48550/arXiv.2310.18742.\nHiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. TABBIE: Pretrained representations of\ntabular data. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy,\nSteven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics: Human",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "https://doi.org/10.48550/arXiv.2310.18742.\nHiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. TABBIE: Pretrained representations of\ntabular data. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy,\nSteven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics: Human"
        }
    },
    {
        "id": "901b66ad656df8c16b119432a2a60bf6cf0e4e52",
        "content": "LLMs have been used in many tabular data applications, such as predictions, data synthesis, question\nanswering and table understanding. Here we outline some practical limitations and considerations for future\nresearch.\nNumerical representation It was revealed that LLM in house embedding is not suitable for representing\nintrinsic relations in numerical features (Gruver et al., 2023), and thus a careful embedding is needed. To-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "LLMs have been used in many tabular data applications, such as predictions, data synthesis, question\nanswering and table understanding. Here we outline some practical limitations and considerations for future\nresearch.\nNumerical representation It was revealed that LLM in house embedding is not suitable for representing\nintrinsic relations in numerical features (Gruver et al., 2023), and thus a careful embedding is needed. To-"
        }
    },
    {
        "id": "919cb55783d1fd752659a6adabd1a5140be6a09f",
        "content": "Definition 1 (Large Language Model). A large language model (LLM) M , parameterized by θ, is a\ntransformer-based model with an architecture that can be autoregressive, autoencoding, or encoder-decoder.\nIt has been trained on a large corpus comprising hundreds of millions to trillions of tokens. LLMs encompass\npre-trained models and for our survey, refers to models that have at least 1 billion parameters.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Definition 1 (Large Language Model). A large language model (LLM) M , parameterized by θ, is a\ntransformer-based model with an architecture that can be autoregressive, autoencoding, or encoder-decoder.\nIt has been trained on a large corpus comprising hundreds of millions to trillions of tokens. LLMs encompass\npre-trained models and for our survey, refers to models that have at least 1 billion parameters."
        }
    },
    {
        "id": "adce967f8c7f0f8ed2f6299f7aafef0bd1be9689",
        "content": "et al. (2023) uses T0 (Sanh et al., 2021). Compared to TabLLM, it is trained using Intrinsic Attention-based\nPrompt Tuning (IA3) (Liu et al., 2022b). However, this method only works for a few short learning, worse\nthan baseline when the number of shots is more or equal to 128. T0 model (Sanh et al., 2021) is commonly\nused as the base model for tabular prediction fine-tuning.\nPLM can be effectively adapted for diverse tabular prediction tasks, demonstrating their versatility across",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al. (2023) uses T0 (Sanh et al., 2021). Compared to TabLLM, it is trained using Intrinsic Attention-based\nPrompt Tuning (IA3) (Liu et al., 2022b). However, this method only works for a few short learning, worse\nthan baseline when the number of shots is more or equal to 128. T0 model (Sanh et al., 2021) is commonly\nused as the base model for tabular prediction fine-tuning.\nPLM can be effectively adapted for diverse tabular prediction tasks, demonstrating their versatility across"
        }
    },
    {
        "id": "e3d0b26898a1ee80cfcf552aaa9f8042d83fe788",
        "content": "in this survey can be attributed as RAG systems. A particular challenge in RAG is to extract the most\nrelevant information out of a large pool of data to better inform the LLMs. This challenge overlaps slightly\nwith the strategies about table sampling mentioned earlier under Section 2.2. Apart from the aforementioned\nmethods, Sundar & Heck (2023) designed a dual-encoder-based Dense Table Retrieval (DTR) model to rank",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "in this survey can be attributed as RAG systems. A particular challenge in RAG is to extract the most\nrelevant information out of a large pool of data to better inform the LLMs. This challenge overlaps slightly\nwith the strategies about table sampling mentioned earlier under Section 2.2. Apart from the aforementioned\nmethods, Sundar & Heck (2023) designed a dual-encoder-based Dense Table Retrieval (DTR) model to rank"
        }
    },
    {
        "id": "1451e2fa4456f1b844d0f4ebb3a21ff4e3b68023",
        "content": "et al., 2020) or optimization (Bassi et al., 2024).\nHallucination LLMs sometimes produce content that is inconsistent with the real-world facts or the user\ninputs (Huang et al., 2023a), which raises concerns over the reliability and usefulness of LLMs in the real-\nworld applications. For tabular prediction, especially when working with patient records and medical data,\n28",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2020) or optimization (Bassi et al., 2024).\nHallucination LLMs sometimes produce content that is inconsistent with the real-world facts or the user\ninputs (Huang et al., 2023a), which raises concerns over the reliability and usefulness of LLMs in the real-\nworld applications. For tabular prediction, especially when working with patient records and medical data,\n28"
        }
    },
    {
        "id": "1a8786b461c094fe89e3b87cbfb81a241aa9617a",
        "content": "Nengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question answering:\nRecent advances, 2022.\nAlexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via\ndiffusion and flow-based gradient-boosted trees, 2023.\nVincent Jung and Lonneke van der Plas. Understanding the effects of language-specific class imbalance in\nmultilingual fine-tuning, 2024.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Nengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question answering:\nRecent advances, 2022.\nAlexia Jolicoeur-Martineau, Kilian Fatras, and Tal Kachman. Generating and imputing tabular data via\ndiffusion and flow-based gradient-boosted trees, 2023.\nVincent Jung and Lonneke van der Plas. Understanding the effects of language-specific class imbalance in\nmultilingual fine-tuning, 2024."
        }
    },
    {
        "id": "254bb20ec95c520f9d2444f3f880e0e19a6334b5",
        "content": "in the medical field. For preprocessing, Meditab utilizes GPT-3.5 (Brown et al., 2020) to convert tabular\ndata into textual format, with a focus on extracting key values. Subsequently, it employs techniques such\nas linearization, prompting, and sanity checks to ensure accuracy and to mitigate errors. For fine-tuning,\n17",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "in the medical field. For preprocessing, Meditab utilizes GPT-3.5 (Brown et al., 2020) to convert tabular\ndata into textual format, with a focus on extracting key values. Subsequently, it employs techniques such\nas linearization, prompting, and sanity checks to ensure accuracy and to mitigate errors. For fine-tuning,\n17"
        }
    },
    {
        "id": "3e37dbc14b34a68833ca0145e8806a4c6e3b6bcb",
        "content": "generation lead to better downstream performance. Zhao et al. (2023d) used OpenAI’s Ada Embedding4\nand Contriever (Izacard et al., 2022) as the dense retriever along with BM25 (Robertson et al., 1995) as the\nsparse retriever. These retrievers help to extract the top-n most related textual and tabular evidence from\nthe source document, which were then provided as the input context to answer the question.\nFor additional information Some papers explore techniques to curate samples for in-context learning.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "generation lead to better downstream performance. Zhao et al. (2023d) used OpenAI’s Ada Embedding4\nand Contriever (Izacard et al., 2022) as the dense retriever along with BM25 (Robertson et al., 1995) as the\nsparse retriever. These retrievers help to extract the top-n most related textual and tabular evidence from\nthe source document, which were then provided as the input context to answer the question.\nFor additional information Some papers explore techniques to curate samples for in-context learning."
        }
    },
    {
        "id": "3f9b124950bc8966c9890a828c24a8a3dac334d3",
        "content": "10.1109/DSAA.2016.49.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguistics, 2018a. doi: 10.18653/\nv1/N18-1202. URL https://aclanthology.org/N18-1202.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "10.1109/DSAA.2016.49.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguistics, 2018a. doi: 10.18653/\nv1/N18-1202. URL https://aclanthology.org/N18-1202."
        }
    },
    {
        "id": "519717d90fe4c6e5d8879fe6fab1a9fcdb658954",
        "content": "then used a data audit module which filters the data based on data Shapley scores, leading to a smaller\nbut cleaner dataset. Secondly, they removed any cells with False values, which removes the chances of the\nLLMs making a false inference on these invalid values. Finally, they performed a sanity check via LLM’s\nreflection: querying the LLM with the input template “What is the {column}? {x}” to check if the answer",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "then used a data audit module which filters the data based on data Shapley scores, leading to a smaller\nbut cleaner dataset. Secondly, they removed any cells with False values, which removes the chances of the\nLLMs making a false inference on these invalid values. Finally, they performed a sanity check via LLM’s\nreflection: querying the LLM with the input template “What is the {column}? {x}” to check if the answer"
        }
    },
    {
        "id": "5c6c925d7940c2ed4153401b263c5e7983d89249",
        "content": "for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.254. URL https://aclanthology.org/\n2021.acl-long.254.\nYitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla, Hyunseung Yoo, Yvonne A\nEvrard, James H Doroshow, and Rick L Stevens. Converting tabular data into images for deep learning\nwith convolutional neural networks. Scientific reports, 11(1):11325, 2021b.\n47",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.254. URL https://aclanthology.org/\n2021.acl-long.254.\nYitan Zhu, Thomas Brettin, Fangfang Xia, Alexander Partin, Maulik Shukla, Hyunseung Yoo, Yvonne A\nEvrard, James H Doroshow, and Rick L Stevens. Converting tabular data into images for deep learning\nwith convolutional neural networks. Scientific reports, 11(1):11325, 2021b.\n47"
        }
    },
    {
        "id": "8f65de0bb718b8a9427535ed23ba4911cbd65051",
        "content": "putation power (Bengio et al., 2000). Next, Neural Language Models (NLM) utilize neural networks\n(e.g. Recurrent neural networks (RNN)) as a probabilistic classifier (Kim et al., 2015). In addition to learning\nthe probabilistic function for word sequence, a key advantage of NLM is that they can learn the distributed\nrepresentation (i.e. word embedding) of each word so that similar words are mapped close to each other in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "putation power (Bengio et al., 2000). Next, Neural Language Models (NLM) utilize neural networks\n(e.g. Recurrent neural networks (RNN)) as a probabilistic classifier (Kim et al., 2015). In addition to learning\nthe probabilistic function for word sequence, a key advantage of NLM is that they can learn the distributed\nrepresentation (i.e. word embedding) of each word so that similar words are mapped close to each other in"
        }
    },
    {
        "id": "9ade0051d94a85a40b362f84e53880cdf7a1bcd0",
        "content": "2023. doi: 10.48550/ARXIV.2307.08674. URL https://doi.org/10.48550/arXiv.2307.08674.\nHan Zhang, Xumeng Wen, Shun Zheng, Wei Xu, and Jiang Bian. Towards foundation models for learning\non tabular data, 2023a.\nHaochen Zhang, Yuyang Dong, Chuan Xiao, and Masafumi Oyamada. Jellyfish: A large language model\nfor data preprocessing. CoRR, abs/2312.01678, 2023b. doi: 10.48550/ARXIV.2312.01678. URL https:\n//doi.org/10.48550/arXiv.2312.01678.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2023. doi: 10.48550/ARXIV.2307.08674. URL https://doi.org/10.48550/arXiv.2307.08674.\nHan Zhang, Xumeng Wen, Shun Zheng, Wei Xu, and Jiang Bian. Towards foundation models for learning\non tabular data, 2023a.\nHaochen Zhang, Yuyang Dong, Chuan Xiao, and Masafumi Oyamada. Jellyfish: A large language model\nfor data preprocessing. CoRR, abs/2312.01678, 2023b. doi: 10.48550/ARXIV.2312.01678. URL https:\n//doi.org/10.48550/arXiv.2312.01678."
        }
    },
    {
        "id": "dd64194f0d83a1076a17b0f9a883abf937779375",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\npseudo-label generation. Their experimental findings demonstrate that by incorporating the additional table\npre-training phase and employing machine learning models to generate labels, TAPTAP can generate superior\nquality training samples compared with GReaT. TabuLa (Zhao et al., 2023f), on the other hand, addresses\nthe slow training of LLMs by using a randomly initialized model as the starting point; the method achieves",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\npseudo-label generation. Their experimental findings demonstrate that by incorporating the additional table\npre-training phase and employing machine learning models to generate labels, TAPTAP can generate superior\nquality training samples compared with GReaT. TabuLa (Zhao et al., 2023f), on the other hand, addresses\nthe slow training of LLMs by using a randomly initialized model as the starting point; the method achieves"
        }
    },
    {
        "id": "ea260054ffc3d9b0c5dc6b7afc1fd44d6bba3680",
        "content": "ing procedure enables arbitrary prompts during generation, making the generation process more efficient.\nMoreover, MLM can easily address the common challenge of missing data in tabular datasets by learning\nfrom missing values through a masking probability setting of 1, streamlining the generation process without\nrequiring separate data imputation steps.\n13The code is in https://github.com/zhao-zilong/Tabula\n20",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ing procedure enables arbitrary prompts during generation, making the generation process more efficient.\nMoreover, MLM can easily address the common challenge of missing data in tabular datasets by learning\nfrom missing values through a masking probability setting of 1, streamlining the generation process without\nrequiring separate data imputation steps.\n13The code is in https://github.com/zhao-zilong/Tabula\n20"
        }
    },
    {
        "id": "fad4c93180c3401cd7e71396a56ad78d807b1ba9",
        "content": "standing of textual and tabular data, 2020a.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. TaBERT: Pretraining for joint under-\nstanding of textual and tabular data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault\n(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8413–\n8426, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.745.\nURL https://aclanthology.org/2020.acl-main.745.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "standing of textual and tabular data, 2020a.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. TaBERT: Pretraining for joint under-\nstanding of textual and tabular data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault\n(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8413–\n8426, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.745.\nURL https://aclanthology.org/2020.acl-main.745."
        }
    },
    {
        "id": "04b867eb5d03dd01dd07d1c53c3b1d335e2d07d9",
        "content": "Tabular-specific challenges The current exploration of LLMs on tabular data remains primarily surface-\nlevel, lacking in-depth analysis tailored to the unique characteristics of tabular datasets. For example, there\nis a paucity of understanding regarding how LLMs handle class imbalanced datasets. Given that LLMs come\nwith prior knowledge, it is reasonable to hypothesize about the synergistic or antagonistic effects between",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tabular-specific challenges The current exploration of LLMs on tabular data remains primarily surface-\nlevel, lacking in-depth analysis tailored to the unique characteristics of tabular datasets. For example, there\nis a paucity of understanding regarding how LLMs handle class imbalanced datasets. Given that LLMs come\nwith prior knowledge, it is reasonable to hypothesize about the synergistic or antagonistic effects between"
        }
    },
    {
        "id": "0ebe48b86c840f319c7dca2b6d7e343054e0974c",
        "content": "Recent studies have increasingly relied on LLMs to synthesize tabular data, leveraging their advanced gener-\native capabilities developed through extensive training on vast text corpora, including markdown-formatted\nserialized tabular data. This proficiency allows LLMs to capture the intricate patterns and relationships\ninherent in tabular datasets (Bordt et al., 2024). Furthermore, LLMs possess rich language and data un-\n9Available at https://Github.com/RyanWangZf/MediTab.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Recent studies have increasingly relied on LLMs to synthesize tabular data, leveraging their advanced gener-\native capabilities developed through extensive training on vast text corpora, including markdown-formatted\nserialized tabular data. This proficiency allows LLMs to capture the intricate patterns and relationships\ninherent in tabular datasets (Bordt et al., 2024). Furthermore, LLMs possess rich language and data un-\n9Available at https://Github.com/RyanWangZf/MediTab."
        }
    },
    {
        "id": "1561c074d51724e2b8caf9211845b9f497493190",
        "content": "which utilizes prior knowledge from LLMs (e.g., GPT4) to augment and enhance training data samples in\nlow-data settings without fine-tuning the LLMs, other methods such as GReaT (Borisov et al., 2023b),\nTAPTAP (Zhang et al., 2023e), TabuLa (Zhao et al., 2023f), and TabMT (Gulati & Roysdon, 2023) all\ninvolve fine-tuning the LLMs on a corresponding table. In standard data scenarios, fine-tuning an LLM\nto improve its ability to capture a table’s data distribution becomes essential. This is because presenting",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "which utilizes prior knowledge from LLMs (e.g., GPT4) to augment and enhance training data samples in\nlow-data settings without fine-tuning the LLMs, other methods such as GReaT (Borisov et al., 2023b),\nTAPTAP (Zhang et al., 2023e), TabuLa (Zhao et al., 2023f), and TabMT (Gulati & Roysdon, 2023) all\ninvolve fine-tuning the LLMs on a corresponding table. In standard data scenarios, fine-tuning an LLM\nto improve its ability to capture a table’s data distribution becomes essential. This is because presenting"
        }
    },
    {
        "id": "1e8d7804ce57d16689ea689f262c7f86f990c5dc",
        "content": "represent numerical token while not increase the dimension of the input.\nCategorical representation Tabular dataset very often contains an excessive number of categorical\ncolumns, which can lead to serialized input strings surpassing the context limit of the language model and\nincreased cost. This is problematic as it results in parts of the data being pruned, thereby negatively im-\npacting the model’s performance. Additionally, there are issues with poorly represented categorical feature,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "represent numerical token while not increase the dimension of the input.\nCategorical representation Tabular dataset very often contains an excessive number of categorical\ncolumns, which can lead to serialized input strings surpassing the context limit of the language model and\nincreased cost. This is problematic as it results in parts of the data being pruned, thereby negatively im-\npacting the model’s performance. Additionally, there are issues with poorly represented categorical feature,"
        }
    },
    {
        "id": "389febcec7afa713bbbed6f9b7ae475631755a03",
        "content": "using conditional gan, 2019.\nWeijie Xu, Wenxiang Hu, Fanyou Wu, and Srinivasan Sengamedu. Detime: Diffusion-enhanced topic\nmodeling using encoder-decoder based llm. In Findings of the Association for Computational Linguis-\ntics: EMNLP 2023. Association for Computational Linguistics, 2023a. doi: 10.18653/v1/2023.findings-\nemnlp.606. URL http://dx.doi.org/10.18653/v1/2023.findings-emnlp.606.\nWeijie Xu, Jinjin Zhao, Francis Iannacci, and Bo Wang. Ffpdg: Fast, fair and private data generation. arXiv",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "using conditional gan, 2019.\nWeijie Xu, Wenxiang Hu, Fanyou Wu, and Srinivasan Sengamedu. Detime: Diffusion-enhanced topic\nmodeling using encoder-decoder based llm. In Findings of the Association for Computational Linguis-\ntics: EMNLP 2023. Association for Computational Linguistics, 2023a. doi: 10.18653/v1/2023.findings-\nemnlp.606. URL http://dx.doi.org/10.18653/v1/2023.findings-emnlp.606.\nWeijie Xu, Jinjin Zhao, Francis Iannacci, and Bo Wang. Ffpdg: Fast, fair and private data generation. arXiv"
        }
    },
    {
        "id": "48074c0eefa615c90e0360d99c81136c724665df",
        "content": "networks, 2021.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential\nquestion answering. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1821–1831, Vancouver,\nCanada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1167. URL https:\n//aclanthology.org/P17-1167.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "networks, 2021.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential\nquestion answering. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1821–1831, Vancouver,\nCanada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1167. URL https:\n//aclanthology.org/P17-1167."
        }
    },
    {
        "id": "4aee0dedffc8230baf4bfa611124ccbe720d4777",
        "content": "aggregated sources, 2020b.\nZhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye,\nZiyue Li, Rui Zhao, and Hangyu Mao. PET-SQL: A prompt-enhanced two-stage text-to-sql framework\nwith cross-consistency. CoRR, abs/2403.09732, 2024. doi: 10.48550/ARXIV.2403.09732. URL https:\n//doi.org/10.48550/arXiv.2403.09732.\nGuang Liu, Jie Yang, and Ledell Wu. Ptab: Using the pre-trained language model for modeling tabular\ndata, 2022a.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "aggregated sources, 2020b.\nZhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye,\nZiyue Li, Rui Zhao, and Hangyu Mao. PET-SQL: A prompt-enhanced two-stage text-to-sql framework\nwith cross-consistency. CoRR, abs/2403.09732, 2024. doi: 10.48550/ARXIV.2403.09732. URL https:\n//doi.org/10.48550/arXiv.2403.09732.\nGuang Liu, Jie Yang, and Ledell Wu. Ptab: Using the pre-trained language model for modeling tabular\ndata, 2022a."
        }
    },
    {
        "id": "79b56120e61594ae27bdd0b75ca92d28f09e3d24",
        "content": "1 Introduction\nLarge language models (LLMs) are deep learning models trained on extensive data, endowing them with\nversatile problem-solving capabilities that extend far beyond the realm of natural language processing (NLP)\ntasks (Fu & Khot, 2022). Recent research has revealed emergent abilities of LLMs, such as improved\nperformance on few-shot prompted tasks (Wei et al., 2022b). The remarkable performance of LLMs have",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "1 Introduction\nLarge language models (LLMs) are deep learning models trained on extensive data, endowing them with\nversatile problem-solving capabilities that extend far beyond the realm of natural language processing (NLP)\ntasks (Fu & Khot, 2022). Recent research has revealed emergent abilities of LLMs, such as improved\nperformance on few-shot prompted tasks (Wei et al., 2022b). The remarkable performance of LLMs have"
        }
    },
    {
        "id": "86e7977775bac376dbe36423b15261e3bfdec1ca",
        "content": "Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David A. Sontag.\nTabllm: Few-shot classification of tabular data with large language models. In Francisco J. R. Ruiz,\nJennifer G. Dy, and Jan-Willem van de Meent (eds.), International Conference on Artificial Intelligence\nand Statistics, 25-27 April 2023, Palau de Congressos, Valencia, Spain, volume 206 of Proceedings of\nMachine Learning Research, pp. 5549–5581. PMLR, 2023. URL https://proceedings.mlr.press/v206/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David A. Sontag.\nTabllm: Few-shot classification of tabular data with large language models. In Francisco J. R. Ruiz,\nJennifer G. Dy, and Jan-Willem van de Meent (eds.), International Conference on Artificial Intelligence\nand Statistics, 25-27 April 2023, Palau de Congressos, Valencia, Spain, volume 206 of Proceedings of\nMachine Learning Research, pp. 5549–5581. PMLR, 2023. URL https://proceedings.mlr.press/v206/"
        }
    },
    {
        "id": "9caeb041e2e9ec1a78994511e481a02bd7eec2f2",
        "content": "the performance of this method does not surpass the in-context learning result. TabLLM (Hegselmann\net al., 2023) uses T0 model (Sanh et al., 2021) and T-few (Liu et al., 2022b) for fine-tuning. TabLLM has\ndemonstrated remarkable few-shot learning capabilities, outperforming traditional deep-learning methods\nand gradient-boosted trees. TabLLM’s efficacy is highlighted by its ability to leverage the extensive knowl-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the performance of this method does not surpass the in-context learning result. TabLLM (Hegselmann\net al., 2023) uses T0 model (Sanh et al., 2021) and T-few (Liu et al., 2022b) for fine-tuning. TabLLM has\ndemonstrated remarkable few-shot learning capabilities, outperforming traditional deep-learning methods\nand gradient-boosted trees. TabLLM’s efficacy is highlighted by its ability to leverage the extensive knowl-"
        }
    },
    {
        "id": "9e1eff84fb1da450c5c3c19f344217bfa3ea84c1",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-"
        }
    },
    {
        "id": "a696b3f4865184ee554344db0de7a1af1393fcf8",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nin selecting appropriate solutions for tabular data modeling using different LLMs. Moreover, we examine\nthe limitations of current approaches, such as susceptibility to hallucination, fairness concerns, data pre-\nprocessing intricacies, and result interpretability challenges. In light of these limitations, we discuss future\ndirections that warrant further exploration in future research endeavors.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nin selecting appropriate solutions for tabular data modeling using different LLMs. Moreover, we examine\nthe limitations of current approaches, such as susceptibility to hallucination, fairness concerns, data pre-\nprocessing intricacies, and result interpretability challenges. In light of these limitations, we discuss future\ndirections that warrant further exploration in future research endeavors."
        }
    },
    {
        "id": "d04b6b339792b8312a8e0a88c8c4dd2418253a81",
        "content": "10.18653/v1/2023.acl-long.334. URL https://aclanthology.org/2023.acl-long.334.\nZilong Zhao, Robert Birke, and Lydia Chen. Tabula: Harnessing language models for tabular data synthesis.\narXiv preprint arXiv:2310.12746, 2023f.\nShuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation in tabular\ndata. arXiv preprint arXiv:2210.17128, 2022.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "10.18653/v1/2023.acl-long.334. URL https://aclanthology.org/2023.acl-long.334.\nZilong Zhao, Robert Birke, and Lydia Chen. Tabula: Harnessing language models for tabular data synthesis.\narXiv preprint arXiv:2310.12746, 2023f.\nShuhan Zheng and Nontawat Charoenphakdee. Diffusion models for missing value imputation in tabular\ndata. arXiv preprint arXiv:2210.17128, 2022.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural"
        }
    },
    {
        "id": "da53342714bdf6a8e650cbf8903963041f8f375e",
        "content": "crucial for LLMs. There are two types of search and retrieval use-cases: (1) to find the information (table,\ncolumn, row, cell) relevant to the question, and (2) to obtain additional information and examples.\nFor main table Zhao et al. (2023d) observed that better performance of a retriever module (that returns\nthe top-n most relevant documents) consistently enhances the final accuracy of LLMs in numerical QA. Sui",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "crucial for LLMs. There are two types of search and retrieval use-cases: (1) to find the information (table,\ncolumn, row, cell) relevant to the question, and (2) to obtain additional information and examples.\nFor main table Zhao et al. (2023d) observed that better performance of a retriever module (that returns\nthe top-n most relevant documents) consistently enhances the final accuracy of LLMs in numerical QA. Sui"
        }
    },
    {
        "id": "e6198a3b224d0c98767741233a5a38da8a9b72be",
        "content": "GTLZhang et al. (2023a) Tabular Finetune Low ACC LLaMA\nSerializeLLMJaitly et al. (2023) Tabular Finetune High AUC T0\nMediTabWang et al. (2023c) Medical Finetune High PRAUC/AUCROC BioBert/GPT3.5/UnifiedQA-v2-T5\nCTRLLi et al. (2023e) Finance Finetune High AUC/LogLoss Roberta/ChatGLM\nFinPTYin et al. (2023) CTR Finetune High F1 Score FlanT5/ChatGPT/GPT4\nTable 3: Prediction methods. Resource is high if it has to finetune a model with size ≥ 1B even if it is",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "GTLZhang et al. (2023a) Tabular Finetune Low ACC LLaMA\nSerializeLLMJaitly et al. (2023) Tabular Finetune High AUC T0\nMediTabWang et al. (2023c) Medical Finetune High PRAUC/AUCROC BioBert/GPT3.5/UnifiedQA-v2-T5\nCTRLLi et al. (2023e) Finance Finetune High AUC/LogLoss Roberta/ChatGLM\nFinPTYin et al. (2023) CTR Finetune High F1 Score FlanT5/ChatGPT/GPT4\nTable 3: Prediction methods. Resource is high if it has to finetune a model with size ≥ 1B even if it is"
        }
    },
    {
        "id": "f944071ac4a4158cccde92ef1b88c1c7516becdc",
        "content": "only learning through a natural language task description and a few in-context examples provided in the\nprompt. The GPT3 model (Brown et al., 2020) with 175 billion parameters presented an impressive in-\ncontext learning ability that was not seen in smaller models. LLMs have also demonstrated the ability\nto complete new tasks by following only the instructions of the task descriptions (also known as zero-shot",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "only learning through a natural language task description and a few in-context examples provided in the\nprompt. The GPT3 model (Brown et al., 2020) with 175 billion parameters presented an impressive in-\ncontext learning ability that was not seen in smaller models. LLMs have also demonstrated the ability\nto complete new tasks by following only the instructions of the task descriptions (also known as zero-shot"
        }
    },
    {
        "id": "18601ff86770d28c38090143f0cd41ab1404fc1f",
        "content": "models are inherently order-variant, with word order significantly impacting predictions and contextual\nunderstanding, little is unknown how LLM performance varies when dealing with tabular data where orders\nof the features and records are invariant. Future research should prioritize an in-depth investigation into\ntabular-specific behaviors of LLMs to enhance their performance on tasks related to tabular data.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "models are inherently order-variant, with word order significantly impacting predictions and contextual\nunderstanding, little is unknown how LLM performance varies when dealing with tabular data where orders\nof the features and records are invariant. Future research should prioritize an in-depth investigation into\ntabular-specific behaviors of LLMs to enhance their performance on tasks related to tabular data."
        }
    },
    {
        "id": "2c6daa145bcc1d93fd017dcea69cd2d62f962f16",
        "content": "into [422, 35, 630]), complicating arithmetic. Newer models like LLaMA tokenize each digit separately. Both\napproaches make LLM difficult to understand the whole number. Also, based on Spathis & Kawsar (2023),\nthe tokenization of integers lacks a coherent decimal representation, leading to a fragmented approach where\n27",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "into [422, 35, 630]), complicating arithmetic. Newer models like LLaMA tokenize each digit separately. Both\napproaches make LLM difficult to understand the whole number. Also, based on Spathis & Kawsar (2023),\nthe tokenization of integers lacks a coherent decimal representation, leading to a fragmented approach where\n27"
        }
    },
    {
        "id": "3926d21f2d59bc5df166c8e426f70acfb00cde13",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nFigure 3: Development of language models and their applications in tabular data modeling.\nthe data and model size usually leads to improved performance, researchers sought to test the boundaries of\nPLM’s performance of a larger size, such as “text-to-text transfer transformers” (T5) (Raffel et al., 2023),\nGPT-3 (Brown et al., 2020), etc. Intriguingly, some advanced abilities emerge as a result. These large-sized",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nFigure 3: Development of language models and their applications in tabular data modeling.\nthe data and model size usually leads to improved performance, researchers sought to test the boundaries of\nPLM’s performance of a larger size, such as “text-to-text transfer transformers” (T5) (Raffel et al., 2023),\nGPT-3 (Brown et al., 2020), etc. Intriguingly, some advanced abilities emerge as a result. These large-sized"
        }
    },
    {
        "id": "5bab69c54421173ea4318bf969f81b895e39d743",
        "content": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal,\nDanielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed, A. Agarwal,\nDanielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:"
        }
    },
    {
        "id": "77f39de71980d3115fe906acb63d2d6887a86549",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\ntables in the reasoning chain for table understanding. CoRR, abs/2401.04398, 2024. doi: 10.48550/\nARXIV.2401.04398. URL https://doi.org/10.48550/arXiv.2401.04398.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022a.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\ntables in the reasoning chain for table understanding. CoRR, abs/2401.04398, 2024. doi: 10.48550/\nARXIV.2401.04398. URL https://doi.org/10.48550/arXiv.2401.04398.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2022a.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,"
        }
    },
    {
        "id": "813c03806a6f040c13235f93d16242bd15daf1db",
        "content": "Dialogue-based applications In various applications where the users are interacting with the LLMs,\nlike in chatbots, the pipeline must allow for LLMs to be called iteratively. Some dialogue-based Text2SQL\ndatasets to consider are the SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) datasets. For SParC,\nthe authors designed subsequent follow-up questions based on Spider (Yu et al., 2018b).\nWorking around constraints or error de-bugging Zhao et al. (2023a) used multi-turn prompts to",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Dialogue-based applications In various applications where the users are interacting with the LLMs,\nlike in chatbots, the pipeline must allow for LLMs to be called iteratively. Some dialogue-based Text2SQL\ndatasets to consider are the SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) datasets. For SParC,\nthe authors designed subsequent follow-up questions based on Spider (Yu et al., 2018b).\nWorking around constraints or error de-bugging Zhao et al. (2023a) used multi-turn prompts to"
        }
    },
    {
        "id": "817912e33e9d438f11c560634a49634db3331fe4",
        "content": "CLM-based methods, as depicted in Figure 5: 1) feature name preconditioning: This method involves\nproviding only a feature’s name, generating samples across the entire joint data distribution. 2) One name-\nvalue pair preconditioning: Here, when a single feature name along with its value is supplied, the LLM will\ngenerate a complete sample. This method produces samples from the conditional distribution. Sampling one",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "CLM-based methods, as depicted in Figure 5: 1) feature name preconditioning: This method involves\nproviding only a feature’s name, generating samples across the entire joint data distribution. 2) One name-\nvalue pair preconditioning: Here, when a single feature name along with its value is supplied, the LLM will\ngenerate a complete sample. This method produces samples from the conditional distribution. Sampling one"
        }
    },
    {
        "id": "8800bb98427d42d34777415fc627839dbc00547d",
        "content": "https://doi.org/10.1145/3447548.3467434.\nZifeng Wang and Jimeng Sun. Transtab: Learning transferable tabular transformers across tables, 2022.\nZifeng Wang, Chufan Gao, Cao Xiao, and Jimeng Sun. Meditab: Scaling medical tabular data predictors\nvia data consolidation, enrichment, and refinement, 2023c.\nZilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly\nMiculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. Chain-of-table: Evolving\n43",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "https://doi.org/10.1145/3447548.3467434.\nZifeng Wang and Jimeng Sun. Transtab: Learning transferable tabular transformers across tables, 2022.\nZifeng Wang, Chufan Gao, Cao Xiao, and Jimeng Sun. Meditab: Scaling medical tabular data predictors\nvia data consolidation, enrichment, and refinement, 2023c.\nZilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos, Vincent Perot, Zifeng Wang, Lesly\nMiculicich, Yasuhisa Fujii, Jingbo Shang, Chen-Yu Lee, and Tomas Pfister. Chain-of-table: Evolving\n43"
        }
    },
    {
        "id": "a5edbd8afbad9e41bda34e453e10e039e3240cd9",
        "content": "strategies to improve performance are described in the next few paragraphs. Sui et al. (2023b) recommended\nthat external information (such as questions and statements) should be placed before the tables in prompts\nfor better performance.\nIn-context learning As one of the emergent abilities of LLMs (see 1.3), in-context learning refers to\nincorporates similar examples to help the LLMs understand the desired output. Sui et al. (2023b) observed",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "strategies to improve performance are described in the next few paragraphs. Sui et al. (2023b) recommended\nthat external information (such as questions and statements) should be placed before the tables in prompts\nfor better performance.\nIn-context learning As one of the emergent abilities of LLMs (see 1.3), in-context learning refers to\nincorporates similar examples to help the LLMs understand the desired output. Sui et al. (2023b) observed"
        }
    },
    {
        "id": "adb26eb70a9990be4cf37364adf798e31843ffcf",
        "content": "values based on the predicted column distribution. TabMT initially sets the masking probability for all\ncolumn values to 1 and then predicts each value gradually, as illustrated in Figure 6.\nFigure 6: The data generation process for masked LMs\n4.2 Evaluation\nAs outlined in Zhang et al. (2023c), the evaluation of synthetic data quality can be approached from four\ndifferent dimensions: 1) Low-order statistics – column-wise density and pair-wise column correlation, es-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "values based on the predicted column distribution. TabMT initially sets the masking probability for all\ncolumn values to 1 and then predicts each value gradually, as illustrated in Figure 6.\nFigure 6: The data generation process for masked LMs\n4.2 Evaluation\nAs outlined in Zhang et al. (2023c), the evaluation of synthetic data quality can be approached from four\ndifferent dimensions: 1) Low-order statistics – column-wise density and pair-wise column correlation, es-"
        }
    },
    {
        "id": "b98f7d680c37f257428cc751c190c8fa9a115337",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\npertinent references and insightful perspectives, empowering them with the necessary tools\nand knowledge to effectively navigate and address the prevailing challenges in the field.\nhttps://github.com/tanfiona/LLM-on-Tabular-Data-Prediction-Table-\nUnderstanding-Data-Generation\n1 Introduction\nLarge language models (LLMs) are deep learning models trained on extensive data, endowing them with",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\npertinent references and insightful perspectives, empowering them with the necessary tools\nand knowledge to effectively navigate and address the prevailing challenges in the field.\nhttps://github.com/tanfiona/LLM-on-Tabular-Data-Prediction-Table-\nUnderstanding-Data-Generation\n1 Introduction\nLarge language models (LLMs) are deep learning models trained on extensive data, endowing them with"
        }
    },
    {
        "id": "d377fbe646f84abb50e6ae17063b180bd0233ddf",
        "content": "//proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nHaifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, and Yu Sun. Pre-trained language models and their\napplications. Engineering, 2022a.\nRuiyu Wang, Zifeng Wang, and Jimeng Sun. Unipredict: Large language models are universal tabular\npredictors. arXiv preprint arXiv:2310.03266, 2023a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "//proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\nHaifeng Wang, Jiwei Li, Hua Wu, Eduard Hovy, and Yu Sun. Pre-trained language models and their\napplications. Engineering, 2022a.\nRuiyu Wang, Zifeng Wang, and Jimeng Sun. Unipredict: Large language models are universal tabular\npredictors. arXiv preprint arXiv:2310.03266, 2023a.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,"
        }
    },
    {
        "id": "d77fa0959f5f1c09b6d1dccfc62145fa9dc92894",
        "content": "Embedding-based Many papers also employ table encoders, which were fine-tuned from PLMs, to encode\ntabular data into numerical representations as the input for LLMs. There are multiple table encoders, built\non BERT (Devlin et al., 2019) for table-related task, like TAPAS (Herzig et al., 2020), TABERT (Yin et al.,\n8",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Embedding-based Many papers also employ table encoders, which were fine-tuned from PLMs, to encode\ntabular data into numerical representations as the input for LLMs. There are multiple table encoders, built\non BERT (Devlin et al., 2019) for table-related task, like TAPAS (Herzig et al., 2020), TABERT (Yin et al.,\n8"
        }
    },
    {
        "id": "d9ad435a64bc4ae21aad03bea8c30d453ca7829a",
        "content": "improves LLaMA in most zero-shot scenarios. Based on the current evidence, we believe that fine-tuning on\nlarge number of datasets could further improve the performance. However, both UniPredict and GTL have\nnot released their code yet.\nMetric Among all tabular prediction methods surveyed, AUC is mostly commonly used metric for classifi-\ncation prediction and RMSE is mostly commonly used metric for regression 3\n3.3 Time Series Forecasting",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "improves LLaMA in most zero-shot scenarios. Based on the current evidence, we believe that fine-tuning on\nlarge number of datasets could further improve the performance. However, both UniPredict and GTL have\nnot released their code yet.\nMetric Among all tabular prediction methods surveyed, AUC is mostly commonly used metric for classifi-\ncation prediction and RMSE is mostly commonly used metric for regression 3\n3.3 Time Series Forecasting"
        }
    },
    {
        "id": "01a66a51f1ec1281c5cb1d99bd84a35e9b930e3b",
        "content": "columns or cells which we will discuss later in Section 5.\nAdditional information about tables for better performance Apart from the table, some papers\nexplored including table schemas and statistics as part of the prompt. Sui et al. (2023c) explored including\nadditional information about the tables: Information like “ dimension, measure, semantic field type\" help the\nLLM achieve higher accuracy across all six datasets explored. “Statistics features” improved performance for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "columns or cells which we will discuss later in Section 5.\nAdditional information about tables for better performance Apart from the table, some papers\nexplored including table schemas and statistics as part of the prompt. Sui et al. (2023c) explored including\nadditional information about the tables: Information like “ dimension, measure, semantic field type\" help the\nLLM achieve higher accuracy across all six datasets explored. “Statistics features” improved performance for"
        }
    },
    {
        "id": "121aba6eba7c3aba6e4dee0a06c6a3dfa6b49121",
        "content": "Machine Learning Research, pp. 5549–5581. PMLR, 2023. URL https://proceedings.mlr.press/v206/\nhegselmann23a.html.\n34",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Machine Learning Research, pp. 5549–5581. PMLR, 2023. URL https://proceedings.mlr.press/v206/\nhegselmann23a.html.\n34"
        }
    },
    {
        "id": "1b0f821c5378402cc75064fd9e51d863d82b036c",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nReynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive\nlanguage model, 2022.\nSebastian Bordt, Harsha Nori, and Rich Caruana. Elephants never forget: Testing language models for\nmemorization of tabular data. arXiv preprint arXiv:2403.06644, 2024.\nVadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nReynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source autoregressive\nlanguage model, 2022.\nSebastian Bordt, Harsha Nori, and Rich Caruana. Elephants never forget: Testing language models for\nmemorization of tabular data. arXiv preprint arXiv:2403.06644, 2024.\nVadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci."
        }
    },
    {
        "id": "35a8bc601d275ed6a38e1914a459eca6f588b7d7",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\n& Liang, 2015b). Semantic parsing is also used when the table is not coming from non-database tables\nsuch as web tables, spreadsheet tables, and others (Jin et al., 2022). Seq2SQL is a sequence-to-sequence\ndeep neural network using reinforcement-learning to generate conditions of query on WikiSQL task (Zhong\net al., 2017a). Some methodologies are sketch-based, wherein a natural language question is translated",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\n& Liang, 2015b). Semantic parsing is also used when the table is not coming from non-database tables\nsuch as web tables, spreadsheet tables, and others (Jin et al., 2022). Seq2SQL is a sequence-to-sequence\ndeep neural network using reinforcement-learning to generate conditions of query on WikiSQL task (Zhong\net al., 2017a). Some methodologies are sketch-based, wherein a natural language question is translated"
        }
    },
    {
        "id": "435d246df7904aa1d2ade1b807d4eb0231aea5e9",
        "content": "columns, permutation-invariant techniques are typically employed in CLM-powered methods. MLM involves\nmasking tokens in the input sequence, with the model learning to predict these masked tokens based on\nsurrounding context. This method benefits from bidirectional context, enabling consideration of both past\nand future tokens during predictions.\n4.1.1 Causal Language Modeling\nBorisov et al. (2023b) proposes the first CLM-based table generative method, GReaT11 (Generation of",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "columns, permutation-invariant techniques are typically employed in CLM-powered methods. MLM involves\nmasking tokens in the input sequence, with the model learning to predict these masked tokens based on\nsurrounding context. This method benefits from bidirectional context, enabling consideration of both past\nand future tokens during predictions.\n4.1.1 Causal Language Modeling\nBorisov et al. (2023b) proposes the first CLM-based table generative method, GReaT11 (Generation of"
        }
    },
    {
        "id": "5baa5ef564cbc618ba841eb9a3b30c84afb2095a",
        "content": "et al., 2023a) (see table 4 in that paper). This discrepancy in benchmark performance makes it impossible\nto come up with a classification performance benchmark against all methods. Therefore, there is a pressing\nneed for more standardized and unified benchmark exercise to bridge this gap effectively.\nTabular-specific challenges The current exploration of LLMs on tabular data remains primarily surface-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2023a) (see table 4 in that paper). This discrepancy in benchmark performance makes it impossible\nto come up with a classification performance benchmark against all methods. Therefore, there is a pressing\nneed for more standardized and unified benchmark exercise to bridge this gap effectively.\nTabular-specific challenges The current exploration of LLMs on tabular data remains primarily surface-"
        }
    },
    {
        "id": "6f7d397fbbabe080dabe7f05c9503ea7c51ce44b",
        "content": "Financial Prediction FinPT (Yin et al., 2023) presents an LLM-based approach to financial risk prediction.\nThe method involves filling tabular financial data into a pre-defined template, prompting LLMs like ChatGPT\nand GPT-4 to generate natural-language customer profiles. These profiles are then used to fine-tune large\nfoundation models such as BERT (Devlin et al., 2019), employing the models’ official tokenizers. The process",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Financial Prediction FinPT (Yin et al., 2023) presents an LLM-based approach to financial risk prediction.\nThe method involves filling tabular financial data into a pre-defined template, prompting LLMs like ChatGPT\nand GPT-4 to generate natural-language customer profiles. These profiles are then used to fine-tune large\nfoundation models such as BERT (Devlin et al., 2019), employing the models’ official tokenizers. The process"
        }
    },
    {
        "id": "737fdddb49c8c1fe7dd32ae382adfec67a9992a8",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language models\nare versatile decomposers: Decomposing evidence and questions for table-based reasoning. In Hsin-Hsi\nChen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete\n(eds.), Proceedings of the 46th International ACM SIGIR Conference on Research and Development in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. Large language models\nare versatile decomposers: Decomposing evidence and questions for table-based reasoning. In Hsin-Hsi\nChen, Wei-Jou (Edward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete\n(eds.), Proceedings of the 46th International ACM SIGIR Conference on Research and Development in"
        }
    },
    {
        "id": "754492112a534e673a353c977115ed2948dfa4c1",
        "content": "information in the middle of long contexts, even for explicitly long-context models (Liu et al., 2023b). For\ntabular data, Cheng et al. (2023); Sui et al. (2023c) highlights that noisy information becomes an issue in\nlarge tables for LMs. Chen (2023) found that for table sizes beyond 1000 tokens, GPT-3’s performance\ndegrades to random guesses.\nThirdly, longer prompts incur higher costs, especially for applications built upon LLM APIs.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "information in the middle of long contexts, even for explicitly long-context models (Liu et al., 2023b). For\ntabular data, Cheng et al. (2023); Sui et al. (2023c) highlights that noisy information becomes an issue in\nlarge tables for LMs. Chen (2023) found that for table sizes beyond 1000 tokens, GPT-3’s performance\ndegrades to random guesses.\nThirdly, longer prompts incur higher costs, especially for applications built upon LLM APIs."
        }
    },
    {
        "id": "8324b18042be45162d39a7036e64edc99a1f0a04",
        "content": "into a tabular format.\nFigure 5: The data generation process for causual LMs\n4.1.2 Masked Language Modeling\nThe MLM structure is suitable for generating tabular data due to its ability to capture bidirectional patterns\nbetween columns. Besides, prompting a tabular generator doesn’t follow a sequential format. MLM’s mask-\ning procedure enables arbitrary prompts during generation, making the generation process more efficient.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "into a tabular format.\nFigure 5: The data generation process for causual LMs\n4.1.2 Masked Language Modeling\nThe MLM structure is suitable for generating tabular data due to its ability to capture bidirectional patterns\nbetween columns. Besides, prompting a tabular generator doesn’t follow a sequential format. MLM’s mask-\ning procedure enables arbitrary prompts during generation, making the generation process more efficient."
        }
    },
    {
        "id": "843fca2a0d299ba599b0747b29c842342567e9d9",
        "content": "representation preceded by hline tag without any headers.\nAnother work worth mentioning is UniPredict (Wang et al., 2023a), which reformats metadata by consoli-\ndating arbitrary input M to a description of the target and the semantic descriptions of features. Feature\nserialization follows a \"column name is value\" format. The objective is to minimize the difference between\nthe output sequence generated by the adapted LLM function and the reference output sequence generated",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "representation preceded by hline tag without any headers.\nAnother work worth mentioning is UniPredict (Wang et al., 2023a), which reformats metadata by consoli-\ndating arbitrary input M to a description of the target and the semantic descriptions of features. Feature\nserialization follows a \"column name is value\" format. The objective is to minimize the difference between\nthe output sequence generated by the adapted LLM function and the reference output sequence generated"
        }
    },
    {
        "id": "aa9f2760a6eafb07b55d2fced6b345d5a42b5f14",
        "content": "https://doi.org/10.1162/tacl_a_00446.\nAvanika Narayan, Ines Chami, Laurel J. Orr, and Christopher Ré. Can foundation models wrangle your data?\nProc. VLDB Endow., 16(4):738–746, 2022. doi: 10.14778/3574245.3574258. URL https://www.vldb.org/\npvldb/vol16/p738-narayan.pdf.\nSoma Onishi and Shoya Meguro. Rethinking data augmentation for tabular data in deep learning, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "https://doi.org/10.1162/tacl_a_00446.\nAvanika Narayan, Ines Chami, Laurel J. Orr, and Christopher Ré. Can foundation models wrangle your data?\nProc. VLDB Endow., 16(4):738–746, 2022. doi: 10.14778/3574245.3574258. URL https://www.vldb.org/\npvldb/vol16/p738-narayan.pdf.\nSoma Onishi and Shoya Meguro. Rethinking data augmentation for tabular data in deep learning, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,"
        }
    },
    {
        "id": "aaa9c3dd7a190acd87ea86c02bf3734fc70e64c1",
        "content": "diction dataset to specialize its knowledge and improve its performance on the prediction. LIFT (Dinh et al.,\n2022) fine-tunes pre-trained language models like GPT-3 and GPT-J using Low-Rank Adaptation (LoRA)\non the training set. They found that LLM with general pretraining could improve performance. However,\nthe performance of this method does not surpass the in-context learning result. TabLLM (Hegselmann",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "diction dataset to specialize its knowledge and improve its performance on the prediction. LIFT (Dinh et al.,\n2022) fine-tunes pre-trained language models like GPT-3 and GPT-J using Low-Rank Adaptation (LoRA)\non the training set. They found that LLM with general pretraining could improve performance. However,\nthe performance of this method does not surpass the in-context learning result. TabLLM (Hegselmann"
        }
    },
    {
        "id": "dd692d829c8e78c2e42ac5667c8381b2e004d14d",
        "content": "Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models\nfor tables. CoRR, abs/2311.09206, 2023f. doi: 10.48550/ARXIV.2311.09206. URL https://doi.org/\n10.48550/arXiv.2311.09206.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models for\ntables, 2023g.\nWeixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models\nfor tables. CoRR, abs/2311.09206, 2023f. doi: 10.48550/ARXIV.2311.09206. URL https://doi.org/\n10.48550/arXiv.2311.09206.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models for\ntables, 2023g.\nWeixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan,"
        }
    },
    {
        "id": "e978d6661b337582e32b2a8932fae1451b46f6ee",
        "content": "DATER QA GPT3 Codex\n(Ye et al., 2023b)\nChen (2023) QA GPT3\ncTBLS QA Custom: Dense Table Retrieval based on RoBERTa\n(Sundar & Heck, 2023)\n+ Coarse State Tracking + Response based on\nGPT3.5\nGPT4Table QA GPT-3.5, GPT-4\n(Sui et al., 2023b)\nZhao et al. (2023a) QA GPT-3.5\nLiu et al. (2023e) QA GPT3.5\nTableGPT QA Custom: Phoenix-7B\n(Zha et al., 2023)\nTAP4LLM QA Instruct GPT3.5, GPT4\n(Sui et al., 2023c)\nUniTabPT QA Custom: T5, Flan-T5\n(Sarkar & Lausen, 2023)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "DATER QA GPT3 Codex\n(Ye et al., 2023b)\nChen (2023) QA GPT3\ncTBLS QA Custom: Dense Table Retrieval based on RoBERTa\n(Sundar & Heck, 2023)\n+ Coarse State Tracking + Response based on\nGPT3.5\nGPT4Table QA GPT-3.5, GPT-4\n(Sui et al., 2023b)\nZhao et al. (2023a) QA GPT-3.5\nLiu et al. (2023e) QA GPT3.5\nTableGPT QA Custom: Phoenix-7B\n(Zha et al., 2023)\nTAP4LLM QA Instruct GPT3.5, GPT4\n(Sui et al., 2023c)\nUniTabPT QA Custom: T5, Flan-T5\n(Sarkar & Lausen, 2023)"
        }
    },
    {
        "id": "ffb209466e4ba5d137ac4aa30c7d4147a529fd7a",
        "content": "Association for Computational Linguistics, pp. 5248–5264, 2020.\nIra Shavitt and Eran Segal. Regularization learning networks: deep learning for tabular datasets. Advances\nin Neural Information Processing Systems, 31, 2018.\nOfir Ben Shoham and Nadav Rappoport. Cpllm: Clinical prediction with large language models. arXiv\npreprint arXiv:2309.11295, 2023.\nRavid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,\n81:84–90, 2022.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Association for Computational Linguistics, pp. 5248–5264, 2020.\nIra Shavitt and Eran Segal. Regularization learning networks: deep learning for tabular datasets. Advances\nin Neural Information Processing Systems, 31, 2018.\nOfir Ben Shoham and Nadav Rappoport. Cpllm: Clinical prediction with large language models. arXiv\npreprint arXiv:2309.11295, 2023.\nRavid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,\n81:84–90, 2022."
        }
    },
    {
        "id": "2805eeab462e65f5bc8fe0b83cfb506d4e294907",
        "content": "generated descriptions by leveraging existing model knowledge, underscoring the potential of LLMs in new\ndomains with limited data. However, it does not perform well when there are many continuous variables.\nFor any new LLM-based prediction method without any fine-tuning, we suggest benchmarking LIFT and\nTABLET. LIFT is the first LLM-based method for inference-only prediction. TABLET shows significantly\nbetter performance than LIFT with 3 different base models.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "generated descriptions by leveraging existing model knowledge, underscoring the potential of LLMs in new\ndomains with limited data. However, it does not perform well when there are many continuous variables.\nFor any new LLM-based prediction method without any fine-tuning, we suggest benchmarking LIFT and\nTABLET. LIFT is the first LLM-based method for inference-only prediction. TABLET shows significantly\nbetter performance than LIFT with 3 different base models."
        }
    },
    {
        "id": "4beee2c4b929fec892691f1de2d4bc7ff1ab6fe9",
        "content": "Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dim-\nitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine\nlearning tasks. In Advances in Neural Information Processing Systems, 2022.\nXuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Lu Chen, Jinshu Lin, and Dongfang Lou.\nC3: zero-shot text-to-sql with chatgpt. CoRR, abs/2307.07306, 2023. doi: 10.48550/ARXIV.2307.07306.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dim-\nitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine\nlearning tasks. In Advances in Neural Information Processing Systems, 2022.\nXuemei Dong, Chao Zhang, Yuhang Ge, Yuren Mao, Yunjun Gao, Lu Chen, Jinshu Lin, and Dongfang Lou.\nC3: zero-shot text-to-sql with chatgpt. CoRR, abs/2307.07306, 2023. doi: 10.48550/ARXIV.2307.07306."
        }
    },
    {
        "id": "5303bd95b1194dd2e6fa921dc242b677d74e679d",
        "content": "Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\nramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Au-\nrélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\nand fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Sub-\nramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Au-\nrélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\nand fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL"
        }
    },
    {
        "id": "6a6d05f972ccaa1cd916ed56bd08816d527020d1",
        "content": "2021), Net-DNF (Katzir et al., 2020)). Another subcategory of methods combine tree-based models with\ndeep neural networks, thus can maintain tree’s capabilities on handling sparse categorical features (Deep-\nGBM (Ke et al., 2019a)), borrow prior structural knowledge from the tree (TabNN (Ke et al., 2019b)),\nor exploit topological information by converting structured data into a directed graph (BGNN (Ivanov &\nProkhorenkova, 2021). 3. Attention-based methods. These models incorporate attention mechanisms for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2021), Net-DNF (Katzir et al., 2020)). Another subcategory of methods combine tree-based models with\ndeep neural networks, thus can maintain tree’s capabilities on handling sparse categorical features (Deep-\nGBM (Ke et al., 2019a)), borrow prior structural knowledge from the tree (TabNN (Ke et al., 2019b)),\nor exploit topological information by converting structured data into a directed graph (BGNN (Ivanov &\nProkhorenkova, 2021). 3. Attention-based methods. These models incorporate attention mechanisms for"
        }
    },
    {
        "id": "7b99b1364dd30fe45a9c4eec0b97e883fe418053",
        "content": "C3: zero-shot text-to-sql with chatgpt. CoRR, abs/2307.07306, 2023. doi: 10.48550/ARXIV.2307.07306.\nURL https://doi.org/10.48550/arXiv.2307.07306.\nYuntao Du and Ninghui Li. Towards principled assessment of tabular data synthesis algorithms. arXiv\npreprint arXiv:2402.06806, 2024.\nJulian Eberius, Katrin Braunschweig, Markus Hentsch, Maik Thiele, Ahmad Ahmadov, and Wolfgang\nLehner. Building the dresden web table corpus: A classification approach. In Ioan Raicu, Omer F.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "C3: zero-shot text-to-sql with chatgpt. CoRR, abs/2307.07306, 2023. doi: 10.48550/ARXIV.2307.07306.\nURL https://doi.org/10.48550/arXiv.2307.07306.\nYuntao Du and Ninghui Li. Towards principled assessment of tabular data synthesis algorithms. arXiv\npreprint arXiv:2402.06806, 2024.\nJulian Eberius, Katrin Braunschweig, Markus Hentsch, Maik Thiele, Ahmad Ahmadov, and Wolfgang\nLehner. Building the dresden web table corpus: A classification approach. In Ioan Raicu, Omer F."
        }
    },
    {
        "id": "7d76c231f46ae41b43d364a103b426d9f58992b9",
        "content": "multiple name-value pairs for arbitrary conditioning. The model then efficiently samples from the distribution\nof the remaining features. After that, we use cell value extraction methods, such as standard pattern-\nmatching algorithms and regular expressions, to transform the generated pre-defined serialized text data\ninto a tabular format.\nFigure 5: The data generation process for causual LMs\n4.1.2 Masked Language Modeling",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "multiple name-value pairs for arbitrary conditioning. The model then efficiently samples from the distribution\nof the remaining features. After that, we use cell value extraction methods, such as standard pattern-\nmatching algorithms and regular expressions, to transform the generated pre-defined serialized text data\ninto a tabular format.\nFigure 5: The data generation process for causual LMs\n4.1.2 Masked Language Modeling"
        }
    },
    {
        "id": "7dd17bbc86e25c804a96187aad0218080ef2cbbf",
        "content": "2023, pp. 622–630. Association for Computational Linguistics, 2023c. URL https://aclanthology.org/\n2023.emnlp-industry.59.\nTennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. Goggle: Generative modelling\nfor tabular data by learning relational structure. In The Eleventh International Conference on Learning\nRepresentations, 2023d.\nTianyang Liu, Fei Wang, and Muhao Chen. Rethinking tabular data understanding with large language\nmodels, 2023e.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2023, pp. 622–630. Association for Computational Linguistics, 2023c. URL https://aclanthology.org/\n2023.emnlp-industry.59.\nTennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. Goggle: Generative modelling\nfor tabular data by learning relational structure. In The Eleventh International Conference on Learning\nRepresentations, 2023d.\nTianyang Liu, Fei Wang, and Muhao Chen. Rethinking tabular data understanding with large language\nmodels, 2023e."
        }
    },
    {
        "id": "8a91ab2830aa3cc722715576736b09ce5dad42cf",
        "content": "Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao\nLiang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Phoenix: De-\nmocratizing chatgpt across languages. CoRR, abs/2304.10453, 2023c. doi: 10.48550/ARXIV.2304.10453.\nURL https://doi.org/10.48550/arXiv.2304.10453.\nZhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davison. Table search using a deep con-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao\nLiang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. Phoenix: De-\nmocratizing chatgpt across languages. CoRR, abs/2304.10453, 2023c. doi: 10.48550/ARXIV.2304.10453.\nURL https://doi.org/10.48550/arXiv.2304.10453.\nZhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davison. Table search using a deep con-"
        }
    },
    {
        "id": "9a62aaecbbe06befd88537912c4f7690811003be",
        "content": "and LLaMA, respectively. It uses spaces and commas for separation and omitting decimal points. Time\nLLM (Jin et al., 2023a) involves concatenating time series sequences into embeddings and integrating them\nwith word embeddings to create a comprehensive input. This input is complemented by dataset context,\ntask instructions, and input statistics as a prefix. TEST (Sun et al., 2023a) introduces an embedding layer",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and LLaMA, respectively. It uses spaces and commas for separation and omitting decimal points. Time\nLLM (Jin et al., 2023a) involves concatenating time series sequences into embeddings and integrating them\nwith word embeddings to create a comprehensive input. This input is complemented by dataset context,\ntask instructions, and input statistics as a prefix. TEST (Sun et al., 2023a) introduces an embedding layer"
        }
    },
    {
        "id": "9f27fc8e451692ac5c52637836d3e83f358f54e2",
        "content": "to image or text data. Thus, this line of research seeks to design an optimal and dynamic regularization\nmechanism to adjust the sensitivity of the model to certain inputs (e.g. RLN (Shavitt & Segal, 2018), Reg-\nularization Cocktails (Kadra et al., 2021). In spite of rigorous attempts in applying deep learning to tabular\ndata modeling, GBDT algorithms, including XGBoost, LightGBM, and CatBoost (Prokhorenkova et al.,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to image or text data. Thus, this line of research seeks to design an optimal and dynamic regularization\nmechanism to adjust the sensitivity of the model to certain inputs (e.g. RLN (Shavitt & Segal, 2018), Reg-\nularization Cocktails (Kadra et al., 2021). In spite of rigorous attempts in applying deep learning to tabular\ndata modeling, GBDT algorithms, including XGBoost, LightGBM, and CatBoost (Prokhorenkova et al.,"
        }
    },
    {
        "id": "a0443a6cebc328d2df6e3aef3e8685b9d9ac58ad",
        "content": "2 |0 |helen | 47| Zhao et al. (2023d); Sui et al. (2023b)\nLaTeX Rows are separated by \\\\\\hline helen & 47 Jaitly et al. (2023)\n“\\\\\\hline”, columns are\nseparated by “&”\nX- Rows are line-separated, , name, age Singha et al. (2023); Narayan et al.\nSeparated columns are separated by “,”, 0, helen, 47 (2022)\n“\\t”, “:”, etc.\nAttribute- Concatenation of paired name:helen ; age:47 Wang et al. (2023c)\nValue Pairs columns and cells {c : v}",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2 |0 |helen | 47| Zhao et al. (2023d); Sui et al. (2023b)\nLaTeX Rows are separated by \\\\\\hline helen & 47 Jaitly et al. (2023)\n“\\\\\\hline”, columns are\nseparated by “&”\nX- Rows are line-separated, , name, age Singha et al. (2023); Narayan et al.\nSeparated columns are separated by “,”, 0, helen, 47 (2022)\n“\\t”, “:”, etc.\nAttribute- Concatenation of paired name:helen ; age:47 Wang et al. (2023c)\nValue Pairs columns and cells {c : v}"
        }
    },
    {
        "id": "a1a4f557378ad76cdf59494ced47c3a0a4bf6f99",
        "content": "Maria Sahakyan, Zeyar Aung, and Talal Rahwan. Explainable artificial intelligence for tabular data: A\nsurvey. IEEE access, 9:135392–135422, 2021.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Maria Sahakyan, Zeyar Aung, and Talal Rahwan. Explainable artificial intelligence for tabular data: A\nsurvey. IEEE access, 9:135392–135422, 2021.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-"
        }
    },
    {
        "id": "a2ed79569fe08ce5ea0e2144d6cabc8f8d553f39",
        "content": "(Zhong et al., SQL Question swer notated Lausen, 2023), 65.60 (Jiang et al.,\n2017b) 2023), 50.48 (Zhang et al., 2023g)\nTable 7: Overview of popular QA/ reasoning datasets and related work for LLMs that worked on tabular\ndata. Only datasets that have been used by more than one relevant method are included in this table.\nTable QA For table QA datasets, FetaQA (Nan et al., 2022), WikiTableQuestion (Pasupat & Liang, 2015a),",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(Zhong et al., SQL Question swer notated Lausen, 2023), 65.60 (Jiang et al.,\n2017b) 2023), 50.48 (Zhang et al., 2023g)\nTable 7: Overview of popular QA/ reasoning datasets and related work for LLMs that worked on tabular\ndata. Only datasets that have been used by more than one relevant method are included in this table.\nTable QA For table QA datasets, FetaQA (Nan et al., 2022), WikiTableQuestion (Pasupat & Liang, 2015a),"
        }
    },
    {
        "id": "c39c6e35c702de851f434d7b09c67fbaa019af44",
        "content": "Attribute- Concatenation of paired name:helen ; age:47 Wang et al. (2023c)\nValue Pairs columns and cells {c : v}\nHTML HTML element for tabular <table><thead><tr><th></th> Singha et al. (2023); Sui et al. (2023c;b)\ndata <th>name</th><th>age</th></tr>\n</thead><tbody><tr><th>0</th>\n<td>helen</td><td>47</td></tr>\n</tbody></table>\nSentences Rows are converted into sen- name is helen, age is 47 Yu et al. (2023); Hegselmann et al.\ntences using templates (2023); Gong et al. (2020); Dinh et al.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Attribute- Concatenation of paired name:helen ; age:47 Wang et al. (2023c)\nValue Pairs columns and cells {c : v}\nHTML HTML element for tabular <table><thead><tr><th></th> Singha et al. (2023); Sui et al. (2023c;b)\ndata <th>name</th><th>age</th></tr>\n</thead><tbody><tr><th>0</th>\n<td>helen</td><td>47</td></tr>\n</tbody></table>\nSentences Rows are converted into sen- name is helen, age is 47 Yu et al. (2023); Hegselmann et al.\ntences using templates (2023); Gong et al. (2020); Dinh et al."
        }
    },
    {
        "id": "c99e5c3ab659d89a2b5fa4c3e28eec7333a4af84",
        "content": "temporal structure of the data, which can be leveraged by the model during training. However, in\ntabular data, such prior knowledge is often lacking, making it challenging for the model to understand\nthe inherent relationships between features (Borisov et al., 2022a; 2023a).\n1.2 Traditional and deep learning in tabular data\nThis survey explores the current research landscape of LLMs in tabular data prediction, with a focus on\nclassification task, data generation, and table understanding.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "temporal structure of the data, which can be leveraged by the model during training. However, in\ntabular data, such prior knowledge is often lacking, making it challenging for the model to understand\nthe inherent relationships between features (Borisov et al., 2022a; 2023a).\n1.2 Traditional and deep learning in tabular data\nThis survey explores the current research landscape of LLMs in tabular data prediction, with a focus on\nclassification task, data generation, and table understanding."
        }
    },
    {
        "id": "13be85e0ea5470bd5d4ddb89fedf4811616670b2",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nsection, we summarize the general findings of LLMs in QA tasks and highlight models that have reported to\nwork well.\nNumerical QA A niche QA task involves answering questions that require mathematical reasoning. An\nexample query could be “What is the average payment volume per transaction for American Express?”\nMany real-world QA applications (E.g. working with financial documents, annual reports, etc.) involve such",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nsection, we summarize the general findings of LLMs in QA tasks and highlight models that have reported to\nwork well.\nNumerical QA A niche QA task involves answering questions that require mathematical reasoning. An\nexample query could be “What is the average payment volume per transaction for American Express?”\nMany real-world QA applications (E.g. working with financial documents, annual reports, etc.) involve such"
        }
    },
    {
        "id": "220d2d63efe6ae183b0f4941d5442ed52c2b461a",
        "content": "Samuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman, Prashant Reddy, and Manuela\nVeloso. Generating synthetic data in finance: opportunities, challenges and pitfalls. In Proceedings of the\nFirst ACM International Conference on AI in Finance, pp. 1–8, 2020.\nGilbert Badaro, Mohammed Saeed, and Paolo Papotti. Transformers for tabular data representation: A\nsurvey of models and applications. Transactions of the Association for Computational Linguistics, 11:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Samuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman, Prashant Reddy, and Manuela\nVeloso. Generating synthetic data in finance: opportunities, challenges and pitfalls. In Proceedings of the\nFirst ACM International Conference on AI in Finance, pp. 1–8, 2020.\nGilbert Badaro, Mohammed Saeed, and Paolo Papotti. Transformers for tabular data representation: A\nsurvey of models and applications. Transactions of the Association for Computational Linguistics, 11:"
        }
    },
    {
        "id": "460d99ef2439044e14f5c6da86db4f8a468dcd5b",
        "content": "obtaining increased scores for five datasets. Liu et al. (2023e) also investigated strategies around SC, along\nwith self-evaluation, which guides the LLM to choose between the two reasoning approaches based on the\nquestion’s nature and each answer’s clarity. Deng et al. (2022b) did consensus voting across a sample a set\nof candidate sequences, then selected final response by ensembling the derived response based on plurality\nvoting.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "obtaining increased scores for five datasets. Liu et al. (2023e) also investigated strategies around SC, along\nwith self-evaluation, which guides the LLM to choose between the two reasoning approaches based on the\nquestion’s nature and each answer’s clarity. Deng et al. (2022b) did consensus voting across a sample a set\nof candidate sequences, then selected final response by ensembling the derived response based on plurality\nvoting."
        }
    },
    {
        "id": "55037067bee80002d0e3fd46d3389b721c30ddd7",
        "content": "hire annotators to evaluate the Informativeness, Coherence and Fluency of the LLM responses Zhang et al.\n(2023h). When connected to programs like Python, Power BI, etc, LLMs’ outputs are not limited to text\nand code. For example, creating visualizations from text and table inputs are a popular task too Zhang\net al. (2023h); Zha et al. (2023).\n6 Limitations and future directions\nLLMs have been used in many tabular data applications, such as predictions, data synthesis, question",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "hire annotators to evaluate the Informativeness, Coherence and Fluency of the LLM responses Zhang et al.\n(2023h). When connected to programs like Python, Power BI, etc, LLMs’ outputs are not limited to text\nand code. For example, creating visualizations from text and table inputs are a popular task too Zhang\net al. (2023h); Zha et al. (2023).\n6 Limitations and future directions\nLLMs have been used in many tabular data applications, such as predictions, data synthesis, question"
        }
    },
    {
        "id": "5efc4b1a48217489e3274d78f1814cf99ccf7be8",
        "content": "Borisov et al. (2023b) proposes the first CLM-based table generative method, GReaT11 (Generation of\nRealistic Tabular data) to generate synthetic samples with original tabular data characteristics. The GReaT\ndata pipeline involves a textual encoding step transforming tabular data into meaningful text using the\nsentences serialization methods as shown in Table 1, followed by fine-tuning a GPT-2 or GPT-2 distill",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Borisov et al. (2023b) proposes the first CLM-based table generative method, GReaT11 (Generation of\nRealistic Tabular data) to generate synthetic samples with original tabular data characteristics. The GReaT\ndata pipeline involves a textual encoding step transforming tabular data into meaningful text using the\nsentences serialization methods as shown in Table 1, followed by fine-tuning a GPT-2 or GPT-2 distill"
        }
    },
    {
        "id": "704c4f8bf22f58b07700c1549bc0338da8b467ba",
        "content": "some benefit in using LMs fine-tuned on tabular tasks.\nHowever, compared to methodologies working on Prediction and Generation tasks, fine-tuning is not as\ncommon. This might be due to the general ability of LLMs (E.g. GPT3.5, GPT4) to perform QA tasks\noff-the-shelf. For SQL generation on Spider, DIN-SQL (Pourreza & Rafiei, 2023) and DAIL-SQL (Gao\net al., 2024) are inference-based techniques using GPT4, and surpassed previous fine-tuned smaller models.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "some benefit in using LMs fine-tuned on tabular tasks.\nHowever, compared to methodologies working on Prediction and Generation tasks, fine-tuning is not as\ncommon. This might be due to the general ability of LLMs (E.g. GPT3.5, GPT4) to perform QA tasks\noff-the-shelf. For SQL generation on Spider, DIN-SQL (Pourreza & Rafiei, 2023) and DAIL-SQL (Gao\net al., 2024) are inference-based techniques using GPT4, and surpassed previous fine-tuned smaller models."
        }
    },
    {
        "id": "8ad23023cd2ab6971945d907d6786ff96bd0b707",
        "content": "V1/2023.FINDINGS-EACL.83. URL https://doi.org/10.18653/v1/2023.findings-eacl.83.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. Tabfact : A large-scale dataset for table-based fact verification. In International\nConference on Learning Representations (ICLR), Addis Ababa, Ethiopia, April 2020a.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. Hybridqa:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "V1/2023.FINDINGS-EACL.83. URL https://doi.org/10.18653/v1/2023.findings-eacl.83.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. Tabfact : A large-scale dataset for table-based fact verification. In International\nConference on Learning Representations (ICLR), Addis Ababa, Ethiopia, April 2020a.\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. Hybridqa:"
        }
    },
    {
        "id": "b47f5ade6183242fa2501fc55225618336527fd3",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\n5.1 Dataset\nTable 7 outlines some of the popular datasets and benchmark in the literature working on tabular QA tasks.\nOther relevant but less commonly cited datasets are mentioned below.\nDataset #Tables Task Input Output Data Evaluation Metric & Best Scores\nType Source Reported\nFetaQA (Nan 10330 QA Table Answer Wikipedia BLEU: 39.05 (Zhang et al., 2023g),\net al., 2022) Question 35.12 (Sarkar & Lausen, 2023), 30.92",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\n5.1 Dataset\nTable 7 outlines some of the popular datasets and benchmark in the literature working on tabular QA tasks.\nOther relevant but less commonly cited datasets are mentioned below.\nDataset #Tables Task Input Output Data Evaluation Metric & Best Scores\nType Source Reported\nFetaQA (Nan 10330 QA Table Answer Wikipedia BLEU: 39.05 (Zhang et al., 2023g),\net al., 2022) Question 35.12 (Sarkar & Lausen, 2023), 30.92"
        }
    },
    {
        "id": "beff3f1d799b6bdcf94deafbc39b6e59a5ffecdc",
        "content": "structures must still be converted back to text. For Zhao et al. (2023a), after converting the table into a\ntree, each cell’s hierarchical structure, position information, and content was represented as a tuple and fed\ninto GPT3.5.\nComparisons Research has shown that LLM performance is sensitive to the input tabular formats. Singha\net al. (2023) found that DFLoader and JSON formats are better for fact-finding and table transformation",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "structures must still be converted back to text. For Zhao et al. (2023a), after converting the table into a\ntree, each cell’s hierarchical structure, position information, and content was represented as a tuple and fed\ninto GPT3.5.\nComparisons Research has shown that LLM performance is sensitive to the input tabular formats. Singha\net al. (2023) found that DFLoader and JSON formats are better for fact-finding and table transformation"
        }
    },
    {
        "id": "c93678fd12fd2c1a95d911b51357870ff2f339dd",
        "content": "et al. (2023) found that DFLoader and JSON formats are better for fact-finding and table transformation\ntasks. Meanwhile, Sui et al. (2023a) found that HTML or XML table formats are better understood by GPT\nmodels over tabular QA and FV tasks. However, they require increased token consumption. Likewise, Sui\net al. (2023b) also found that markup languages, specifically HTML, outperformed X-separated formats for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al. (2023) found that DFLoader and JSON formats are better for fact-finding and table transformation\ntasks. Meanwhile, Sui et al. (2023a) found that HTML or XML table formats are better understood by GPT\nmodels over tabular QA and FV tasks. However, they require increased token consumption. Likewise, Sui\net al. (2023b) also found that markup languages, specifically HTML, outperformed X-separated formats for"
        }
    },
    {
        "id": "ca9f35aedeb18623a959edd88efb2ff71fa37dfe",
        "content": "series as univariate series for sequential template filling.\nTarget Augmentation In terms of output mapping, ZeroTS (Gruver et al., 2023) proposes drawing multiple\nsamples and using statistical methods or quantiles for point estimates or ranges. For Time-LLM (Jin et al.,\n2023a), the output processing is done through flattening and linear projection. The target augmentation\nmethod of ZeroTS is easy to implement 8.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "series as univariate series for sequential template filling.\nTarget Augmentation In terms of output mapping, ZeroTS (Gruver et al., 2023) proposes drawing multiple\nsamples and using statistical methods or quantiles for point estimates or ranges. For Time-LLM (Jin et al.,\n2023a), the output processing is done through flattening and linear projection. The target augmentation\nmethod of ZeroTS is easy to implement 8."
        }
    },
    {
        "id": "d9f4b59fc942dc5456ad25c9c23c5693762d9612",
        "content": "pacting the model’s performance. Additionally, there are issues with poorly represented categorical feature,\nsuch as nonsensical characters, which the model struggles to process and understand effectively. Another\nconcern is inadequate or ambiguous metadata, characterized by unclear or meaningless column names and\nmetadata, leading to confusion in the model’s interpretation of inputs. Better categorical features encod-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "pacting the model’s performance. Additionally, there are issues with poorly represented categorical feature,\nsuch as nonsensical characters, which the model struggles to process and understand effectively. Another\nconcern is inadequate or ambiguous metadata, characterized by unclear or meaningless column names and\nmetadata, leading to confusion in the model’s interpretation of inputs. Better categorical features encod-"
        }
    },
    {
        "id": "ebb2e114e642b7f63edc40d41123c029d6c52085",
        "content": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pp. 3446–3456, Online, June 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.naacl-main.270. URL https://aclanthology.org/2021.naacl-main.270.\nSergei Ivanov and Liudmila Prokhorenkova. Boost then convolve: Gradient boosting meets graph neural\nnetworks, 2021.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pp. 3446–3456, Online, June 2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.naacl-main.270. URL https://aclanthology.org/2021.naacl-main.270.\nSergei Ivanov and Liudmila Prokhorenkova. Boost then convolve: Gradient boosting meets graph neural\nnetworks, 2021.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. Search-based neural structured learning for sequential"
        }
    },
    {
        "id": "fd161da201a2e70d071f26faa844cfc88b75da1b",
        "content": "Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation"
        }
    },
    {
        "id": "20a41859c43cef87d7f05fe9f1387773d1a0a0e9",
        "content": "creation of synthetic replicas of sensitive data, protecting confidential information while still preserving the\nstatistical properties essential for analysis. Additionally, tabular data synthesis aids in data preprocessing,\nfilling missing values (Zheng & Charoenphakdee, 2022) and ensuring dataset integrity and completeness.\nThis enhances the reliability of subsequent analyses and model building.\nRecent studies have increasingly relied on LLMs to synthesize tabular data, leveraging their advanced gener-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "creation of synthetic replicas of sensitive data, protecting confidential information while still preserving the\nstatistical properties essential for analysis. Additionally, tabular data synthesis aids in data preprocessing,\nfilling missing values (Zheng & Charoenphakdee, 2022) and ensuring dataset integrity and completeness.\nThis enhances the reliability of subsequent analyses and model building.\nRecent studies have increasingly relied on LLMs to synthesize tabular data, leveraging their advanced gener-"
        }
    },
    {
        "id": "2e93af2298211b22c7217a7908503bd0cbba9ced",
        "content": "However, for the same dataset, the same method, and the same task, different papers report different perfor-\nmance. For prediction task, the performance of TabLLM (Hegselmann et al., 2023) in Blood dataset (Kadra\net al., 2021) is 0.70 in GTL (Zhang et al., 2023a) (see table 2 in that paper) and 0.66 in UniPredict (Wang\net al., 2023a) (see table 4 in that paper). This discrepancy in benchmark performance makes it impossible",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "However, for the same dataset, the same method, and the same task, different papers report different perfor-\nmance. For prediction task, the performance of TabLLM (Hegselmann et al., 2023) in Blood dataset (Kadra\net al., 2021) is 0.70 in GTL (Zhang et al., 2023a) (see table 2 in that paper) and 0.66 in UniPredict (Wang\net al., 2023a) (see table 4 in that paper). This discrepancy in benchmark performance makes it impossible"
        }
    },
    {
        "id": "61c42c0f2fe09048a8d122628d8771fb6d0d7c4f",
        "content": "Dylan Slack and Sameer Singh. Tablet: Learning from instructions for tabular data, 2023.\nAivin V Solatorio and Olivier Dupriez. Realtabformer: Generating realistic relational and tabular data using\ntransformers. arXiv preprint arXiv:2302.02041, 2023.\nGowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, and Tom Goldstein. Saint:\nImproved neural networks for tabular data via row attention and contrastive pre-training, 2021.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Dylan Slack and Sameer Singh. Tablet: Learning from instructions for tabular data, 2023.\nAivin V Solatorio and Olivier Dupriez. Realtabformer: Generating realistic relational and tabular data using\ntransformers. arXiv preprint arXiv:2302.02041, 2023.\nGowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C. Bayan Bruss, and Tom Goldstein. Saint:\nImproved neural networks for tabular data via row attention and contrastive pre-training, 2021."
        }
    },
    {
        "id": "7ce489a8c62c979bbc573c79e788edaead6a7cd2",
        "content": "models are few-shot learners, 2020.\nShaofeng Cai, Kaiping Zheng, Gang Chen, H. V. Jagadish, Beng Chin Ooi, and Meihui Zhang. Arm-net:\nAdaptive relation modeling network for structured data. In Proceedings of the 2021 International Con-\nference on Management of Data, SIGMOD/PODS ’21. ACM, June 2021. doi: 10.1145/3448016.3457321.\nURL http://dx.doi.org/10.1145/3448016.3457321.\nShuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot, single-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "models are few-shot learners, 2020.\nShaofeng Cai, Kaiping Zheng, Gang Chen, H. V. Jagadish, Beng Chin Ooi, and Meihui Zhang. Arm-net:\nAdaptive relation modeling network for structured data. In Proceedings of the 2021 International Con-\nference on Management of Data, SIGMOD/PODS ’21. ACM, June 2021. doi: 10.1145/3448016.3457321.\nURL http://dx.doi.org/10.1145/3448016.3457321.\nShuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot, single-"
        }
    },
    {
        "id": "9fec941f21fa5817186d78916b4ebd1b31f0311e",
        "content": "Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language\nmodels are realistic tabular data generators. In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. URL https:\n//openreview.net/pdf?id=cEygmQNOeI.\nVadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Vadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language\nmodels are realistic tabular data generators. In The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. URL https:\n//openreview.net/pdf?id=cEygmQNOeI.\nVadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models"
        }
    },
    {
        "id": "d34d6e8c0de087391fe2742407667917b1b40adf",
        "content": "and adds a minimal description of the task; as output, it uses the target after it converts it to a sentence.\nZeroTS (Gruver et al., 2023) argues that numbers are not encoded well in the original LLM encoding\nmethod. Thus, it encodes numbers by breaking them down by a few digits or by each single digit for GPT-3\nand LLaMA, respectively. It uses spaces and commas for separation and omitting decimal points. Time",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and adds a minimal description of the task; as output, it uses the target after it converts it to a sentence.\nZeroTS (Gruver et al., 2023) argues that numbers are not encoded well in the original LLM encoding\nmethod. Thus, it encodes numbers by breaking them down by a few digits or by each single digit for GPT-3\nand LLaMA, respectively. It uses spaces and commas for separation and omitting decimal points. Time"
        }
    },
    {
        "id": "d51cb52c62fc3b0d2ef62dddac1a7e114976f2b8",
        "content": "arXiv:2403.01570, 2024a.\nJiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen.\nMaking pre-trained language models great on tabular prediction. arXiv preprint arXiv:2403.01841, 2024b.\nBohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, and Chenghua Lin. Effective distillation of table-based\nreasoning ability from llms, 2023.\nChao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao. Ct-bert: Learning",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "arXiv:2403.01570, 2024a.\nJiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen.\nMaking pre-trained language models great on tabular prediction. arXiv preprint arXiv:2403.01841, 2024b.\nBohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, and Chenghua Lin. Effective distillation of table-based\nreasoning ability from llms, 2023.\nChao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao. Ct-bert: Learning"
        }
    },
    {
        "id": "d72f965a55edcb02230b8a2846d88ad6d2d6041f",
        "content": "relevance to the query as obtained from the coarse state tracker. The prompt includes the dialogue history,\nranked knowledge sources, and the query to be answered. Their method produced more coherent responses\nthan previous methods, suggesting that improvements in table retrieval, knowledge retrieval, and response\ngeneration lead to better downstream performance. Zhao et al. (2023d) used OpenAI’s Ada Embedding4",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "relevance to the query as obtained from the coarse state tracker. The prompt includes the dialogue history,\nranked knowledge sources, and the query to be answered. Their method produced more coherent responses\nthan previous methods, suggesting that improvements in table retrieval, knowledge retrieval, and response\ngeneration lead to better downstream performance. Zhao et al. (2023d) used OpenAI’s Ada Embedding4"
        }
    },
    {
        "id": "ef48e0e25e76e4e523f098a6495b74b57a402673",
        "content": "methods like prompt engineering. NumQA: Numerical QA.\n5.2 General ability of LLMs in QA\nTable 8 outlines papers that investigated the effectiveness of LLMs on QA and reasoning, and the models\nexplored. The most popular LLM used today is GPT3.5 and GPT4. Although these GPT models were not\nspecifically optimized for table-based tasks, many of these papers found them to be competent in performing\ncomplex table reasoning tasks, especially when combined with prompt engineering tricks like CoT. In this",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "methods like prompt engineering. NumQA: Numerical QA.\n5.2 General ability of LLMs in QA\nTable 8 outlines papers that investigated the effectiveness of LLMs on QA and reasoning, and the models\nexplored. The most popular LLM used today is GPT3.5 and GPT4. Although these GPT models were not\nspecifically optimized for table-based tasks, many of these papers found them to be competent in performing\ncomplex table reasoning tasks, especially when combined with prompt engineering tricks like CoT. In this"
        }
    },
    {
        "id": "ffe78ba7bdb7d6c3a47f2dd07eab421f7fa8b1f0",
        "content": "generate a complete sample. This method produces samples from the conditional distribution. Sampling one\ndata point from a single feature distribution is generally feasible and then use name-value pair Preconditioning\nto generate the rest of the features. 3) Multiple Name-Value Pair Preconditioning: This involves providing\nmultiple name-value pairs for arbitrary conditioning. The model then efficiently samples from the distribution",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "generate a complete sample. This method produces samples from the conditional distribution. Sampling one\ndata point from a single feature distribution is generally feasible and then use name-value pair Preconditioning\nto generate the rest of the features. 3) Multiple Name-Value Pair Preconditioning: This involves providing\nmultiple name-value pairs for arbitrary conditioning. The model then efficiently samples from the distribution"
        }
    },
    {
        "id": "1770fb56be1026b445fa433843cbf9bb814e1d6d",
        "content": "Jiang et al. (2023) QA GPT3.5, ChatGPT3.5\nLiu et al. (2023c) QA & Text2SQL Vicuna, GPT4\nGao et al. (2024) Text2SQL GPT4\nPourreza & Rafiei (2023) Text2SQL GPT4\nHuang et al. (2023b) Text2SQL GPT4\nDong et al. (2023) Text2SQL ChatGPT3.5\nChang & Fosler-Lussier (2023) Text2SQL GPT3 Codex, ChatGPT3.5\nZhang et al. (2023d) Text2SQL LLaMA2 70b\nAbraham et al. (2022) Text2SQL Custom: Table Selector + Known & Unknown Fields\nExtractor + AggFn Classifier",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jiang et al. (2023) QA GPT3.5, ChatGPT3.5\nLiu et al. (2023c) QA & Text2SQL Vicuna, GPT4\nGao et al. (2024) Text2SQL GPT4\nPourreza & Rafiei (2023) Text2SQL GPT4\nHuang et al. (2023b) Text2SQL GPT4\nDong et al. (2023) Text2SQL ChatGPT3.5\nChang & Fosler-Lussier (2023) Text2SQL GPT3 Codex, ChatGPT3.5\nZhang et al. (2023d) Text2SQL LLaMA2 70b\nAbraham et al. (2022) Text2SQL Custom: Table Selector + Known & Unknown Fields\nExtractor + AggFn Classifier"
        }
    },
    {
        "id": "1aecff0383a236b36573a39aa10262348dc10bb1",
        "content": "1. A formal break down of key techniques for LLMs’ applications on tabular data We\nsplit the application of LLM in tabular data to tabular data prediction, tabular data synthesis,\ntabular data question answering and table understanding. We further extract key techniques that\ncan apply to all applications. We organize these key techniques in a taxonomy that researchers and\npractitioners can leverage to describe their methods, find relevant techniques and understand the",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "1. A formal break down of key techniques for LLMs’ applications on tabular data We\nsplit the application of LLM in tabular data to tabular data prediction, tabular data synthesis,\ntabular data question answering and table understanding. We further extract key techniques that\ncan apply to all applications. We organize these key techniques in a taxonomy that researchers and\npractitioners can leverage to describe their methods, find relevant techniques and understand the"
        }
    },
    {
        "id": "26e075d144c240cfa11255600d5bcc03a80c991d",
        "content": "names, value consistency, data coverage, and granularity.\nRobustness of LLM performance to table manipulations Liu et al. (2023e) critically analyzed the\nrobustness of GPT3.5 across structural perturbations in tables (transpose and shuffle). They find that\nLLMs suffer from structural bias in the interpretation of table orientations, and when tasked to transpose\nthe table, LLMs performs poorly ( 50% accuracy). However, LLMs can identify if the first row or first column\n10",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "names, value consistency, data coverage, and granularity.\nRobustness of LLM performance to table manipulations Liu et al. (2023e) critically analyzed the\nrobustness of GPT3.5 across structural perturbations in tables (transpose and shuffle). They find that\nLLMs suffer from structural bias in the interpretation of table orientations, and when tasked to transpose\nthe table, LLMs performs poorly ( 50% accuracy). However, LLMs can identify if the first row or first column\n10"
        }
    },
    {
        "id": "2e2f28c85cad1b6dc5b65ea070b7315090cfd6b9",
        "content": "application, we categorize and discuss a wide range of metrics that can be used to evaluate the\nperformance of that application. For each application, we documented the metric of all relevant\nmethods, and we identify benefits/limitations of each class of metrics to capture application’s per-\nformance. We also provide recommended metrics when necessary.\n3. A survey and taxonomy of datasets for LLMs’ applications on tabular data. For",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "application, we categorize and discuss a wide range of metrics that can be used to evaluate the\nperformance of that application. For each application, we documented the metric of all relevant\nmethods, and we identify benefits/limitations of each class of metrics to capture application’s per-\nformance. We also provide recommended metrics when necessary.\n3. A survey and taxonomy of datasets for LLMs’ applications on tabular data. For"
        }
    },
    {
        "id": "4f76a40226364b2a710d54eed787492130937d39",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nMikel Hernandez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, and Debbie Rankin. Synthetic data generation\nfor tabular health records: A systematic review. Neurocomputing, 493:28–45, 2022.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. Tapas: Weakly supervised table parsing via pre-training. In Dan Jurafsky, Joyce Chai, Natalie",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nMikel Hernandez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, and Debbie Rankin. Synthetic data generation\nfor tabular health records: A systematic review. Neurocomputing, 493:28–45, 2022.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas Müller, Francesco Piccinno, and Julian Martin Eisen-\nschlos. Tapas: Weakly supervised table parsing via pre-training. In Dan Jurafsky, Joyce Chai, Natalie"
        }
    },
    {
        "id": "5482915abe4605d37c7d44bfd03764e9e5deb3e0",
        "content": "convolutional neural networks and applications: A survey, 2019.\nKezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos,\nHuzefa Rangwala, and George Karypis. Opentab: Advancing large language models as open-domain table\nreasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=Qa0ULgosc9.\nJannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, and Yarin Gal. Self-attention",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "convolutional neural networks and applications: A survey, 2019.\nKezhi Kong, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Chuan Lei, Christos Faloutsos,\nHuzefa Rangwala, and George Karypis. Opentab: Advancing large language models as open-domain table\nreasoners. In The Twelfth International Conference on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=Qa0ULgosc9.\nJannik Kossen, Neil Band, Clare Lyle, Aidan N. Gomez, Tom Rainforth, and Yarin Gal. Self-attention"
        }
    },
    {
        "id": "5b22c310b06c7ff1711404b8fac01d22724299db",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nprobability score (CRPS), noting LLMs’ preference for simple rule-based completions and their tendency\ntowards repetition and capturing trends. The study reports that LLMs are able to capture time series data\ndistributions and to handle missing data without special treatment. However, this approach is constrained\nby the size of the window and the tokenization method of numerical feature, preventing it from further\nimprovement.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nprobability score (CRPS), noting LLMs’ preference for simple rule-based completions and their tendency\ntowards repetition and capturing trends. The study reports that LLMs are able to capture time series data\ndistributions and to handle missing data without special treatment. However, this approach is constrained\nby the size of the window and the tokenization method of numerical feature, preventing it from further\nimprovement."
        }
    },
    {
        "id": "6d16d80c7df59d430cb7bf2d5c7b39d9ee18119d",
        "content": "v1/N18-1202. URL https://aclanthology.org/N18-1202.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations, 2018b.\nSergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning\non tabular data, 2019.\nMohammadreza Pourreza and Davood Rafiei. DIN-SQL: decomposed in-context learning of text-to-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "v1/N18-1202. URL https://aclanthology.org/N18-1202.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations, 2018b.\nSergei Popov, Stanislav Morozov, and Artem Babenko. Neural oblivious decision ensembles for deep learning\non tabular data, 2019.\nMohammadreza Pourreza and Davood Rafiei. DIN-SQL: decomposed in-context learning of text-to-"
        }
    },
    {
        "id": "7eff4d0f434370cea4d48bde684989b581d4ec54",
        "content": "tables, 2023g.\nWeixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan,\nRaymond Chi-Wing Wong, and Haiqin Yang. Natural language interfaces for tabular data querying\nand visualization: A survey. CoRR, abs/2310.17894, 2023h. doi: 10.48550/ARXIV.2310.17894. URL\nhttps://doi.org/10.48550/arXiv.2310.17894.\nBowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, and Xiaobo Zhang.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tables, 2023g.\nWeixu Zhang, Yifei Wang, Yuanfeng Song, Victor Junqiu Wei, Yuxing Tian, Yiyan Qi, Jonathan H. Chan,\nRaymond Chi-Wing Wong, and Haiqin Yang. Natural language interfaces for tabular data querying\nand visualization: A survey. CoRR, abs/2310.17894, 2023h. doi: 10.48550/ARXIV.2310.17894. URL\nhttps://doi.org/10.48550/arXiv.2310.17894.\nBowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, and Xiaobo Zhang."
        }
    },
    {
        "id": "9f9538c1b0a7fc70d2b58be7c68f45bcdb0a709c",
        "content": "For deep learning methods in tabular data prediction, the methodologies can be broadly grouped into the\nfollowing categories: 1. Data transformation. These models either strive to convert heterogenous tabular\ninput into homogenous data more suitable to neural networks, like an image, on which CNN-like mecha-\nnism can be applied (SuperTML (Sun et al., 2019), IGTD (Zhu et al., 2021b), 1D-CNN (Kiranyaz et al.,\n2019)), or methods focusing on combining feature transformation with deep neural networks (Wide&Deep",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "For deep learning methods in tabular data prediction, the methodologies can be broadly grouped into the\nfollowing categories: 1. Data transformation. These models either strive to convert heterogenous tabular\ninput into homogenous data more suitable to neural networks, like an image, on which CNN-like mecha-\nnism can be applied (SuperTML (Sun et al., 2019), IGTD (Zhu et al., 2021b), 1D-CNN (Kiranyaz et al.,\n2019)), or methods focusing on combining feature transformation with deep neural networks (Wide&Deep"
        }
    },
    {
        "id": "badf3edee22a90124e91e580e780563f5aca73de",
        "content": "attributes are given.\n5 LLMs for table understanding\nIn this section, we cover datasets, trends and methods explored by researchers for question answering (QA),\nfact verification (FV) and table reasoning tasks. There are many papers working on database manipulation,\nmanagement and integration (Lobo et al., 2023; Fernandez et al., 2023; Narayan et al., 2022; Zhang et al.,\n2023b), which also include instructions and tabular inputs to LLMs. However, they are not typically referred",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "attributes are given.\n5 LLMs for table understanding\nIn this section, we cover datasets, trends and methods explored by researchers for question answering (QA),\nfact verification (FV) and table reasoning tasks. There are many papers working on database manipulation,\nmanagement and integration (Lobo et al., 2023; Fernandez et al., 2023; Narayan et al., 2022; Zhang et al.,\n2023b), which also include instructions and tabular inputs to LLMs. However, they are not typically referred"
        }
    },
    {
        "id": "cf4436d480df1ff8beed184550e56f6273d58873",
        "content": "into the second prompt to request the final answer for a downstream task. Ye et al. (2023b) also guided\nthe LLM to decompose a huge table into a small table, and to convert a complex question into simpler sub-\nquestions for text reasoning. Their strategy achieved significantly better results than competitive baselines\nfor table-based reasoning, outperforms human performance for the first time on the TabFact dataset. For",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "into the second prompt to request the final answer for a downstream task. Ye et al. (2023b) also guided\nthe LLM to decompose a huge table into a small table, and to convert a complex question into simpler sub-\nquestions for text reasoning. Their strategy achieved significantly better results than competitive baselines\nfor table-based reasoning, outperforms human performance for the first time on the TabFact dataset. For"
        }
    },
    {
        "id": "d2c00e287ed17ea881e3bd9556e7ac63512c760c",
        "content": "pre-trained models and for our survey, refers to models that have at least 1 billion parameters.\nSeveral key emergent abilities of LLMs are critical for data understanding and modeling including in-context\nlearning, instruction following, and multi-step reasoning. In-context learning refers to designing\nlarge auto-regressive language models that generate responses on unseen task without gradient update,\nonly learning through a natural language task description and a few in-context examples provided in the",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "pre-trained models and for our survey, refers to models that have at least 1 billion parameters.\nSeveral key emergent abilities of LLMs are critical for data understanding and modeling including in-context\nlearning, instruction following, and multi-step reasoning. In-context learning refers to designing\nlarge auto-regressive language models that generate responses on unseen task without gradient update,\nonly learning through a natural language task description and a few in-context examples provided in the"
        }
    },
    {
        "id": "dac0e922bd14862e8108a2068b34d16fdcb0eeaa",
        "content": "methods into two typical classes, Causal Language Modeling (CLM)-powered methods and Masked Language\nModeling (MLM)-powered methods. CLM, as an autoregressive method used in GPT-based models, predicts\nthe next token based on previous ones, focusing solely on past context. To model tabular data with unordered\ncolumns, permutation-invariant techniques are typically employed in CLM-powered methods. MLM involves",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "methods into two typical classes, Causal Language Modeling (CLM)-powered methods and Masked Language\nModeling (MLM)-powered methods. CLM, as an autoregressive method used in GPT-based models, predicts\nthe next token based on previous ones, focusing solely on past context. To model tabular data with unordered\ncolumns, permutation-invariant techniques are typically employed in CLM-powered methods. MLM involves"
        }
    },
    {
        "id": "f880316bb8ba2ddcade7285d82520bbb4ba75f63",
        "content": "Pfister. Sql-palm: Improved large language model adaptation for text-to-sql. CoRR, abs/2306.00739,\n2023b. doi: 10.48550/ARXIV.2306.00739. URL https://doi.org/10.48550/arXiv.2306.00739.\nAnirudh S. Sundar and Larry Heck. cTBLS: Augmenting large language models with conversational tables. In\nYun-Nung Chen and Abhinav Rastogi (eds.), Proceedings of the 5th Workshop on NLP for Conversational\nAI (NLP4ConvAI 2023), pp. 59–70, Toronto, Canada, July 2023. Association for Computational Linguis-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Pfister. Sql-palm: Improved large language model adaptation for text-to-sql. CoRR, abs/2306.00739,\n2023b. doi: 10.48550/ARXIV.2306.00739. URL https://doi.org/10.48550/arXiv.2306.00739.\nAnirudh S. Sundar and Larry Heck. cTBLS: Augmenting large language models with conversational tables. In\nYun-Nung Chen and Abhinav Rastogi (eds.), Proceedings of the 5th Workshop on NLP for Conversational\nAI (NLP4ConvAI 2023), pp. 59–70, Toronto, Canada, July 2023. Association for Computational Linguis-"
        }
    },
    {
        "id": "fee8070813747f15e60484efb195643ba28d247a",
        "content": "on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial\nIntelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI\n2023, Washington, DC, USA, February 7-14, 2023, pp. 13076–13084. AAAI Press, 2023b. doi: 10.1609/\nAAAI.V37I11.26536. URL https://doi.org/10.1609/aaai.v37i11.26536.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial\nIntelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI\n2023, Washington, DC, USA, February 7-14, 2023, pp. 13076–13084. AAAI Press, 2023b. doi: 10.1609/\nAAAI.V37I11.26536. URL https://doi.org/10.1609/aaai.v37i11.26536.\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,"
        }
    },
    {
        "id": "0d5304ad733d4ce9ef7978c95b851722b1a6c2e9",
        "content": "GPT-3 (Brown et al., 2020), etc. Intriguingly, some advanced abilities emerge as a result. These large-sized\nPLMs (i.e. LLMs) show unprecedentedly powerful capabilities (also called emergent abilities) that go beyond\ntraditional language modeling and start to gain capability to solve more general and complex tasks which\nwas not seen in PLM. Formally, we define a LLM as follows:\nDefinition 1 (Large Language Model). A large language model (LLM) M , parameterized by θ, is a",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "GPT-3 (Brown et al., 2020), etc. Intriguingly, some advanced abilities emerge as a result. These large-sized\nPLMs (i.e. LLMs) show unprecedentedly powerful capabilities (also called emergent abilities) that go beyond\ntraditional language modeling and start to gain capability to solve more general and complex tasks which\nwas not seen in PLM. Formally, we define a LLM as follows:\nDefinition 1 (Large Language Model). A large language model (LLM) M , parameterized by θ, is a"
        }
    },
    {
        "id": "1d9bec42930a0a1e2281405a0fc118a3ca1aff96",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nWenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S. Yu, Shafiq Joty, Yingbo Zhou, and Semih Yavuz.\nDivknowqa: Assessing the reasoning ability of llms via open-domain question answering over knowledge\nbase and text, 2023c.\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xian-\ngru Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating numerical reasoning capabili-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nWenting Zhao, Ye Liu, Tong Niu, Yao Wan, Philip S. Yu, Shafiq Joty, Yingbo Zhou, and Semih Yavuz.\nDivknowqa: Assessing the reasoning ability of llms via open-domain question answering over knowledge\nbase and text, 2023c.\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xian-\ngru Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating numerical reasoning capabili-"
        }
    },
    {
        "id": "1df92a13f20f8f2ce6fca427d8a263fd9ef202e0",
        "content": "has been constrained by differences in the inherent data structure. Some research efforts have sought to\nutilize the generic semantic knowledge contained in PLM, predominantly BERT-based models, for modeling\ntabular data (Figure 3). This involves employing PLM to learn contextual representation with semantic\ninformation taking header information into account (Chen et al., 2020c). The typical approach includes",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "has been constrained by differences in the inherent data structure. Some research efforts have sought to\nutilize the generic semantic knowledge contained in PLM, predominantly BERT-based models, for modeling\ntabular data (Figure 3). This involves employing PLM to learn contextual representation with semantic\ninformation taking header information into account (Chen et al., 2020c). The typical approach includes"
        }
    },
    {
        "id": "3f0a63a31b9f6e86c536b2d2369d8eb232d393ef",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nexample, tabular data prediction can be breakdown by pre-processing (modifying model inputs),\ntarget augmentation (modifying the outputs), fine-tuning (fine-tuning the model). We construct\ngranular subcategories at each stage to draw similarities and trends between classes of methods, and\nwe provide examples of main techniques. Practitioners and researchers can look at the section and",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nexample, tabular data prediction can be breakdown by pre-processing (modifying model inputs),\ntarget augmentation (modifying the outputs), fine-tuning (fine-tuning the model). We construct\ngranular subcategories at each stage to draw similarities and trends between classes of methods, and\nwe provide examples of main techniques. Practitioners and researchers can look at the section and"
        }
    },
    {
        "id": "5b98e64d8437f443563e4afae05329fe220ea97a",
        "content": "Tianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, and Qian Liu. Generative table pre-training\nempowers models for tabular prediction. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 14836–14854. Association for Computational Linguistics, December\n2023e. doi: 10.18653/v1/2023.emnlp-main.917. URL https://aclanthology.org/2023.emnlp-main.917.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tianping Zhang, Shaowen Wang, Shuicheng Yan, Li Jian, and Qian Liu. Generative table pre-training\nempowers models for tabular prediction. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 14836–14854. Association for Computational Linguistics, December\n2023e. doi: 10.18653/v1/2023.emnlp-main.917. URL https://aclanthology.org/2023.emnlp-main.917.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun. Tablellama: Towards open large generalist models"
        }
    },
    {
        "id": "6add3c3c80bcd229d06d53e6a502aefd51471b7d",
        "content": "et al., 2020b) and TAPAS (Herzig et al., 2020). Dataset in Tapas has 6.2 million tables and is useful for\nsemantic parsing. TAPAS has 26 million tables and their associated english contexts. It can help model gain\nbetter understanding in both textual and table. The dataset is in footnote. 21.\n23",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2020b) and TAPAS (Herzig et al., 2020). Dataset in Tapas has 6.2 million tables and is useful for\nsemantic parsing. TAPAS has 26 million tables and their associated english contexts. It can help model gain\nbetter understanding in both textual and table. The dataset is in footnote. 21.\n23"
        }
    },
    {
        "id": "6effccb85b685a09206ddc9b0118761d5411d2f4",
        "content": "Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nSoumajyoti Sarkar and Leonard Lausen. Testing the limits of unified sequence to sequence LLM pretraining\non diverse table data tasks. CoRR, abs/2310.00789, 2023. doi: 10.48550/ARXIV.2310.00789. URL\nhttps://doi.org/10.48550/arXiv.2310.00789.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nSoumajyoti Sarkar and Leonard Lausen. Testing the limits of unified sequence to sequence LLM pretraining\non diverse table data tasks. CoRR, abs/2310.00789, 2023. doi: 10.48550/ARXIV.2310.00789. URL\nhttps://doi.org/10.48550/arXiv.2310.00789."
        }
    },
    {
        "id": "8fb5b68df9d8830c9bf22a96c80d5960d46b822a",
        "content": "j.metrad.2023.100017. URL http://dx.doi.org/10.1016/j.metrad.2023.100017.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\nZhaocheng Liu, Qiang Liu, Haoli Zhang, and Yuntian Chen. Dnn2lr: Interpretation-inspired feature crossing\nfor real-world tabular data, 2021.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "j.metrad.2023.100017. URL http://dx.doi.org/10.1016/j.metrad.2023.100017.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692, 2019. URL http://arxiv.org/abs/1907.11692.\nZhaocheng Liu, Qiang Liu, Haoli Zhang, and Yuntian Chen. Dnn2lr: Interpretation-inspired feature crossing\nfor real-world tabular data, 2021."
        }
    },
    {
        "id": "9c0e12ddebf2e1cfb35922cf03a7aeae64cb1fd8",
        "content": "multi-turn dialogue strategies, where they decompose the original question into sub-tasks or sub-questions\nto guide the LLM’s reasoning. Sui et al. (2023c) instructed the LLM to “identify critical values and ranges\nof the last table related to the statement” to obtain additional information that were fed to the final LLM,\nobtaining increased scores for five datasets. Liu et al. (2023e) also investigated strategies around SC, along",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "multi-turn dialogue strategies, where they decompose the original question into sub-tasks or sub-questions\nto guide the LLM’s reasoning. Sui et al. (2023c) instructed the LLM to “identify critical values and ranges\nof the last table related to the statement” to obtain additional information that were fed to the final LLM,\nobtaining increased scores for five datasets. Liu et al. (2023e) also investigated strategies around SC, along"
        }
    },
    {
        "id": "9cf5d973be810279ffafe300f812320d58288797",
        "content": "research further reveals that flipping the labels of the in-context examples significantly narrows the gap in\nfairness metrics across different subgroups, but comes at the expected cost of a reduction in predictive perfor-\nmance. Other research shows that the inherent bias of LLM is hard to mitigate through prompt (Hegselmann\net al., 2023). Thus, it is worth to explore through other bias mitigation methods such as pre-processing (Shah\net al., 2020) or optimization (Bassi et al., 2024).",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "research further reveals that flipping the labels of the in-context examples significantly narrows the gap in\nfairness metrics across different subgroups, but comes at the expected cost of a reduction in predictive perfor-\nmance. Other research shows that the inherent bias of LLM is hard to mitigate through prompt (Hegselmann\net al., 2023). Thus, it is worth to explore through other bias mitigation methods such as pre-processing (Shah\net al., 2020) or optimization (Bassi et al., 2024)."
        }
    },
    {
        "id": "a07644f3ac44a324b31eff45ed4425d52e835991",
        "content": "LLMs in prediction tasks (Section 3), data augmentation and enrichment tasks (Section 4), and question an-\nswering/table understanding tasks (Section 5). Finally, Section 6 discusses limitations and future directions,\nwhile Section 7 concludes. The overview of this paper is shown in Figure 1.\n1.1 Characteristics of tabular data\nTabular data, commonly known as structured data, refers to data organized into rows and columns, where",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "LLMs in prediction tasks (Section 3), data augmentation and enrichment tasks (Section 4), and question an-\nswering/table understanding tasks (Section 5). Finally, Section 6 discusses limitations and future directions,\nwhile Section 7 concludes. The overview of this paper is shown in Figure 1.\n1.1 Characteristics of tabular data\nTabular data, commonly known as structured data, refers to data organized into rows and columns, where"
        }
    },
    {
        "id": "a419d84f2a7001eecbd271a71a1733c2bd9c3a39",
        "content": "(Zha et al., 2023)\nTAP4LLM QA Instruct GPT3.5, GPT4\n(Sui et al., 2023c)\nUniTabPT QA Custom: T5, Flan-T5\n(Sarkar & Lausen, 2023)\nYu et al. (2023) Multi-modal QA Custom: Retrieval trained on contrastive loss, Rank\nby softmax, Generation built on T5\nTableLlama QA Custom: TableLlama\n(Zhang et al., 2023g)\nDIVKNOWQA QA GPT3.5, DSP, ReAct\nZhao et al. (2023c)\nJiang et al. (2023) QA GPT3.5, ChatGPT3.5\nLiu et al. (2023c) QA & Text2SQL Vicuna, GPT4\nGao et al. (2024) Text2SQL GPT4",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(Zha et al., 2023)\nTAP4LLM QA Instruct GPT3.5, GPT4\n(Sui et al., 2023c)\nUniTabPT QA Custom: T5, Flan-T5\n(Sarkar & Lausen, 2023)\nYu et al. (2023) Multi-modal QA Custom: Retrieval trained on contrastive loss, Rank\nby softmax, Generation built on T5\nTableLlama QA Custom: TableLlama\n(Zhang et al., 2023g)\nDIVKNOWQA QA GPT3.5, DSP, ReAct\nZhao et al. (2023c)\nJiang et al. (2023) QA GPT3.5, ChatGPT3.5\nLiu et al. (2023c) QA & Text2SQL Vicuna, GPT4\nGao et al. (2024) Text2SQL GPT4"
        }
    },
    {
        "id": "b6c4cec7cd69bb90df4fb25ce5451173e8767768",
        "content": "feeding them into a collaborative CTR model and a pre-trained language model such as ChatGLM (Zeng\net al., 2023), respectively. CTRL employs a two-stage training process: the first stage involves cross-modal\ncontrastive learning for fine-grained knowledge alignment, while the second stage focuses on fine-tuning a\nlightweight collaborative model for downstream tasks. The approach outperforms all the SOTA baselines",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "feeding them into a collaborative CTR model and a pre-trained language model such as ChatGLM (Zeng\net al., 2023), respectively. CTRL employs a two-stage training process: the first stage involves cross-modal\ncontrastive learning for fine-grained knowledge alignment, while the second stage focuses on fine-tuning a\nlightweight collaborative model for downstream tasks. The approach outperforms all the SOTA baselines"
        }
    },
    {
        "id": "b77f0be4ffdcde7e9345c5d6f67e2fb057c399eb",
        "content": "PRAUC is useful in imbalanced datasets, which is always the case for medical data.\nWithout any pretraining, LLM has also demonstrated superior performance. CPLLM (Shoham & Rappoport,\n2023) leverages LLMs (Llama2 and BioMedLM) and does fine-tuning with QLora to predict diseases using\nstructured EHR data. CPLLM demonstrated significant improvements over the state-of-the-art in all tested\ndisease prediction tasks. Additionally, this approach, with an extended sequence length, is also suitable",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "PRAUC is useful in imbalanced datasets, which is always the case for medical data.\nWithout any pretraining, LLM has also demonstrated superior performance. CPLLM (Shoham & Rappoport,\n2023) leverages LLMs (Llama2 and BioMedLM) and does fine-tuning with QLora to predict diseases using\nstructured EHR data. CPLLM demonstrated significant improvements over the state-of-the-art in all tested\ndisease prediction tasks. Additionally, this approach, with an extended sequence length, is also suitable"
        }
    },
    {
        "id": "d7aaa7fa3a6deeeb68825dde9a7e94c96e3c595e",
        "content": "and step-wise reasoning. The opportunities for applying LLMs to tabular data modeling are as follows:\n1. Deep learning methods often exhibit suboptimal performance on datasets they were not initially\ntrained on, making transfer learning using the pre-training and fine-tuning paradigm highly promis-\ning (Shwartz-Ziv & Armon, 2022).\n2. The transformation of tabular data into LLM-readable natural language addresses the curse of\ndimensionality (created by the one-hot encoding of categorical data).",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and step-wise reasoning. The opportunities for applying LLMs to tabular data modeling are as follows:\n1. Deep learning methods often exhibit suboptimal performance on datasets they were not initially\ntrained on, making transfer learning using the pre-training and fine-tuning paradigm highly promis-\ning (Shwartz-Ziv & Armon, 2022).\n2. The transformation of tabular data into LLM-readable natural language addresses the curse of\ndimensionality (created by the one-hot encoding of categorical data)."
        }
    },
    {
        "id": "da151c879528938abd228e0d2417b863a0aa5a84",
        "content": "includes two parts: 1) the first part specifies the task background and description with optionally some\nexamples as in-context examples (Prompt Engineering); 2) the second part describes feature meanings and\nvalues of the current instance to be inferred (Serialization); LIFT and TabLLM have been benchmarked by\nat least 3 other papers. The code for both methods is available. 7\nSome other methods leverage an LLM to rewrite the serialization or perform prompt engineering.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "includes two parts: 1) the first part specifies the task background and description with optionally some\nexamples as in-context examples (Prompt Engineering); 2) the second part describes feature meanings and\nvalues of the current instance to be inferred (Serialization); LIFT and TabLLM have been benchmarked by\nat least 3 other papers. The code for both methods is available. 7\nSome other methods leverage an LLM to rewrite the serialization or perform prompt engineering."
        }
    },
    {
        "id": "f2dc9ed0e94a6a237dc03b6a0108f1204bdbf287",
        "content": "or fewer representative features. GTL (Zhang et al., 2023a) fine-tunes LLaMA to predict the next token\nusing 115 tabular datasets. To balance the number of instances across different datasets, they randomly\nsample up to 2,048 instances from each tabular dataset for GTL. They employed GTL which significantly\nimproves LLaMA in most zero-shot scenarios. Based on the current evidence, we believe that fine-tuning on",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "or fewer representative features. GTL (Zhang et al., 2023a) fine-tunes LLaMA to predict the next token\nusing 115 tabular datasets. To balance the number of instances across different datasets, they randomly\nsample up to 2,048 instances from each tabular dataset for GTL. They employed GTL which significantly\nimproves LLaMA in most zero-shot scenarios. Based on the current evidence, we believe that fine-tuning on"
        }
    },
    {
        "id": "f42ca55ff823e7ddf00aa7c4056cb8eef19aeae2",
        "content": "for table-based reasoning, outperforms human performance for the first time on the TabFact dataset. For\nLiu et al. (2023e), in encouraging symbolic CoT reasoning pathways, they allowed the model to interact\nwith a Python shell that could execute commands, process data, and scrutinize results, particularly within\na pandas dataframe, limited to a maximum of five iterative steps.\nDialogue-based applications In various applications where the users are interacting with the LLMs,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "for table-based reasoning, outperforms human performance for the first time on the TabFact dataset. For\nLiu et al. (2023e), in encouraging symbolic CoT reasoning pathways, they allowed the model to interact\nwith a Python shell that could execute commands, process data, and scrutinize results, particularly within\na pandas dataframe, limited to a maximum of five iterative steps.\nDialogue-based applications In various applications where the users are interacting with the LLMs,"
        }
    },
    {
        "id": "ff95877809386fb497ff4d49932da31c557072d9",
        "content": "method.\n4 LLMs for tabular data generation\nTabular data synthesis serves numerous purposes across diverse domains, including augmenting training\ndatasets for machine learning models (Fonseca & Bacao, 2023) to improve models’ predictive accuracy and\ngeneralization capabilities. Moreover, it’s crucial for data privacy (Assefa et al., 2020), where it enables the\ncreation of synthetic replicas of sensitive data, protecting confidential information while still preserving the",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "method.\n4 LLMs for tabular data generation\nTabular data synthesis serves numerous purposes across diverse domains, including augmenting training\ndatasets for machine learning models (Fonseca & Bacao, 2023) to improve models’ predictive accuracy and\ngeneralization capabilities. Moreover, it’s crucial for data privacy (Assefa et al., 2020), where it enables the\ncreation of synthetic replicas of sensitive data, protecting confidential information while still preserving the"
        }
    },
    {
        "id": "027d87a2b2c7d983946dc3d91a4f2d41e2371472",
        "content": "advancements in understanding, inferring, and generating tabular data. We provide recommendations for\ndataset and model selection tailored to specific tasks, aimed at aiding both ML researchers and practitioners\n29",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "advancements in understanding, inferring, and generating tabular data. We provide recommendations for\ndataset and model selection tailored to specific tasks, aimed at aiding both ML researchers and practitioners\n29"
        }
    },
    {
        "id": "04ebd43e5a172b1fca0aaf864462ed4f5fb56b64",
        "content": "accurate recall results. Similarly, SC method is used. cTBLS Sundar & Heck (2023) designed a three-\nstep architecture to retrieve and generate dialogue responses grounded on retrieved tabular information.\nIn the first step, a dual-encoder-based Dense Table Retrieval (DTR) model, initialized from RoBERTa\nLiu et al. (2019), identifies the most relevant table for the query. In the second step, a Coarse System",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "accurate recall results. Similarly, SC method is used. cTBLS Sundar & Heck (2023) designed a three-\nstep architecture to retrieve and generate dialogue responses grounded on retrieved tabular information.\nIn the first step, a dual-encoder-based Dense Table Retrieval (DTR) model, initialized from RoBERTa\nLiu et al. (2019), identifies the most relevant table for the query. In the second step, a Coarse System"
        }
    },
    {
        "id": "1887e1393738abf9f5fcedb33ccc19b539d4656e",
        "content": "and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021,\nvirtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n68d30a9594728bc39aa24be94b319d21-Abstract-round1.html.\nSercan O. Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning, 2020.\nSamuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman, Prashant Reddy, and Manuela",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track\non Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021,\nvirtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\n68d30a9594728bc39aa24be94b319d21-Abstract-round1.html.\nSercan O. Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning, 2020.\nSamuel A Assefa, Danial Dervovic, Mahmoud Mahfouz, Robert E Tillman, Prashant Reddy, and Manuela"
        }
    },
    {
        "id": "3d12f65e5aa1f55594df265ef34c5f3a46f1713a",
        "content": "on tabular data, 2019.\nMohammadreza Pourreza and Davood Rafiei. DIN-SQL: decomposed in-context learning of text-to-\nsql with self-correction. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz\nHardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: An-\nnual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\nUSA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "on tabular data, 2019.\nMohammadreza Pourreza and Davood Rafiei. DIN-SQL: decomposed in-context learning of text-to-\nsql with self-correction. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz\nHardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: An-\nnual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,\nUSA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/"
        }
    },
    {
        "id": "3ee12d0fbed892c7ea073df49785ef7ddc6db092",
        "content": "2023c.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can large language\nmodels understand structured table data? a benchmark and empirical study. In Proceedings of the 17th\nACM International Conference on Web Search and Data Mining, pp. 645–654, 2024.\nBaohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, and Jason Dong. Su-\npertml: Two-dimensional word embedding for the precognition on structured tabular data, 2019.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2023c.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can large language\nmodels understand structured table data? a benchmark and empirical study. In Proceedings of the 17th\nACM International Conference on Web Search and Data Mining, pp. 645–654, 2024.\nBaohua Sun, Lin Yang, Wenhan Zhang, Michael Lin, Patrick Dong, Charles Young, and Jason Dong. Su-\npertml: Two-dimensional word embedding for the precognition on structured tabular data, 2019."
        }
    },
    {
        "id": "49700878f07340aa5402afe00c8c661db3c417c1",
        "content": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: unifying\nlanguage learning paradigms. In The Eleventh International Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/\npdf?id=6ruVLB727MC.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: unifying\nlanguage learning paradigms. In The Eleventh International Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/\npdf?id=6ruVLB727MC."
        }
    },
    {
        "id": "4a980cbdc036b068a85ce10c8f457824f26bbf71",
        "content": "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. Hybridqa:\nA dataset of multi-hop question answering over tabular and textual data. In Trevor Cohn, Yulan He,\nand Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 1026–1036. Association\nfor Computational Linguistics, 2020b. doi: 10.18653/V1/2020.FINDINGS-EMNLP.91. URL https:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. Hybridqa:\nA dataset of multi-hop question answering over tabular and textual data. In Trevor Cohn, Yulan He,\nand Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 1026–1036. Association\nfor Computational Linguistics, 2020b. doi: 10.18653/V1/2020.FINDINGS-EMNLP.91. URL https:"
        }
    },
    {
        "id": "6a45f7d7abd68a0341b703ca5685ba2627546b2e",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nGgaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. Prompt engineering\nin large language models. In International Conference on Data Intelligence and Cognitive Informatics,\npp. 387–402. Springer, 2023.\nChristopher McMaster, David FL Liew, and Douglas EV Pires. Adapting pretrained language models for\nsolving tabular prediction problems in the electronic health record, 2023.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nGgaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende. Prompt engineering\nin large language models. In International Conference on Data Intelligence and Cognitive Informatics,\npp. 387–402. Springer, 2023.\nChristopher McMaster, David FL Liew, and Douglas EV Pires. Adapting pretrained language models for\nsolving tabular prediction problems in the electronic health record, 2023."
        }
    },
    {
        "id": "6cfcd097170348855cc0e65f1f943371512efae8",
        "content": "distributions due to the “lost-in-the-middle” phenomenon.\nUsed LLM Fine-tuned or not Serialization Metric\nGReaT (Borisov et al., 2023b) GPT2/DistilGPT2 Fine-tuned Sentences DCR, MLE\nREaLTabFormer (Solatorio & Dupriez, 2023) GPT2 Fine-tuned Sentences DCR, MLE\nTAPTAP (Zhang et al., 2023e) GPT2/DistilGPT2 Fine-tuned Sentences DCR, MLE\nTabuLa (Zhao et al., 2023f) DistilGPT2 Fine-tuned X-Separated MLE\nCLLM (Seedat et al., 2023) GPT4 Non Fine-tuned X-Separated MLE\nMasked Transformers",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "distributions due to the “lost-in-the-middle” phenomenon.\nUsed LLM Fine-tuned or not Serialization Metric\nGReaT (Borisov et al., 2023b) GPT2/DistilGPT2 Fine-tuned Sentences DCR, MLE\nREaLTabFormer (Solatorio & Dupriez, 2023) GPT2 Fine-tuned Sentences DCR, MLE\nTAPTAP (Zhang et al., 2023e) GPT2/DistilGPT2 Fine-tuned Sentences DCR, MLE\nTabuLa (Zhao et al., 2023f) DistilGPT2 Fine-tuned X-Separated MLE\nCLLM (Seedat et al., 2023) GPT4 Non Fine-tuned X-Separated MLE\nMasked Transformers"
        }
    },
    {
        "id": "6eec4f8b3ee2da331fb095b56d9502f9aa466ac7",
        "content": "2023b), which also include instructions and tabular inputs to LLMs. However, they are not typically referred\nto as a QA task, and they will not be covered by this paper.\n21",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2023b), which also include instructions and tabular inputs to LLMs. However, they are not typically referred\nto as a QA task, and they will not be covered by this paper.\n21"
        }
    },
    {
        "id": "a7b2d7062d55dee4c4416345fcc1dd95b918326f",
        "content": "may lead to information loss, a sparse matrix, and it may introduce multi-collinearity (e.g. with\n1We would like to thank Fanyou for his valuable contributions in discussing the project and idetifying relevant methoods.\n2",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "may lead to information loss, a sparse matrix, and it may introduce multi-collinearity (e.g. with\n1We would like to thank Fanyou for his valuable contributions in discussing the project and idetifying relevant methoods.\n2"
        }
    },
    {
        "id": "e545320ede31df97e1ad068c88fa50fa320d90e5",
        "content": "and symbolic reasoning to get right answer. The dataset is in footnote. 19.\nDomain-Specific Some datasets and task focus on domain-specific applications. AIT-QA (Katsis et al.,\n2022) worked on airline industry specific table question answer. It highlights the unique challenges posed\nby domain-specific tables, such as complex layouts, hierarchical headers, and specialized terminology. For\nfinance related table question answer, TAT-QA Zhu et al. (2021a) assesses numerical reasoning, involving",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and symbolic reasoning to get right answer. The dataset is in footnote. 19.\nDomain-Specific Some datasets and task focus on domain-specific applications. AIT-QA (Katsis et al.,\n2022) worked on airline industry specific table question answer. It highlights the unique challenges posed\nby domain-specific tables, such as complex layouts, hierarchical headers, and specialized terminology. For\nfinance related table question answer, TAT-QA Zhu et al. (2021a) assesses numerical reasoning, involving"
        }
    },
    {
        "id": "fbfd152533adb89445f8350c5b9b46c6800d4759",
        "content": "directions that warrant further exploration in future research endeavors.\nWith the rapid development of LLMs and their impressive emergent capabilities, there is a growing demand\nfor new ideas and research to explore their potential in modeling structured data for a variety of tasks.\nThrough this comprehensive review, we hope it can provide interested readers with pertinent references and\ninsightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "directions that warrant further exploration in future research endeavors.\nWith the rapid development of LLMs and their impressive emergent capabilities, there is a growing demand\nfor new ideas and research to explore their potential in modeling structured data for a variety of tasks.\nThrough this comprehensive review, we hope it can provide interested readers with pertinent references and\ninsightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and"
        }
    },
    {
        "id": "fea59618a9054b9810cee2e43fe6c0e6526ff1ee",
        "content": "PLM can be effectively adapted for diverse tabular prediction tasks, demonstrating their versatility across\nheterogeneous datasets (Yan et al., 2024b). UniPredict (Wang et al., 2023a) trains a single LLM (GPT2) on\nan aggregation of 169 tabular datasets with diverse targets and observes advantages over existing methods.\nThis model does not require fine-tuning LLM on specific datasets. Model accuracy and ranking are better",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "PLM can be effectively adapted for diverse tabular prediction tasks, demonstrating their versatility across\nheterogeneous datasets (Yan et al., 2024b). UniPredict (Wang et al., 2023a) trains a single LLM (GPT2) on\nan aggregation of 169 tabular datasets with diverse targets and observes advantages over existing methods.\nThis model does not require fine-tuning LLM on specific datasets. Model accuracy and ranking are better"
        }
    },
    {
        "id": "3485a9e5a00e2d2bad207ebbe6a526f17d0d1fb8",
        "content": "tasks (Wei et al., 2023). These new abilities of LLMs lay the groundwork for exploring their integration into\nintricate tasks extending beyond traditional NLP applications across diverse data types.\n1.3.1 Applications of LLMs in tabular data\nDespite the impressive capabilities of LM in addressing NLP tasks, its utilization for tabular data learning\nhas been constrained by differences in the inherent data structure. Some research efforts have sought to",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tasks (Wei et al., 2023). These new abilities of LLMs lay the groundwork for exploring their integration into\nintricate tasks extending beyond traditional NLP applications across diverse data types.\n1.3.1 Applications of LLMs in tabular data\nDespite the impressive capabilities of LM in addressing NLP tasks, its utilization for tabular data learning\nhas been constrained by differences in the inherent data structure. Some research efforts have sought to"
        }
    },
    {
        "id": "393476afcd6c77a7c8967dd8a4943ca4ea7f0ad2",
        "content": "Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,\n81:84–90, 2022.\nAnanya Singha, José Cambronero, Sumit Gulwani, Vu Le, and Chris Parnin. Tabular representation, noisy\noperators, and impacts on table structure understanding tasks in llms. CoRR, abs/2310.10358, 2023. doi:\n10.48550/ARXIV.2310.10358. URL https://doi.org/10.48550/arXiv.2310.10358.\nDylan Slack and Sameer Singh. Tablet: Learning from instructions for tabular data, 2023.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,\n81:84–90, 2022.\nAnanya Singha, José Cambronero, Sumit Gulwani, Vu Le, and Chris Parnin. Tabular representation, noisy\noperators, and impacts on table structure understanding tasks in llms. CoRR, abs/2310.10358, 2023. doi:\n10.48550/ARXIV.2310.10358. URL https://doi.org/10.48550/arXiv.2310.10358.\nDylan Slack and Sameer Singh. Tablet: Learning from instructions for tabular data, 2023."
        }
    },
    {
        "id": "4300015cff06747eb75cbefa25c1d1705f0a197e",
        "content": "200K tokens).\nSecondly, even if the table could fit the context length, most LLMs are slow to process long sentences due to\nthe quadratic complexity of self-attention (Sui et al., 2023b; Tay et al., 2023a; Vaswani et al., 2017). When\ndealing with long contexts, performance of LLMs significantly degrades when models must access relevant\ninformation in the middle of long contexts, even for explicitly long-context models (Liu et al., 2023b). For",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "200K tokens).\nSecondly, even if the table could fit the context length, most LLMs are slow to process long sentences due to\nthe quadratic complexity of self-attention (Sui et al., 2023b; Tay et al., 2023a; Vaswani et al., 2017). When\ndealing with long contexts, performance of LLMs significantly degrades when models must access relevant\ninformation in the middle of long contexts, even for explicitly long-context models (Liu et al., 2023b). For"
        }
    },
    {
        "id": "609f58f87677902e8936489338aa56432789e48d",
        "content": "and gradient-boosted trees. TabLLM’s efficacy is highlighted by its ability to leverage the extensive knowl-\nedge encoded in pre-trained LLMs from these models, requiring minimal labeled data. However, the sample\nefficiency of TabLLM is highly task-dependent. Other research also leverages T0 as the based model. Jaitly\net al. (2023) uses T0 (Sanh et al., 2021). Compared to TabLLM, it is trained using Intrinsic Attention-based",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and gradient-boosted trees. TabLLM’s efficacy is highlighted by its ability to leverage the extensive knowl-\nedge encoded in pre-trained LLMs from these models, requiring minimal labeled data. However, the sample\nefficiency of TabLLM is highly task-dependent. Other research also leverages T0 as the based model. Jaitly\net al. (2023) uses T0 (Sanh et al., 2021). Compared to TabLLM, it is trained using Intrinsic Attention-based"
        }
    },
    {
        "id": "679e735772b2323622c283e5ffaf8fec30ce7409",
        "content": "LLMs to generate these commands. Deng et al. (2022b) proposed the QA task be split into three subtasks:\nClarification Need Prediction (CNP) to determine whether to ask a question for clarifying the uncertainty;\nClarification Question Generation (CQG) to generate a clarification question as the response, if CNP detects\nthe need for clarification; and Conversational Question Answering (CQA) to directly produce the answer as",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "LLMs to generate these commands. Deng et al. (2022b) proposed the QA task be split into three subtasks:\nClarification Need Prediction (CNP) to determine whether to ask a question for clarifying the uncertainty;\nClarification Question Generation (CQG) to generate a clarification question as the response, if CNP detects\nthe need for clarification; and Conversational Question Answering (CQA) to directly produce the answer as"
        }
    },
    {
        "id": "7703e246482219ec44dbdd39ce49926714824ad8",
        "content": "et al., 2022) Question 35.12 (Sarkar & Lausen, 2023), 30.92\n(Ye et al., 2023b), 27.02 (Chen, 2023),\nWikiTableQuestion2108 QA Table Answer Wikipedia Execution Accuracy: 73.65 (Liu et al.,\n(Pasupat & Question 2023e), 67.31 (Wang et al., 2024), 65.90\nLiang, 2015a) (Ye et al., 2023b), 65.90 (Jiang et al.,\n2023), 62.45 (Sarkar & Lausen, 2023),\n48.80 (Chen, 2023), 35.01 (Zhang et al.,\n2023g)\nHybridQA 13000 QA Table Answer Wikipedia Exact Match Accuracy: 39.38 (Zhang",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2022) Question 35.12 (Sarkar & Lausen, 2023), 30.92\n(Ye et al., 2023b), 27.02 (Chen, 2023),\nWikiTableQuestion2108 QA Table Answer Wikipedia Execution Accuracy: 73.65 (Liu et al.,\n(Pasupat & Question 2023e), 67.31 (Wang et al., 2024), 65.90\nLiang, 2015a) (Ye et al., 2023b), 65.90 (Jiang et al.,\n2023), 62.45 (Sarkar & Lausen, 2023),\n48.80 (Chen, 2023), 35.01 (Zhang et al.,\n2023g)\nHybridQA 13000 QA Table Answer Wikipedia Exact Match Accuracy: 39.38 (Zhang"
        }
    },
    {
        "id": "84c39402febda1c1c2d2bec75729e3423f19639d",
        "content": "doi: 10.1609/AAAI.V37I11.26535. URL https://doi.org/10.1609/aaai.v37i11.26535.\nJinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo\nSi, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql\nparsing. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI Conference\non Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "doi: 10.1609/AAAI.V37I11.26535. URL https://doi.org/10.1609/aaai.v37i11.26535.\nJinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo\nSi, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sql\nparsing. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI Conference\non Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial"
        }
    },
    {
        "id": "8846350ee66bc76fc7b842a8406d6c73e7a4b8de",
        "content": "to complete new tasks by following only the instructions of the task descriptions (also known as zero-shot\nprompts). Some papers also fine-tuned LLMs on a variety of tasks presented as instructions (Thoppilan\net al., 2022). However, instruction-tuning is reported to work best only for larger-size models (Wei et al.,\n2022a; Chung et al., 2022). Solving complex tasks involving multiple steps have been challenging for LLMs.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to complete new tasks by following only the instructions of the task descriptions (also known as zero-shot\nprompts). Some papers also fine-tuned LLMs on a variety of tasks presented as instructions (Thoppilan\net al., 2022). However, instruction-tuning is reported to work best only for larger-size models (Wei et al.,\n2022a; Chung et al., 2022). Solving complex tasks involving multiple steps have been challenging for LLMs."
        }
    },
    {
        "id": "991b98deb263c99ea2dea740cd9ec8db2cdfa324",
        "content": "information taking header information into account (Chen et al., 2020c). The typical approach includes\ntransforming tabular data into text through serialization (detailed explanation in Section 2) and employing\na masked-language-modeling (MLM) approach for fine-tuning the PLM, similar to that in BERT (PTab,\nCT-BERT, TABERT (Liu et al., 2022a; Ye et al., 2023a; Yin et al., 2020a). In addition to being able\nto incorporate semantic knowledge from column names, converting heterogenous tabular data into textual",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "information taking header information into account (Chen et al., 2020c). The typical approach includes\ntransforming tabular data into text through serialization (detailed explanation in Section 2) and employing\na masked-language-modeling (MLM) approach for fine-tuning the PLM, similar to that in BERT (PTab,\nCT-BERT, TABERT (Liu et al., 2022a; Ye et al., 2023a; Yin et al., 2020a). In addition to being able\nto incorporate semantic knowledge from column names, converting heterogenous tabular data into textual"
        }
    },
    {
        "id": "9c29f9e9fb323ac82d8bff5e1560a44671d4c720",
        "content": "to these approaches as inference-only prediction. TABLET (Slack & Singh, 2023) utilizes models like Tk-\nInstruct (Wang et al., 2022b), Flan-T5 (Chung et al., 2022), GPT-J (Black et al., 2022), and ChatGPT\nto perform inference, but it reports that a KNN approach with feature weights from XGBoost surpasses\nFlan-T5 11b in performance using similar examples and instructions. Summary Boosting (Manikandan\net al., 2023) creates multiple inputs through the serialization step. The AdaBoost algorithm then creates an",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to these approaches as inference-only prediction. TABLET (Slack & Singh, 2023) utilizes models like Tk-\nInstruct (Wang et al., 2022b), Flan-T5 (Chung et al., 2022), GPT-J (Black et al., 2022), and ChatGPT\nto perform inference, but it reports that a KNN approach with feature weights from XGBoost surpasses\nFlan-T5 11b in performance using similar examples and instructions. Summary Boosting (Manikandan\net al., 2023) creates multiple inputs through the serialization step. The AdaBoost algorithm then creates an"
        }
    },
    {
        "id": "a7bba72864e97d6a627a659ff139e599a7759229",
        "content": "URL https://aclanthology.org/2020.acl-main.745.\nYuwei Yin, Yazheng Yang, Jian Yang, and Qi Liu. Finpt: Financial risk prediction with profile tuning on\npretrained foundation models, 2023.\nBowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, and Yongbin Li. Unified language representation for ques-\ntion answering over text, tables, and images. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki\nOkazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "URL https://aclanthology.org/2020.acl-main.745.\nYuwei Yin, Yazheng Yang, Jian Yang, and Qi Liu. Finpt: Financial risk prediction with profile tuning on\npretrained foundation models, 2023.\nBowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, and Yongbin Li. Unified language representation for ques-\ntion answering over text, tables, and images. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki\nOkazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,"
        }
    },
    {
        "id": "b17d6412846152a4884d7b238f1201ab66f25f52",
        "content": "Dongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table-tuned GPT for diverse table tasks.\nCoRR, abs/2310.09263, 2023d. doi: 10.48550/ARXIV.2310.09263. URL https://doi.org/10.48550/\narXiv.2310.09263.\nXiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. Ctrl: Connect collaborative and language model for\nctr prediction, 2023e.\nYikuan Li, Shishir Rao, José Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Dongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table-tuned GPT for diverse table tasks.\nCoRR, abs/2310.09263, 2023d. doi: 10.48550/ARXIV.2310.09263. URL https://doi.org/10.48550/\narXiv.2310.09263.\nXiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. Ctrl: Connect collaborative and language model for\nctr prediction, 2023e.\nYikuan Li, Shishir Rao, José Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy,"
        }
    },
    {
        "id": "c70e8c610bf5086a4041c6a3515c3d82a2473f79",
        "content": "data, and scrutinize results (within a Pandas DataFrame), iteratively over a maximum of five iterations.\nZhang et al. (2023d) demonstrated that we can obtain errors from the SQL tool to be fed back to the\nLLMs. By implementing this iterative process of calling LLMs, they improved the success rate of the SQL\nquery generation. Finally, Liu et al. (2023c) proposes a no-code data analytics platform that uses LLMs",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "data, and scrutinize results (within a Pandas DataFrame), iteratively over a maximum of five iterations.\nZhang et al. (2023d) demonstrated that we can obtain errors from the SQL tool to be fed back to the\nLLMs. By implementing this iterative process of calling LLMs, they improved the success rate of the SQL\nquery generation. Finally, Liu et al. (2023c) proposes a no-code data analytics platform that uses LLMs"
        }
    },
    {
        "id": "e6caf725946f33f8256a3e88b63ec6296d06eb9c",
        "content": "foundation models such as BERT (Devlin et al., 2019), employing the models’ official tokenizers. The process\nenhances the ability of these models to predict financial risks, with Flan-T5 emerging as the most effective\nbackbone model in this context, particularly across eight datasets. For financial data, FinBench contains 10\ndatasets with varied training set sizes (from 2k - 140k) and feature sizes (from 9 - 120) 10.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "foundation models such as BERT (Devlin et al., 2019), employing the models’ official tokenizers. The process\nenhances the ability of these models to predict financial risks, with Flan-T5 emerging as the most effective\nbackbone model in this context, particularly across eight datasets. For financial data, FinBench contains 10\ndatasets with varied training set sizes (from 2k - 140k) and feature sizes (from 9 - 120) 10."
        }
    },
    {
        "id": "f8f70b798e37c1c14a00529afb2a81ea6eb1fa08",
        "content": "Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,\nBo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent\nZhang, Caiming Xiong, Richard Socher, and Dragomir R. Radev. Sparc: Cross-domain semantic parsing\nin context. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,\nBo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Jonathan Kraft, Vincent\nZhang, Caiming Xiong, Richard Socher, and Dragomir R. Radev. Sparc: Cross-domain semantic parsing\nin context. In Anna Korhonen, David R. Traum, and Lluís Màrquez (eds.), Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August"
        }
    },
    {
        "id": "05e681221fab5ce94e3d5deb313924b5c75dcffd",
        "content": "Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.\nDeep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning\nSystems, 2022a.\nVadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models\nare realistic tabular data generators. arXiv preprint arXiv:2210.06280, 2022b.\nVadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Vadim Borisov, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.\nDeep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning\nSystems, 2022a.\nVadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models\nare realistic tabular data generators. arXiv preprint arXiv:2210.06280, 2022b.\nVadim Borisov, Kathrin Seßler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language"
        }
    },
    {
        "id": "097b6548a50c9970ae2e3d76ca30aad15300fc98",
        "content": "Jul-uX7EV_I.\nKai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen, and William Yang Wang. HybriDialogue: An\ninformation-seeking dialogue dataset grounded on tabular and textual data. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL\n2022, pp. 481–492, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.findings-acl.41. URL https://aclanthology.org/2022.findings-acl.41.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jul-uX7EV_I.\nKai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen, and William Yang Wang. HybriDialogue: An\ninformation-seeking dialogue dataset grounded on tabular and textual data. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL\n2022, pp. 481–492, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.findings-acl.41. URL https://aclanthology.org/2022.findings-acl.41."
        }
    },
    {
        "id": "164b9e5a46e986e42ed1f17a4b986d930665a05a",
        "content": "Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,\nNan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database\ngrounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2023c.\nPeng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman,\nDongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table-tuned GPT for diverse table tasks.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,\nNan Huo, et al. Can llm already serve as a database interface? a big bench for large-scale database\ngrounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2023c.\nPeng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman,\nDongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table-tuned GPT for diverse table tasks."
        }
    },
    {
        "id": "352d75aa1eb9cc668e1eb666fcb3e476b86f21b0",
        "content": "framework for privacy preserving training data release for machine learning, 2023.\nHariharan Manikandan, Yiding Jiang, and J Zico Kolter. Language models are weak learners, 2023.\n38",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "framework for privacy preserving training data release for machine learning, 2023.\nHariharan Manikandan, Yiding Jiang, and J Zico Kolter. Language models are weak learners, 2023.\n38"
        }
    },
    {
        "id": "4e467a4060906be32d217942f2b05c3357e1b8ba",
        "content": "et al., 2021; Rasmy et al., 2021; Li et al., 2020a) perform better than XGBoost. However, these models only\nfocused on predicting a small fraction of the International Statistical Classification of Diseases and Related\nHealth Problems (ICD) codes. Currently, Meditab (Wang et al., 2023c) aims to create a foundation model\nin the medical field. For preprocessing, Meditab utilizes GPT-3.5 (Brown et al., 2020) to convert tabular",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2021; Rasmy et al., 2021; Li et al., 2020a) perform better than XGBoost. However, these models only\nfocused on predicting a small fraction of the International Statistical Classification of Diseases and Related\nHealth Problems (ICD) codes. Currently, Meditab (Wang et al., 2023c) aims to create a foundation model\nin the medical field. For preprocessing, Meditab utilizes GPT-3.5 (Brown et al., 2020) to convert tabular"
        }
    },
    {
        "id": "4f0f84f22cc484ec5ff313fa28d90ba8765585a7",
        "content": "Abraham et al. (2022) Text2SQL Custom: Table Selector + Known & Unknown Fields\nExtractor + AggFn Classifier\nTable 8: Overview of Papers and Models for LLMs for tabular QA tasks. We only include papers that\nwork with models of >1B parameters. Models that are described as “Custom” indicates papers that fine-\ntuned specific portions of their pipeline for the task, whereas the other papers focus more on non-finetuning\nmethods like prompt engineering. NumQA: Numerical QA.\n5.2 General ability of LLMs in QA",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Abraham et al. (2022) Text2SQL Custom: Table Selector + Known & Unknown Fields\nExtractor + AggFn Classifier\nTable 8: Overview of Papers and Models for LLMs for tabular QA tasks. We only include papers that\nwork with models of >1B parameters. Models that are described as “Custom” indicates papers that fine-\ntuned specific portions of their pipeline for the task, whereas the other papers focus more on non-finetuning\nmethods like prompt engineering. NumQA: Numerical QA.\n5.2 General ability of LLMs in QA"
        }
    },
    {
        "id": "533cebe0c3a6f1b814c86b88313570be30932477",
        "content": "methods are illustrated in Table 4 and discussed in detail below.\nAs one of the earliest endeavors, LIFT (Dinh et al., 2022) tried a few different serialization methods, such\nas feature and value as a natural sentence such as \"The column name is Value\" or a set of equations, such\nas col = val , col = val , .... The former achieves higher prediction accuracy, especially in low-dimensional\n1 1 2 2\ntasks. The same conclusion was drawn in TabLLM (Hegselmann et al., 2023) where they evaluated 9",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "methods are illustrated in Table 4 and discussed in detail below.\nAs one of the earliest endeavors, LIFT (Dinh et al., 2022) tried a few different serialization methods, such\nas feature and value as a natural sentence such as \"The column name is Value\" or a set of equations, such\nas col = val , col = val , .... The former achieves higher prediction accuracy, especially in low-dimensional\n1 1 2 2\ntasks. The same conclusion was drawn in TabLLM (Hegselmann et al., 2023) where they evaluated 9"
        }
    },
    {
        "id": "55693f4408f87033dcdf67fae6993b59ac5d926c",
        "content": "order for LLMs to ingest tabular data efficiently, it is important to compress the tables to fit context lengths,\nfor better performance and reduced costs. Therefore, table manipulations are required in some scenarios, as\ndescribed below.\nCompacting tables to fit context lengths, for better performance and reduced costs For smaller\ntables, it might be possible to include the whole table within a prompt. However, for larger tables, there are\nthree challenges:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "order for LLMs to ingest tabular data efficiently, it is important to compress the tables to fit context lengths,\nfor better performance and reduced costs. Therefore, table manipulations are required in some scenarios, as\ndescribed below.\nCompacting tables to fit context lengths, for better performance and reduced costs For smaller\ntables, it might be possible to include the whole table within a prompt. However, for larger tables, there are\nthree challenges:"
        }
    },
    {
        "id": "7e1aaac4d3b153c754e76a3f462d4fd04e303bde",
        "content": "to improve its ability to capture a table’s data distribution becomes essential. This is because presenting\nthe entire training table (often comprising millions of rows) to LLMs for in-context learning poses several\nchallenges: 1) the low success ratio to extract the output cell values, where generated data samples may\ndiverge from intended model output formats; 2) LLMs, acting as ICL, struggle to capture column-wise tail\ndistributions due to the “lost-in-the-middle” phenomenon.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to improve its ability to capture a table’s data distribution becomes essential. This is because presenting\nthe entire training table (often comprising millions of rows) to LLMs for in-context learning poses several\nchallenges: 1) the low success ratio to extract the output cell values, where generated data samples may\ndiverge from intended model output formats; 2) LLMs, acting as ICL, struggle to capture column-wise tail\ndistributions due to the “lost-in-the-middle” phenomenon."
        }
    },
    {
        "id": "a275bfc420801885eb50205540f2b46e567ff03d",
        "content": "Haoran Luo, Fan Cheng, Heng Yu, and Yuqi Yi. Sdtr: Soft decision tree regressor for tabular data. IEEE\nAccess, 9:55999–56011, 2021.\nChao Ma, Sebastian Tschiatschek, José Miguel Hernández-Lobato, Richard Turner, and Cheng Zhang. Vaem:\na deep generative model for heterogeneous mixed type data, 2020.\nTamas Madl, Weijie Xu, Olivia Choudhury, and Matthew Howard. Approximate, adapt, anonymize (3a): a\nframework for privacy preserving training data release for machine learning, 2023.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Haoran Luo, Fan Cheng, Heng Yu, and Yuqi Yi. Sdtr: Soft decision tree regressor for tabular data. IEEE\nAccess, 9:55999–56011, 2021.\nChao Ma, Sebastian Tschiatschek, José Miguel Hernández-Lobato, Richard Turner, and Cheng Zhang. Vaem:\na deep generative model for heterogeneous mixed type data, 2020.\nTamas Madl, Weijie Xu, Olivia Choudhury, and Matthew Howard. Approximate, adapt, anonymize (3a): a\nframework for privacy preserving training data release for machine learning, 2023."
        }
    },
    {
        "id": "a50b75dcb56594620dfb0be1476a27d3f3543f7e",
        "content": "Shapley values have been used to evaluate the prompt for LLMs (Liu et al., 2023a). It could also be useful\nto understand how each feature influence the result. For instance, in prediction for diseases, providing\nexplanation is crucial. In this case, an explanation based on Shapley values would list the most important\nfeatures that led to the final decision. However, the performance of Shapley or other explanation methods",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Shapley values have been used to evaluate the prompt for LLMs (Liu et al., 2023a). It could also be useful\nto understand how each feature influence the result. For instance, in prediction for diseases, providing\nexplanation is crucial. In this case, an explanation based on Shapley values would list the most important\nfeatures that led to the final decision. However, the performance of Shapley or other explanation methods"
        }
    },
    {
        "id": "b9f2f2dae1c242c3ed713dd7822330cf0314f3e9",
        "content": "Liu et al. (2019), identifies the most relevant table for the query. In the second step, a Coarse System\nState Tracking system, trained using triplet loss, is used to rank cells. Finally, GPT-3.5 is prompted to\ngenerate a natural language response to a follow-up query conditioned on cells of the table ranked by their\nrelevance to the query as obtained from the coarse state tracker. The prompt includes the dialogue history,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Liu et al. (2019), identifies the most relevant table for the query. In the second step, a Coarse System\nState Tracking system, trained using triplet loss, is used to rank cells. Finally, GPT-3.5 is prompted to\ngenerate a natural language response to a follow-up query conditioned on cells of the table ranked by their\nrelevance to the query as obtained from the coarse state tracker. The prompt includes the dialogue history,"
        }
    },
    {
        "id": "bd33aaa3a7e93b06b3a33b22fdaa36dc255523d7",
        "content": "gru Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating numerical reasoning capabili-\nties of llms in understanding long documents with tabular data. CoRR, abs/2311.09805, 2023d. doi:\n10.48550/ARXIV.2311.09805. URL https://doi.org/10.48550/arXiv.2311.09805.\nYilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and\nDragomir Radev. RobuT: A systematic study of table QA robustness against human-annotated ad-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "gru Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating numerical reasoning capabili-\nties of llms in understanding long documents with tabular data. CoRR, abs/2311.09805, 2023d. doi:\n10.48550/ARXIV.2311.09805. URL https://doi.org/10.48550/arXiv.2311.09805.\nYilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, and\nDragomir Radev. RobuT: A systematic study of table QA robustness against human-annotated ad-"
        }
    },
    {
        "id": "d0776b953fb45d02b6f138b55a39266766b393ef",
        "content": "Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via\nbroader cross-format training. arXiv preprint arXiv:2202.12359, 2022.\nJayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. arXiv preprint\narXiv:2210.04018, 2022a.\nJayoung Kim, Chaejeong Lee, Yehjin Shin, Sewon Park, Minjung Kim, Noseong Park, and Jihoon Cho.\nSos: Score-based oversampling for tabular data. In Proceedings of the 28th ACM SIGKDD Conference on",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Daniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via\nbroader cross-format training. arXiv preprint arXiv:2202.12359, 2022.\nJayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. arXiv preprint\narXiv:2210.04018, 2022a.\nJayoung Kim, Chaejeong Lee, Yehjin Shin, Sewon Park, Minjung Kim, Noseong Park, and Jihoon Cho.\nSos: Score-based oversampling for tabular data. In Proceedings of the 28th ACM SIGKDD Conference on"
        }
    },
    {
        "id": "df33af9f0908d000ebaa2087511b8077a2898461",
        "content": "https://aclanthology.org/2023.emnlp-main.914.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of\nlarge language models, 2023b.\n46",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "https://aclanthology.org/2023.emnlp-main.914.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of\nlarge language models, 2023b.\n46"
        }
    },
    {
        "id": "e38286637eafc2f72723140333e64a05c4fa496c",
        "content": "Weijie Xu, Jinjin Zhao, Francis Iannacci, and Bo Wang. Ffpdg: Fast, fair and private data generation. arXiv\npreprint arXiv:2307.00161, 2023b.\nXiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language\nwithout reinforcement learning, 2017.\nHao Xue and Flora D Salim. Prompt-based time series forecasting: A new task and dataset. arXiv preprint\narXiv:2210.08964, 2022.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Weijie Xu, Jinjin Zhao, Francis Iannacci, and Bo Wang. Ffpdg: Fast, fair and private data generation. arXiv\npreprint arXiv:2307.00161, 2023b.\nXiaojun Xu, Chang Liu, and Dawn Song. Sqlnet: Generating structured queries from natural language\nwithout reinforcement learning, 2017.\nHao Xue and Flora D Salim. Prompt-based time series forecasting: A new task and dataset. arXiv preprint\narXiv:2210.08964, 2022.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural"
        }
    },
    {
        "id": "e4fb8d17f53068e2622600e5d160a403c141b5c5",
        "content": "CLLM (Seedat et al., 2023) GPT4 Non Fine-tuned X-Separated MLE\nMasked Transformers\nTabMT (Gulati & Roysdon, 2023) Fine-tuned \"[Value]\" MLE\n-24layer\nTable 6: LLM-powered data synthesis methods. “DCR” stands for Distance to the Closest Record and\n“MLE” stands for Machine Learning Efficiency.\nIn this section we survey methodologies that leverage LLMs for tabular data synthesis. We categorize the\nmethods into two typical classes, Causal Language Modeling (CLM)-powered methods and Masked Language",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "CLLM (Seedat et al., 2023) GPT4 Non Fine-tuned X-Separated MLE\nMasked Transformers\nTabMT (Gulati & Roysdon, 2023) Fine-tuned \"[Value]\" MLE\n-24layer\nTable 6: LLM-powered data synthesis methods. “DCR” stands for Distance to the Closest Record and\n“MLE” stands for Machine Learning Efficiency.\nIn this section we survey methodologies that leverage LLMs for tabular data synthesis. We categorize the\nmethods into two typical classes, Causal Language Modeling (CLM)-powered methods and Masked Language"
        }
    },
    {
        "id": "fafcce6bf6a126d7a16b87c83b23fa0b3f448214",
        "content": "admits multiple different ways of thinking leading to its unique correct answer. SC samples a diverse set\nof reasoning paths from an LLM, then selects the most consistent answer by marginalizing out the sampled\nreasoning paths. Inspired by these strategies, Zhao et al. (2023a); Ye et al. (2023b) experimented with\nmulti-turn dialogue strategies, where they decompose the original question into sub-tasks or sub-questions",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "admits multiple different ways of thinking leading to its unique correct answer. SC samples a diverse set\nof reasoning paths from an LLM, then selects the most consistent answer by marginalizing out the sampled\nreasoning paths. Inspired by these strategies, Zhao et al. (2023a); Ye et al. (2023b) experimented with\nmulti-turn dialogue strategies, where they decompose the original question into sub-tasks or sub-questions"
        }
    },
    {
        "id": "17e896affde0a7269132c3050040dd744d26339d",
        "content": "approach significantly aligns most generated answers with the intended labels. Additionally, to address\npotential inaccuracies in inference outputs, LIFT conducts five inference attempts, defaulting to the training\nset’s average value if all attempts fail. In streamlining the two-step approach, TabLLM (Hegselmann et al.,\n2023) incorporates the use of Verbalizer (Cui et al., 2022) to map the answer to a valid class. To calculate",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "approach significantly aligns most generated answers with the intended labels. Additionally, to address\npotential inaccuracies in inference outputs, LIFT conducts five inference attempts, defaulting to the training\nset’s average value if all attempts fail. In streamlining the two-step approach, TabLLM (Hegselmann et al.,\n2023) incorporates the use of Verbalizer (Cui et al., 2022) to map the answer to a valid class. To calculate"
        }
    },
    {
        "id": "1c9aeabfa131500ae5f9fcdaeb234c665eb7549f",
        "content": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\n2023. OpenReview.net, 2023b. URL https://openreview.net/pdf?id=1PL1NIMMrw.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunk-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The\nEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,\n2023. OpenReview.net, 2023b. URL https://openreview.net/pdf?id=1PL1NIMMrw.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunk-"
        }
    },
    {
        "id": "20df7fb6c1e78aca1aaa7347c2d36c5eef8a573b",
        "content": "UCI ML 20 Manikandan et al. (2023); Slack & Singh (2023)\nDDX 10 Slack & Singh (2023)\nTable 2: Combo is the combination of the following dataset in the form of dataset name (number of rows,\nnumber of features): Bank (45,211 rows, 16 feats), Blood (748, 4), California (20,640, 8), Car (1,728, 8),\nCreditg (1,000, 20), Income (48,842, 14), and Jungle (44,819, 6), Diabetes (768, 8) and Heart (918, 11).\nAlgorithm Type Method Resource Metric Used Model",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "UCI ML 20 Manikandan et al. (2023); Slack & Singh (2023)\nDDX 10 Slack & Singh (2023)\nTable 2: Combo is the combination of the following dataset in the form of dataset name (number of rows,\nnumber of features): Bank (45,211 rows, 16 feats), Blood (748, 4), California (20,640, 8), Car (1,728, 8),\nCreditg (1,000, 20), Income (48,842, 14), and Jungle (44,819, 6), Diabetes (768, 8) and Heart (918, 11).\nAlgorithm Type Method Resource Metric Used Model"
        }
    },
    {
        "id": "36813db90810b593df7216b09a8a57ff026cd5bc",
        "content": "distilled by gbdt for online prediction tasks. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pp. 384–394, 2019a.\nGuolin Ke, Jia Zhang, Zhenhui Xu, Jiang Bian, and Tie-Yan Liu. TabNN: A universal neural network\nsolution for tabular data, 2019b. URL https://openreview.net/forum?id=r1eJssCqY7.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "distilled by gbdt for online prediction tasks. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, pp. 384–394, 2019a.\nGuolin Ke, Jia Zhang, Zhenhui Xu, Jiang Bian, and Tie-Yan Liu. TabNN: A universal neural network\nsolution for tabular data, 2019b. URL https://openreview.net/forum?id=r1eJssCqY7.\nDaniel Khashabi, Yeganeh Kordi, and Hannaneh Hajishirzi. Unifiedqa-v2: Stronger generalization via"
        }
    },
    {
        "id": "3744d1cce4e672a7446b96a7ad4342cae7cff39b",
        "content": "Table Classification FEVEROUS (Aly et al., 2021) focuses on both unstructured text and structured\ntables for fact extraction and verification tasks. Dresden Web Tables (Eberius et al., 2015)is useful for\ntasks requiring the classification of web table layouts, particularly useful in data extraction and web content\nanalysis where table structures are crucial. The dataset is in footnote. 16\nText2SQL Spider (Yu et al., 2018b), Magellan (Das et al., 2015) or WikiSQL (Zhong et al., 2017b), and",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Table Classification FEVEROUS (Aly et al., 2021) focuses on both unstructured text and structured\ntables for fact extraction and verification tasks. Dresden Web Tables (Eberius et al., 2015)is useful for\ntasks requiring the classification of web table layouts, particularly useful in data extraction and web content\nanalysis where table structures are crucial. The dataset is in footnote. 16\nText2SQL Spider (Yu et al., 2018b), Magellan (Das et al., 2015) or WikiSQL (Zhong et al., 2017b), and"
        }
    },
    {
        "id": "4b46ab3e7402bd5e79cb2dabd4516e636d264b83",
        "content": "1.3.2 Opportunities for LLMs in tabular data modeling\nMany studies today explore the potential of using LLMs for various tabular data tasks, ranging from predic-\ntion, data generation, to data understanding (further divided into question answering and data reasoning).\nThis exploration is driven by LLMs’ unique capabilities such as in-context learning, instruction following,\nand step-wise reasoning. The opportunities for applying LLMs to tabular data modeling are as follows:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "1.3.2 Opportunities for LLMs in tabular data modeling\nMany studies today explore the potential of using LLMs for various tabular data tasks, ranging from predic-\ntion, data generation, to data understanding (further divided into question answering and data reasoning).\nThis exploration is driven by LLMs’ unique capabilities such as in-context learning, instruction following,\nand step-wise reasoning. The opportunities for applying LLMs to tabular data modeling are as follows:"
        }
    },
    {
        "id": "4c28456e0af3169b5558b8469e26432136e7f6de",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nMethod Description Example Papers that investigated this\nDFLoader Python code where a dictio- pd.DataFrame({ Singha et al. (2023)\nnary is loaded as a Pandas name:[‘helen’], age:[47] })\ndataframe\nJSON Row number as indexes, with {“0”: {“name”: “helen”, “age”: Singha et al. (2023); Sui et al. (2023b)\neach row represented as a “47”}}\ndictionary of keys (column\nnames) and values",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nMethod Description Example Papers that investigated this\nDFLoader Python code where a dictio- pd.DataFrame({ Singha et al. (2023)\nnary is loaded as a Pandas name:[‘helen’], age:[47] })\ndataframe\nJSON Row number as indexes, with {“0”: {“name”: “helen”, “age”: Singha et al. (2023); Sui et al. (2023b)\neach row represented as a “47”}}\ndictionary of keys (column\nnames) and values"
        }
    },
    {
        "id": "67e8dc1c032d8f534e892208feda972f38cb4e02",
        "content": "widespread applications across diverse domains such as finance, medicine, business, agriculture, education,\nand other sectors that heavily rely on relational databases (Sahakyan et al., 2021; Rundo et al., 2019;\nHernandez et al., 2022; Umer et al., 2019; Luan & Tsai, 2021).\nIn the current work, we provide a comprehensive review of recent advancements in modeling tabular data\nusing LLMs. In the first section, we introduce the characteristics of tabular data, then provide a brief re-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "widespread applications across diverse domains such as finance, medicine, business, agriculture, education,\nand other sectors that heavily rely on relational databases (Sahakyan et al., 2021; Rundo et al., 2019;\nHernandez et al., 2022; Umer et al., 2019; Luan & Tsai, 2021).\nIn the current work, we provide a comprehensive review of recent advancements in modeling tabular data\nusing LLMs. In the first section, we introduce the characteristics of tabular data, then provide a brief re-"
        }
    },
    {
        "id": "768a2b65af95dc607264385718924c8f18a05f02",
        "content": "text-to-sql semantic parsing: Two simple semantic boundary-based techniques. In Anna Rogers, Jordan L.\nBoyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp.\n150–160. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-SHORT.15. URL\nhttps://doi.org/10.18653/v1/2023.acl-short.15.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "text-to-sql semantic parsing: Two simple semantic boundary-based techniques. In Anna Rogers, Jordan L.\nBoyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp.\n150–160. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-SHORT.15. URL\nhttps://doi.org/10.18653/v1/2023.acl-short.15."
        }
    },
    {
        "id": "7bdb263fe86d8a728938cb3fcf012e20a093fab9",
        "content": "4. A survey and taxonomy of techniques for LLMs’ applications on tabular data. For each\napplication, we break down an extensive range of tabular data modeling methods by steps. For\n7",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "4. A survey and taxonomy of techniques for LLMs’ applications on tabular data. For each\napplication, we break down an extensive range of tabular data modeling methods by steps. For\n7"
        }
    },
    {
        "id": "926d4a037b20af2ee1225f3512db712c6d6fa578",
        "content": "tabular data classification and regression, 2022a.\nJintai Chen, Jiahuan Yan, Danny Ziyi Chen, and Jian Wu. Excelformer: A neural network surpassing gbdts\non tabular data, 2023a.\nNuo Chen, Linjun Shou, Ming Gong, Jian Pei, Chenyu You, Jianhui Chang, Daxin Jiang, and Jia Li.\nBridge the gap between language models and tabular understanding. CoRR, abs/2302.09302, 2023b. doi:\n10.48550/ARXIV.2302.09302. URL https://doi.org/10.48550/arXiv.2302.09302.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tabular data classification and regression, 2022a.\nJintai Chen, Jiahuan Yan, Danny Ziyi Chen, and Jian Wu. Excelformer: A neural network surpassing gbdts\non tabular data, 2023a.\nNuo Chen, Linjun Shou, Ming Gong, Jian Pei, Chenyu You, Jianhui Chang, Daxin Jiang, and Jia Li.\nBridge the gap between language models and tabular understanding. CoRR, abs/2302.09302, 2023b. doi:\n10.48550/ARXIV.2302.09302. URL https://doi.org/10.48550/arXiv.2302.09302."
        }
    },
    {
        "id": "9be921f825a628fa6e41abe5fa8a71fbb6e8d04b",
        "content": "with many open-source fine-tuned models available22.\nImpact of model size on performance Chen (2023) found that size does matter: On WebTableQues-\ntions, when comparing the 6.7B vs. 175B GPT-3 model, the smaller model achieved only half the scores of\nthe larger one. On TabFact, they found that smaller models (<=6.7B) obtained almost random accuracy.\nFinetuning or No finetuning? There are some larger models that fine-tune on various tabular tasks,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "with many open-source fine-tuned models available22.\nImpact of model size on performance Chen (2023) found that size does matter: On WebTableQues-\ntions, when comparing the 6.7B vs. 175B GPT-3 model, the smaller model achieved only half the scores of\nthe larger one. On TabFact, they found that smaller models (<=6.7B) obtained almost random accuracy.\nFinetuning or No finetuning? There are some larger models that fine-tune on various tabular tasks,"
        }
    },
    {
        "id": "ae0cb3b606b6ccf92324f6c9c5c147d69bf37eac",
        "content": "the tables. “Table size” had minimal improvements, while “header hierarchy” added unnecessary complexity,\nand hurt performance. For the Text2SQL task, Chang & Fosler-Lussier (2023) also find that some table\nrelationships and database content are useful. Huang et al. (2023b) report improvements in GPT-4’s accuracy\nby 28.9% when incorporating documentation that disambiguate terms present in the table like data column\nnames, value consistency, data coverage, and granularity.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the tables. “Table size” had minimal improvements, while “header hierarchy” added unnecessary complexity,\nand hurt performance. For the Text2SQL task, Chang & Fosler-Lussier (2023) also find that some table\nrelationships and database content are useful. Huang et al. (2023b) report improvements in GPT-4’s accuracy\nby 28.9% when incorporating documentation that disambiguate terms present in the table like data column\nnames, value consistency, data coverage, and granularity."
        }
    },
    {
        "id": "b1add51b730fe58d1420af72a506afb7dd7a4576",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nChaejeong Lee, Jayoung Kim, and Noseong Park. Codi: Co-evolving contrastive diffusion models for mixed-\ntype tabular synthesis. arXiv preprint arXiv:2304.12654, 2023.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinfor-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nChaejeong Lee, Jayoung Kim, and Noseong Park. Codi: Co-evolving contrastive diffusion models for mixed-\ntype tabular synthesis. arXiv preprint arXiv:2304.12654, 2023.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinfor-"
        }
    },
    {
        "id": "cd889eb8387ee793d8ac97b7b79b61f9f8acf574",
        "content": "AI (NLP4ConvAI 2023), pp. 59–70, Toronto, Canada, July 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.nlp4convai-1.6. URL https://aclanthology.org/2023.nlp4convai-1.6.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput.\nSurv., 55(6):109:1–109:28, 2023a. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "AI (NLP4ConvAI 2023), pp. 59–70, Toronto, Canada, July 2023. Association for Computational Linguis-\ntics. doi: 10.18653/v1/2023.nlp4convai-1.6. URL https://aclanthology.org/2023.nlp4convai-1.6.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput.\nSurv., 55(6):109:1–109:28, 2023a. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara"
        }
    },
    {
        "id": "d5d449edda2cd1bcbbe2ff87e7bf2d549dafa1f5",
        "content": "the-art results in predictive tasks without needing fine-tuning. Following the similar paradigm, Zhang et al.\n(2023e) proposed the TAPTAP12 (Table Pretraining for Tabular Prediction) which incorporates several\nenhancements. The method involves continue pretraining the GPT2 on 450 Kaggle/UCI/OpenML tables,\ngenerating label columns using a machine learning model. Other improvements improvements include a\nrevised numerical encoding scheme and the use of external models like gradient-boosted decision trees for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the-art results in predictive tasks without needing fine-tuning. Following the similar paradigm, Zhang et al.\n(2023e) proposed the TAPTAP12 (Table Pretraining for Tabular Prediction) which incorporates several\nenhancements. The method involves continue pretraining the GPT2 on 450 Kaggle/UCI/OpenML tables,\ngenerating label columns using a machine learning model. Other improvements improvements include a\nrevised numerical encoding scheme and the use of external models like gradient-boosted decision trees for"
        }
    },
    {
        "id": "d6089a805ebdc77c1dd592fae14686df6b21fe94",
        "content": "performance on few-shot prompted tasks (Wei et al., 2022b). The remarkable performance of LLMs have\nincited interest in both academia and industry, raising beliefs that they could serve as the foundation\nfor Artificial General Intelligence (AGI) of this era (Chang et al., 2024; Zhao et al., 2023b; Wei et al.,\n2022b). A noteworthy example is ChatGPT, designed specifically for engaging in human conversation, that\ndemonstrates the ability to comprehend and generate human language text (Liu et al., 2023g).1",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "performance on few-shot prompted tasks (Wei et al., 2022b). The remarkable performance of LLMs have\nincited interest in both academia and industry, raising beliefs that they could serve as the foundation\nfor Artificial General Intelligence (AGI) of this era (Chang et al., 2024; Zhao et al., 2023b; Wei et al.,\n2022b). A noteworthy example is ChatGPT, designed specifically for engaging in human conversation, that\ndemonstrates the ability to comprehend and generate human language text (Liu et al., 2023g).1"
        }
    },
    {
        "id": "f1caead09194f192a867722f4e871ad2ed4d1ade",
        "content": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo,\nS. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS\n2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022c. URL http://papers.nips.cc/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo,\nS. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information\nProcessing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS\n2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022c. URL http://papers.nips.cc/"
        }
    },
    {
        "id": "f40a85643517bfcbfcf51d0f789e077f9c594ade",
        "content": "positive results divided by the number of all samples that should have been identified as positive. CRPS is\ncontinous ranked probability score. We will introduce other metrics in relevant sections.\n3.2 Tabular prediction\nPreprocessing Preprocessing in LLM-based tabular prediction involves steps like table manipulation, serial-\nization, and prompt engineering, which have been discussed earlier. Specifically, some LLM-based prediction",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "positive results divided by the number of all samples that should have been identified as positive. CRPS is\ncontinous ranked probability score. We will introduce other metrics in relevant sections.\n3.2 Tabular prediction\nPreprocessing Preprocessing in LLM-based tabular prediction involves steps like table manipulation, serial-\nization, and prompt engineering, which have been discussed earlier. Specifically, some LLM-based prediction"
        }
    },
    {
        "id": "1e6841d9ca78550c87304c4a359d0ec356e254e3",
        "content": "approaches have demonstrated superior performance over classical methods such as Bayesian networks ((Xu\net al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data\nsynthesis methods can be found in Du & Li (2024).\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language\ninference (NLI), Text2SQL tasks, and more. Many earlier methods fine-tune BERT (Devlin et al., 2019)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "approaches have demonstrated superior performance over classical methods such as Bayesian networks ((Xu\net al., 2019)). A comprehensive understanding of the strengths and weaknesses of different tabular data\nsynthesis methods can be found in Du & Li (2024).\nTable understanding is a broad field, covering various tasks like question answering (QA), natural language\ninference (NLI), Text2SQL tasks, and more. Many earlier methods fine-tune BERT (Devlin et al., 2019)"
        }
    },
    {
        "id": "261e2b846fa0d3eefdf814f26e15e1e6c1cd519c",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nLarge Language Models (LLMs) on Tabular Data: Predic-\ntion, Generation, and Understanding - A Survey\nXi Fang∗ lxifan@amazon.com\nAmazon\nWeijie Xu∗ weijiexu@amazon.com\nAmazon\nFiona Anting Tan∗† fiona.anting.tan@gmail.com\nNational University of Singapore\nJiani Zhang zhajiani@amazon.com\nAWS\nZiqing Hu ziqinghu@amazon.com\nAWS\nYanjun Qi yanjunqi@amazon.com\nAWS\nUniversity of Virginia\nScott Nickleach nickleac@amazon.com\nAmazon",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nLarge Language Models (LLMs) on Tabular Data: Predic-\ntion, Generation, and Understanding - A Survey\nXi Fang∗ lxifan@amazon.com\nAmazon\nWeijie Xu∗ weijiexu@amazon.com\nAmazon\nFiona Anting Tan∗† fiona.anting.tan@gmail.com\nNational University of Singapore\nJiani Zhang zhajiani@amazon.com\nAWS\nZiqing Hu ziqinghu@amazon.com\nAWS\nYanjun Qi yanjunqi@amazon.com\nAWS\nUniversity of Virginia\nScott Nickleach nickleac@amazon.com\nAmazon"
        }
    },
    {
        "id": "2d2400e0510b1f2de28ed9a5ff1940e0a78f508a",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nHowever, for the few-shot classification task, they find that traditional list and text templates outperformed\nthe LLM-based serialization method. Amongst LLMs, the more complex and larger the LLM, the better the\nperformance (GPT-3 has 175B, T0 11B, and fine-tuned BLOOM model 0.56B parameters). A key reason\nwhy the LLMs are worse off at serializing tables to sentences is due to the tendency for LLMs to hallucinate:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nHowever, for the few-shot classification task, they find that traditional list and text templates outperformed\nthe LLM-based serialization method. Amongst LLMs, the more complex and larger the LLM, the better the\nperformance (GPT-3 has 175B, T0 11B, and fine-tuned BLOOM model 0.56B parameters). A key reason\nwhy the LLMs are worse off at serializing tables to sentences is due to the tendency for LLMs to hallucinate:"
        }
    },
    {
        "id": "2d75a7f8e1de181ee64c68f3d5ee3c11bd97aacd",
        "content": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor\nLeahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria\n30",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor\nLeahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria\n30"
        }
    },
    {
        "id": "3b20b9407ffd214d7d385b61419b116d6090179f",
        "content": "ries, and gaps in the existing literature, while providing some insights for future research\ndirections in this vital and rapidly evolving field. It also provides relevant code and datasets\nreferences. Through this comprehensive review, we hope to provide interested readers with\n∗These authors contributed equally to this work.\n†The author worked on this project during her intern at Amazon.\n1\n4202\nnuJ\n12\n]LC.sc[\n4v44971.2042:viXra",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ries, and gaps in the existing literature, while providing some insights for future research\ndirections in this vital and rapidly evolving field. It also provides relevant code and datasets\nreferences. Through this comprehensive review, we hope to provide interested readers with\n∗These authors contributed equally to this work.\n†The author worked on this project during her intern at Amazon.\n1\n4202\nnuJ\n12\n]LC.sc[\n4v44971.2042:viXra"
        }
    },
    {
        "id": "41231f1fd819a8a8e20621a22c951595148debd2",
        "content": "methods, Sundar & Heck (2023) designed a dual-encoder-based Dense Table Retrieval (DTR) model to rank\ncells of the table according to their relevance to the query. The ranked knowledge sources are incorporated\nwithin the prompt, and led to top ROUGE scores.\nRole-play Another popular prompt engineering technique is role-play, which refers to including descrip-\ntions in the prompt about the person the LLM should portray as it completes a task. For example, Zhao",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "methods, Sundar & Heck (2023) designed a dual-encoder-based Dense Table Retrieval (DTR) model to rank\ncells of the table according to their relevance to the query. The ranked knowledge sources are incorporated\nwithin the prompt, and led to top ROUGE scores.\nRole-play Another popular prompt engineering technique is role-play, which refers to including descrip-\ntions in the prompt about the person the LLM should portray as it completes a task. For example, Zhao"
        }
    },
    {
        "id": "6671bdf5c30e6460a3765cff860d9cf37fb73a30",
        "content": "features that led to the final decision. However, the performance of Shapley or other explanation methods\non tabular prediction and table understanding remains unexplored. Future research is needed to explore\nthe existed explanation mechanisms for LLM based tabular prediction and table understanding and develop\nmore suited explanation methods.\nEase of use Existed LLM based tools such as ChatGPT and models on huggingface are easy to inference.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "features that led to the final decision. However, the performance of Shapley or other explanation methods\non tabular prediction and table understanding remains unexplored. Future research is needed to explore\nthe existed explanation mechanisms for LLM based tabular prediction and table understanding and develop\nmore suited explanation methods.\nEase of use Existed LLM based tools such as ChatGPT and models on huggingface are easy to inference."
        }
    },
    {
        "id": "7815a77738be5134f8f5f51e725a36b6d34313a5",
        "content": "Ease of use Existed LLM based tools such as ChatGPT and models on huggingface are easy to inference.\nCurrently, most relevant tabular based LLM models require fine-tuning or data serialization, which could\nmake these models hard to access. Pretrained models, such as Wang et al. (2023c;a) which integrate data\nconsolidation, enrichment, and refinement, have the potential to streamline user experience. These methods",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Ease of use Existed LLM based tools such as ChatGPT and models on huggingface are easy to inference.\nCurrently, most relevant tabular based LLM models require fine-tuning or data serialization, which could\nmake these models hard to access. Pretrained models, such as Wang et al. (2023c;a) which integrate data\nconsolidation, enrichment, and refinement, have the potential to streamline user experience. These methods"
        }
    },
    {
        "id": "7a5ef039e12239a60707aeecddf29fd9d38b94da",
        "content": "intrinsic relations in numerical features (Gruver et al., 2023), and thus a careful embedding is needed. To-\nkenization significantly impacts pattern formation and operations in language models. Traditional methods\nlike Byte Pair Encoding (BPE) used in GPT-3 often split numbers into non-aligned tokens (e.g., 42235630\ninto [422, 35, 630]), complicating arithmetic. Newer models like LLaMA tokenize each digit separately. Both",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "intrinsic relations in numerical features (Gruver et al., 2023), and thus a careful embedding is needed. To-\nkenization significantly impacts pattern formation and operations in language models. Traditional methods\nlike Byte Pair Encoding (BPE) used in GPT-3 often split numbers into non-aligned tokens (e.g., 42235630\ninto [422, 35, 630]), complicating arithmetic. Newer models like LLaMA tokenize each digit separately. Both"
        }
    },
    {
        "id": "7b3e7f4b8a99b230d04939f151e76d3036065a42",
        "content": "Association, 26(3):228–241, 2019.\nPedro RAS Bassi, Sergio SJ Dertkigil, and Andrea Cavalli. Improving deep neural network generalization and\nrobustness to background bias via layer-wise relevance propagation optimization. Nature Communications,\n15(1):291, 2024.\nAnastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg Corrado,\nAndrew Carroll, Cory Y McLean, and Nicholas A Furlotte. Multimodal llms for health grounded in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Association, 26(3):228–241, 2019.\nPedro RAS Bassi, Sergio SJ Dertkigil, and Andrea Cavalli. Improving deep neural network generalization and\nrobustness to background bias via layer-wise relevance propagation optimization. Nature Communications,\n15(1):291, 2024.\nAnastasiya Belyaeva, Justin Cosentino, Farhad Hormozdiari, Krish Eswaran, Shravya Shetty, Greg Corrado,\nAndrew Carroll, Cory Y McLean, and Nicholas A Furlotte. Multimodal llms for health grounded in"
        }
    },
    {
        "id": "98906f183822f3c858d9e9c2705e8c03d1f742d9",
        "content": "et al., 2024) are inference-based techniques using GPT4, and surpassed previous fine-tuned smaller models.\nInterestingly, in the paper by Gao et al. (2024), the authors fine-tuned a Llama 2 13B model on the Text2SQL\ntasks. However, this model did not beat the GPT4 model that was not fine-tuned. Instead, many papers\nworking on using LLMs for table understanding tasks focus on tweaking aspects across serialization, prompt",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2024) are inference-based techniques using GPT4, and surpassed previous fine-tuned smaller models.\nInterestingly, in the paper by Gao et al. (2024), the authors fine-tuned a Llama 2 13B model on the Text2SQL\ntasks. However, this model did not beat the GPT4 model that was not fine-tuned. Instead, many papers\nworking on using LLMs for table understanding tasks focus on tweaking aspects across serialization, prompt"
        }
    },
    {
        "id": "b4defac586003f0d6b23233605767914b90f9754",
        "content": "tabular-specific behaviors of LLMs to enhance their performance on tasks related to tabular data.\nBias and fairness In existed tabular prediction and table understanding methods, LLMs tend to inherit\nsocial biases from their training data, which significantly impact their fairness metric. For example, Liu et al.\n(2023f) uses GPT3.5 and do few-shot learning to evaluate the fairness of tabular prediction on in context",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tabular-specific behaviors of LLMs to enhance their performance on tasks related to tabular data.\nBias and fairness In existed tabular prediction and table understanding methods, LLMs tend to inherit\nsocial biases from their training data, which significantly impact their fairness metric. For example, Liu et al.\n(2023f) uses GPT3.5 and do few-shot learning to evaluate the fairness of tabular prediction on in context"
        }
    },
    {
        "id": "bc1f394cd724c81c248dbfb059ee279eb309bf92",
        "content": "https://doi.org/10.18653/v1/2023.acl-short.15.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. Med-bert: pretrained contextualized em-\nbeddings on large-scale structured electronic health records for disease prediction. NPJ digital medicine,\n4(1):86, 2021.\nStephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu. Large test collection experiments\non an operational, interactive system: Okapi at TREC. Inf. Process. Manag., 31(3):345–360, 1995. doi:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "https://doi.org/10.18653/v1/2023.acl-short.15.\nLaila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, and Degui Zhi. Med-bert: pretrained contextualized em-\nbeddings on large-scale structured electronic health records for disease prediction. NPJ digital medicine,\n4(1):86, 2021.\nStephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu. Large test collection experiments\non an operational, interactive system: Okapi at TREC. Inf. Process. Manag., 31(3):345–360, 1995. doi:"
        }
    },
    {
        "id": "c3b6582761c9a6ecb5ee961ed439b62dad82782e",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nDataset Dataset Number Papers that used this dataset\nOpenML 11 Dinh et al. (2022); Manikandan et al. (2023)\nKaggle API 169 Hegselmann et al. (2023); Wang et al. (2023a); Zhang et al. (2023a)\nCombo 9 Hegselmann et al. (2023); Wang et al. (2023a); Zhang et al. (2023a)\nUCI ML 20 Manikandan et al. (2023); Slack & Singh (2023)\nDDX 10 Slack & Singh (2023)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nDataset Dataset Number Papers that used this dataset\nOpenML 11 Dinh et al. (2022); Manikandan et al. (2023)\nKaggle API 169 Hegselmann et al. (2023); Wang et al. (2023a); Zhang et al. (2023a)\nCombo 9 Hegselmann et al. (2023); Wang et al. (2023a); Zhang et al. (2023a)\nUCI ML 20 Manikandan et al. (2023); Slack & Singh (2023)\nDDX 10 Slack & Singh (2023)"
        }
    },
    {
        "id": "c552d79ecfff92bcc78e12ab9378d1b5a733bd4f",
        "content": "databases or spreadsheets. These manipulations involve actions such as filtering, sorting, joining, aggregating,\nand transforming data. An important characteristic of tabular data is its heterogeneity in structure and\ncontent. They often come in large size with different dimensions encompassing various feature types. In\norder for LLMs to ingest tabular data efficiently, it is important to compress the tables to fit context lengths,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "databases or spreadsheets. These manipulations involve actions such as filtering, sorting, joining, aggregating,\nand transforming data. An important characteristic of tabular data is its heterogeneity in structure and\ncontent. They often come in large size with different dimensions encompassing various feature types. In\norder for LLMs to ingest tabular data efficiently, it is important to compress the tables to fit context lengths,"
        }
    },
    {
        "id": "d62ba82224587f92fbd20d5879f08323818be151",
        "content": "Guang Liu, Jie Yang, and Ledell Wu. Ptab: Using the pre-trained language model for modeling tabular\ndata, 2022a.\nHanxi Liu, Xiaokai Mao, Haocheng Xia, Jian Lou, and Jinfei Liu. Prompt valuation based on shapley values.\nCoRR, abs/2312.15395, 2023a. doi: 10.48550/ARXIV.2312.15395. URL https://doi.org/10.48550/\narXiv.2312.15395.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Guang Liu, Jie Yang, and Ledell Wu. Ptab: Using the pre-trained language model for modeling tabular\ndata, 2022a.\nHanxi Liu, Xiaokai Mao, Haocheng Xia, Jian Lou, and Jinfei Liu. Prompt valuation based on shapley values.\nCoRR, abs/2312.15395, 2023a. doi: 10.48550/ARXIV.2312.15395. URL https://doi.org/10.48550/\narXiv.2312.15395.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A"
        }
    },
    {
        "id": "d9f2f88dd0e7b0198674f45175d608aca2ec3371",
        "content": "datasets. It shows that larger models always perform better. Time LLM (Jin et al., 2023a) presents a novel\napproach to time series forecasting by fine-tuning LLMs like LLaMa (Touvron et al., 2023a) and GPT-\n2 (Brown et al., 2020). Time-LLM is evaluated using metrics like the symmetric mean absolute percentage\nerror (SMAPE), the mean absolute scaled error (MSAE), and the overall weighted average (OWA). It demon-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "datasets. It shows that larger models always perform better. Time LLM (Jin et al., 2023a) presents a novel\napproach to time series forecasting by fine-tuning LLMs like LLaMa (Touvron et al., 2023a) and GPT-\n2 (Brown et al., 2020). Time-LLM is evaluated using metrics like the symmetric mean absolute percentage\nerror (SMAPE), the mean absolute scaled error (MSAE), and the overall weighted average (OWA). It demon-"
        }
    },
    {
        "id": "db9c1dbd7f52baea8185439c18ff28f86a4c1ebd",
        "content": "(LLaMa-2, Vicuna, Mistral, Starcoder, MPT, Qwen, AquilaChat2, etc.) lag behind.\nText2SQL Liu et al. (2023c) designed a question matcher that identifies three keyword types: 1) column\nname-related terms, 2) restriction-related phrases (e.g. \"top ten\"), and 3) algorithm or module keywords.\nOnce these keywords are identified, the module begins to merge the specific restrictions associated with\neach column into a unified combination, which is then matched with an SQL algorithm or module indicated",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(LLaMa-2, Vicuna, Mistral, Starcoder, MPT, Qwen, AquilaChat2, etc.) lag behind.\nText2SQL Liu et al. (2023c) designed a question matcher that identifies three keyword types: 1) column\nname-related terms, 2) restriction-related phrases (e.g. \"top ten\"), and 3) algorithm or module keywords.\nOnce these keywords are identified, the module begins to merge the specific restrictions associated with\neach column into a unified combination, which is then matched with an SQL algorithm or module indicated"
        }
    },
    {
        "id": "0127709708c5155c592f5529953115cddca676b7",
        "content": "Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 6970–\n6984. Association for Computational Linguistics, 2022b. doi: 10.18653/V1/2022.EMNLP-MAIN.469. URL\nhttps://doi.org/10.18653/v1/2022.emnlp-main.469.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 6970–\n6984. Association for Computational Linguistics, 2022b. doi: 10.18653/V1/2022.EMNLP-MAIN.469. URL\nhttps://doi.org/10.18653/v1/2022.emnlp-main.469.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-"
        }
    },
    {
        "id": "04981a040a0896d4fc40df14991ce01556893d4a",
        "content": "Prompt format The simplest format is concatenating task description with the serialized table as string.\nAn LLM would then attempt to perform the task described and return a text-based answer. Clearly-defined\nand well-formatted task descriptions are reported to be effective prompts (Marvin et al., 2023). Some other\nstrategies to improve performance are described in the next few paragraphs. Sui et al. (2023b) recommended",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Prompt format The simplest format is concatenating task description with the serialized table as string.\nAn LLM would then attempt to perform the task described and return a text-based answer. Clearly-defined\nand well-formatted task descriptions are reported to be effective prompts (Marvin et al., 2023). Some other\nstrategies to improve performance are described in the next few paragraphs. Sui et al. (2023b) recommended"
        }
    },
    {
        "id": "0553af69d5a8ce948aec3a3211ed144a95af2e55",
        "content": "quality data is essential for model development. Data generation is used for augmentation when the data\nis sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class\nrebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic\ndata generation are mostly based on Copulas (Patki et al., 2016; Li et al., 2020b) and Bayesian networks",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "quality data is essential for model development. Data generation is used for augmentation when the data\nis sparse (Onishi & Meguro, 2023), imputing missing values (Jolicoeur-Martineau et al., 2023), and class\nrebalancing in imbalanced data (Sauber-Cole & Khoshgoftaar, 2022). Traditional methods for synthetic\ndata generation are mostly based on Copulas (Patki et al., 2016; Li et al., 2020b) and Bayesian networks"
        }
    },
    {
        "id": "14b601777fd1a6c66a9e0dc2eb336c2302d00714",
        "content": "description, a summary, and example data points. Summary Boosting(Manikandan et al., 2023) serializes\ndata and metadata into text prompts for summary generation. This includes categorizing numerical fea-\ntures and using a representative dataset subset selected via weighted stratified sampling based on language\nembeddings. Serialize-LM (Jaitly et al., 2023) introduces 3 novel serialization techniques that boost LLM",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "description, a summary, and example data points. Summary Boosting(Manikandan et al., 2023) serializes\ndata and metadata into text prompts for summary generation. This includes categorizing numerical fea-\ntures and using a representative dataset subset selected via weighted stratified sampling based on language\nembeddings. Serialize-LM (Jaitly et al., 2023) introduces 3 novel serialization techniques that boost LLM"
        }
    },
    {
        "id": "40209c4c36e16b819a553e6d70d3eb24b36ed4b1",
        "content": "has the highest number of datasets, but the size of the largest dataset is only 5600 rows. Half of the datasets\nin UCI ML collections are relevant to medical use cases. Thus, the combo of 9 datasets is recommended\nfor benchmark 5 since it contains larger size datasets and more diverse feature sets. For general fine-tuning,\npublished methods choose the Kaggle API6 as it has 169 datasets, and its datasets are very diverse.\n12",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "has the highest number of datasets, but the size of the largest dataset is only 5600 rows. Half of the datasets\nin UCI ML collections are relevant to medical use cases. Thus, the combo of 9 datasets is recommended\nfor benchmark 5 since it contains larger size datasets and more diverse feature sets. For general fine-tuning,\npublished methods choose the Kaggle API6 as it has 169 datasets, and its datasets are very diverse.\n12"
        }
    },
    {
        "id": "431e66e068ed896c81c68ae4e76733d98de7080c",
        "content": "Some other methods leverage an LLM to rewrite the serialization or perform prompt engineering.\nTabLLM (Hegselmann et al., 2023) showed that LLM is not good for serialization because it is not faithful\nand may hallucinate. Summary Boosting(Manikandan et al., 2023) uses GPT3 to convert metadata to data\ndescription and generate a summary for a subset of datasets in each sample round. TABLET (Slack &\nSingh, 2023) fits a simple model such as a one-layer rule set model or prototype with the 10 most important",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Some other methods leverage an LLM to rewrite the serialization or perform prompt engineering.\nTabLLM (Hegselmann et al., 2023) showed that LLM is not good for serialization because it is not faithful\nand may hallucinate. Summary Boosting(Manikandan et al., 2023) uses GPT3 to convert metadata to data\ndescription and generate a summary for a subset of datasets in each sample round. TABLET (Slack &\nSingh, 2023) fits a simple model such as a one-layer rule set model or prototype with the 10 most important"
        }
    },
    {
        "id": "43a0dc6f38b967a1b0c3915424a21734a503c712",
        "content": "for Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp.1028.\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Chris-\ntos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. FEVEROUS: fact extrac-\ntion and verification over unstructured and structured information. In Joaquin Vanschoren\nand Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "for Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp.1028.\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Chris-\ntos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. FEVEROUS: fact extrac-\ntion and verification over unstructured and structured information. In Joaquin Vanschoren\nand Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track"
        }
    },
    {
        "id": "5e2fefe0786d7c06f0931d86e27d99f2ff663b77",
        "content": "1 1 2 2\ntasks. The same conclusion was drawn in TabLLM (Hegselmann et al., 2023) where they evaluated 9\ndifferent serialization methods along with a description for the classification problem. They found that a\ntextual enumeration of all features: ’The column name is Value’, performs the best. For medical prediction,\nthey mimic the thinking process of medical professionals as prompt engineering and found that LLM makes\nuse of column name and their relationships in few-shot learning settings.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "1 1 2 2\ntasks. The same conclusion was drawn in TabLLM (Hegselmann et al., 2023) where they evaluated 9\ndifferent serialization methods along with a description for the classification problem. They found that a\ntextual enumeration of all features: ’The column name is Value’, performs the best. For medical prediction,\nthey mimic the thinking process of medical professionals as prompt engineering and found that LLM makes\nuse of column name and their relationships in few-shot learning settings."
        }
    },
    {
        "id": "917619fb89bf6aa3d46d6a48d27f2cb67d7dfdee",
        "content": "Raul Castro Fernandez, Aaron J. Elmore, Michael J. Franklin, Sanjay Krishnan, and Chenhao Tan. How\nlarge language models will disrupt data management. Proc. VLDB Endow., 16(11):3302–3309, 2023. doi:\n10.14778/3611479.3611527. URL https://www.vldb.org/pvldb/vol16/p3302-fernandez.pdf.\nJoao Fonseca and Fernando Bacao. Tabular and latent space synthetic data generation: a literature review.\nJournal of Big Data, 10(1):115, 2023.\n33",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Raul Castro Fernandez, Aaron J. Elmore, Michael J. Franklin, Sanjay Krishnan, and Chenhao Tan. How\nlarge language models will disrupt data management. Proc. VLDB Endow., 16(11):3302–3309, 2023. doi:\n10.14778/3611479.3611527. URL https://www.vldb.org/pvldb/vol16/p3302-fernandez.pdf.\nJoao Fonseca and Fernando Bacao. Tabular and latent space synthetic data generation: a literature review.\nJournal of Big Data, 10(1):115, 2023.\n33"
        }
    },
    {
        "id": "972bc2fd0b44e5b2844e483e197066f8d226c7f0",
        "content": "terized the it into four different stages: The first stage is Statistical Language Models (SLM), which\nlearns the probability of word occurrence in an example sequence from previous words (e.g. N-Gram) based\non Markov assumption (Saul & Pereira, 1997). Although a more accurate prediction can be achieved by\nincreasing the context window, SLM is limited by the curse of high dimensionality and high demand for com-\nputation power (Bengio et al., 2000). Next, Neural Language Models (NLM) utilize neural networks",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "terized the it into four different stages: The first stage is Statistical Language Models (SLM), which\nlearns the probability of word occurrence in an example sequence from previous words (e.g. N-Gram) based\non Markov assumption (Saul & Pereira, 1997). Although a more accurate prediction can be achieved by\nincreasing the context window, SLM is limited by the curse of high dimensionality and high demand for com-\nputation power (Bengio et al., 2000). Next, Neural Language Models (NLM) utilize neural networks"
        }
    },
    {
        "id": "abe07d5f7db4285ae20a8173da37051de9635a04",
        "content": "19Official site for InfoTabs: https://infotabs.github.io/. Official site for TabFact: https://tabfact.github.io/\n20Official sites for the domain-specific datasets: AIT-QA (https://github.com/IBM/AITQA), TAT-QA (https://github.com/\nNExTplusplus/TAT-QA), SciGen (https://github.com/UKPLab/SciGen) and TranX (https://github.com/pcyin/tranX)\n21The dataset for TaBERT is in https://github.com/facebookresearch/TaBERT. The dataset for TAPAS is in https://\ngithub.com/google-research/tapas\n24",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "19Official site for InfoTabs: https://infotabs.github.io/. Official site for TabFact: https://tabfact.github.io/\n20Official sites for the domain-specific datasets: AIT-QA (https://github.com/IBM/AITQA), TAT-QA (https://github.com/\nNExTplusplus/TAT-QA), SciGen (https://github.com/UKPLab/SciGen) and TranX (https://github.com/pcyin/tranX)\n21The dataset for TaBERT is in https://github.com/facebookresearch/TaBERT. The dataset for TAPAS is in https://\ngithub.com/google-research/tapas\n24"
        }
    },
    {
        "id": "b05c041d890d1bee629ae6c4b836503b9a4c1c00",
        "content": "metadata, leading to confusion in the model’s interpretation of inputs. Better categorical features encod-\ning is needed to solve these problems. Traditional machine learning methods such as lightGBM require\nexpanding dimension for categorical features (Borisov et al., 2022b) and can lead to bias categorical rep-\nresentation (Prokhorenkova et al., 2019). Thus, good categorical features encoding could add competitive\nadvantage for LLM based method compared to traditional machine learning methods.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "metadata, leading to confusion in the model’s interpretation of inputs. Better categorical features encod-\ning is needed to solve these problems. Traditional machine learning methods such as lightGBM require\nexpanding dimension for categorical features (Borisov et al., 2022b) and can lead to bias categorical rep-\nresentation (Prokhorenkova et al., 2019). Thus, good categorical features encoding could add competitive\nadvantage for LLM based method compared to traditional machine learning methods."
        }
    },
    {
        "id": "c1d456b3961fa88a15d786e961f2bd2bb2a5e3aa",
        "content": "the need for clarification; and Conversational Question Answering (CQA) to directly produce the answer as\nthe response if it is not required for clarification. They trained a UniPCQA model which unifies all subtasks\nin QA through multi-task learning.\n5.3.2 Search and retrieval\nThe ability to accurately search and retrieve information from specific positions within structured data is\ncrucial for LLMs. There are two types of search and retrieval use-cases: (1) to find the information (table,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the need for clarification; and Conversational Question Answering (CQA) to directly produce the answer as\nthe response if it is not required for clarification. They trained a UniPCQA model which unifies all subtasks\nin QA through multi-task learning.\n5.3.2 Search and retrieval\nThe ability to accurately search and retrieve information from specific positions within structured data is\ncrucial for LLMs. There are two types of search and retrieval use-cases: (1) to find the information (table,"
        }
    },
    {
        "id": "c494c4e28051b7a6eb672ddc1803fb0602e23b3f",
        "content": "https://doi.org/10.48550/arXiv.2310.00789.\nRick Sauber-Cole and Taghi M Khoshgoftaar. The use of generative adversarial networks to alleviate class\nimbalance in tabular data: a survey. Journal of Big Data, 9(1):98, 2022.\nLawrence Saul and Fernando Pereira. Aggregate and mixed-order markov models for statistical language\nprocessing, 1997.\nNabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated llm: Synergy of llms",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "https://doi.org/10.48550/arXiv.2310.00789.\nRick Sauber-Cole and Taghi M Khoshgoftaar. The use of generative adversarial networks to alleviate class\nimbalance in tabular data: a survey. Journal of Big Data, 9(1):98, 2022.\nLawrence Saul and Fernando Pereira. Aggregate and mixed-order markov models for statistical language\nprocessing, 1997.\nNabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated llm: Synergy of llms"
        }
    },
    {
        "id": "e39d2b4607884b984034041955cfa12d01569693",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables, 2015b.\nNeha Patki, Roy Wedge, and Kalyan Veeramachaneni. The synthetic data vault. In 2016 IEEE In-\nternational Conference on Data Science and Advanced Analytics (DSAA), pp. 399–410, 2016. doi:\n10.1109/DSAA.2016.49.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables, 2015b.\nNeha Patki, Roy Wedge, and Kalyan Veeramachaneni. The synthetic data vault. In 2016 IEEE In-\nternational Conference on Data Science and Advanced Analytics (DSAA), pp. 399–410, 2016. doi:\n10.1109/DSAA.2016.49.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke"
        }
    },
    {
        "id": "f5180cd944bbd1be4ac88fe36731ff76faa40507",
        "content": "embeddings. Serialize-LM (Jaitly et al., 2023) introduces 3 novel serialization techniques that boost LLM\nperformance in domain-specific datasets. They included related features in one sentence to make the prompt\nmore descriptive and easier for an LLM to understand. Take the task of car classification as an example:\nattributes like make, color, and body type are now combined into a single,richer sentence. It leverages",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "embeddings. Serialize-LM (Jaitly et al., 2023) introduces 3 novel serialization techniques that boost LLM\nperformance in domain-specific datasets. They included related features in one sentence to make the prompt\nmore descriptive and easier for an LLM to understand. Take the task of car classification as an example:\nattributes like make, color, and body type are now combined into a single,richer sentence. It leverages"
        }
    },
    {
        "id": "fcd892a828b28b7d0968519f94a726cc51eccf4f",
        "content": "few-shot, and zero-shot learning scenarios within the medical domain, demonstrating superior performance\ncompared to gradient-boosting methods and existing LLM-based approaches. However, it may have limited\napplicability beyond the medical domain. The code is available.9 for tabular prediction tasks specifically in\nthe medical domain. On top of AUCROC, they also use a precision-recall curve (PRAUC) for evaluation.\nPRAUC is useful in imbalanced datasets, which is always the case for medical data.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "few-shot, and zero-shot learning scenarios within the medical domain, demonstrating superior performance\ncompared to gradient-boosting methods and existing LLM-based approaches. However, it may have limited\napplicability beyond the medical domain. The code is available.9 for tabular prediction tasks specifically in\nthe medical domain. On top of AUCROC, they also use a precision-recall curve (PRAUC) for evaluation.\nPRAUC is useful in imbalanced datasets, which is always the case for medical data."
        }
    },
    {
        "id": "01d941642cad0d5b7e559294d2cbb72394bf8090",
        "content": "Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pp. 4756–4765. Association for Computational Linguistics, 2023. doi: 10.18653/V1/\n2023.FINDINGS-ACL.292. URL https://doi.org/10.18653/v1/2023.findings-acl.292.\nTao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir Radev. Typesql: Knowledge-based type-aware\nneural text-to-sql generation, 2018a.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pp. 4756–4765. Association for Computational Linguistics, 2023. doi: 10.18653/V1/\n2023.FINDINGS-ACL.292. URL https://doi.org/10.18653/v1/2023.findings-acl.292.\nTao Yu, Zifan Li, Zilin Zhang, Rui Zhang, and Dragomir Radev. Typesql: Knowledge-based type-aware\nneural text-to-sql generation, 2018a."
        }
    },
    {
        "id": "0a8f29d965d7ef8cd0261a106e0511e261c02371",
        "content": "(DANet (Chen et al., 2022a), T2G-Former (Yan et al., 2023), ExcelFormer (Chen et al., 2023a), ARM-net\n(Cai et al., 2021)), or aiding intrasample information sharing (SAINT (Somepalli et al., 2021), NPT (Kossen\net al., 2022)). 4. Regularization methods. The importance of features varies in tabular data, in contrast\nto image or text data. Thus, this line of research seeks to design an optimal and dynamic regularization",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(DANet (Chen et al., 2022a), T2G-Former (Yan et al., 2023), ExcelFormer (Chen et al., 2023a), ARM-net\n(Cai et al., 2021)), or aiding intrasample information sharing (SAINT (Somepalli et al., 2021), NPT (Kossen\net al., 2022)). 4. Regularization methods. The importance of features varies in tabular data, in contrast\nto image or text data. Thus, this line of research seeks to design an optimal and dynamic regularization"
        }
    },
    {
        "id": "18369b512445bc61024e8d562a1d0e106fa83b80",
        "content": "(2023f) uses GPT3.5 and do few-shot learning to evaluate the fairness of tabular prediction on in context\nlearning. For LLMs based tabular prediction method, the research concludes that the fairness metric gap\nbetween different subgroups is larger than that in traditional machine learning model. Additionally, the\nresearch further reveals that flipping the labels of the in-context examples significantly narrows the gap in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(2023f) uses GPT3.5 and do few-shot learning to evaluate the fairness of tabular prediction on in context\nlearning. For LLMs based tabular prediction method, the research concludes that the fairness metric gap\nbetween different subgroups is larger than that in traditional machine learning model. Additionally, the\nresearch further reveals that flipping the labels of the in-context examples significantly narrows the gap in"
        }
    },
    {
        "id": "1851a2ad4198fb77bcd2f19672b5607c1c7a3f6b",
        "content": "but labor-intensive way is manual labeling, as was used by Serilize-LM (Jaitly et al., 2023). To be more\nautomatic, LIFT (Dinh et al., 2022) utilizes ### and @@@ to demarcate question-answer pairs and signify\nthe end of generation. These markers prompt the LLM to position answers between ### and @@@. This\napproach significantly aligns most generated answers with the intended labels. Additionally, to address",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "but labor-intensive way is manual labeling, as was used by Serilize-LM (Jaitly et al., 2023). To be more\nautomatic, LIFT (Dinh et al., 2022) utilizes ### and @@@ to demarcate question-answer pairs and signify\nthe end of generation. These markers prompt the LLM to position answers between ### and @@@. This\napproach significantly aligns most generated answers with the intended labels. Additionally, to address"
        }
    },
    {
        "id": "1b3a9bc6c22c700aae4cec1e64602cc2613ce6e5",
        "content": "working on using LLMs for table understanding tasks focus on tweaking aspects across serialization, prompt\nengineering, search and retrieval, and end-to-end pipelines (user interfaces), which we describe further in the\nnext section.\n5.3 Key components in Tabular QA\nIn the simplest QA architecture, an LLM takes in an input prompt (query and serialized table)23, and\nreturns an answer. In more involved architectures, the system might be connected to external databases",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "working on using LLMs for table understanding tasks focus on tweaking aspects across serialization, prompt\nengineering, search and retrieval, and end-to-end pipelines (user interfaces), which we describe further in the\nnext section.\n5.3 Key components in Tabular QA\nIn the simplest QA architecture, an LLM takes in an input prompt (query and serialized table)23, and\nreturns an answer. In more involved architectures, the system might be connected to external databases"
        }
    },
    {
        "id": "261d967f554f6506271066393e94a635227446c1",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nderstanding, enabling them to produce synthetic datasets faithful to real-world statistics, with semantic\ncoherence and contextuality (Sui et al., 2024).\n4.1 Methodologies\nTable 6 summarizes different LLM-powered table synthesis methods. Except for CLLM (Seedat et al., 2023),\nwhich utilizes prior knowledge from LLMs (e.g., GPT4) to augment and enhance training data samples in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nderstanding, enabling them to produce synthetic datasets faithful to real-world statistics, with semantic\ncoherence and contextuality (Sui et al., 2024).\n4.1 Methodologies\nTable 6 summarizes different LLM-powered table synthesis methods. Except for CLLM (Seedat et al., 2023),\nwhich utilizes prior knowledge from LLMs (e.g., GPT4) to augment and enhance training data samples in"
        }
    },
    {
        "id": "3471fc92285f503d0ee427f85cacd19a2d14a538",
        "content": "Shangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, Chung-Wei Hsiung, Yi-Chen Hsieh, Yu-Ping\nCheng, Sian-Hong Luo, and Jianwei Zhang. Jarvix: A LLM no code platform for tabular data analysis and\noptimization. In Mingxuan Wang and Imed Zitouni (eds.), Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing: EMNLP 2023 - Industry Track, Singapore, December 6-10,\n2023, pp. 622–630. Association for Computational Linguistics, 2023c. URL https://aclanthology.org/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Shangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, Chung-Wei Hsiung, Yi-Chen Hsieh, Yu-Ping\nCheng, Sian-Hong Luo, and Jianwei Zhang. Jarvix: A LLM no code platform for tabular data analysis and\noptimization. In Mingxuan Wang and Imed Zitouni (eds.), Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing: EMNLP 2023 - Industry Track, Singapore, December 6-10,\n2023, pp. 622–630. Association for Computational Linguistics, 2023c. URL https://aclanthology.org/"
        }
    },
    {
        "id": "54f833928a11dab6eb04c26326a5b379dc7713f2",
        "content": "et al., 2022). Deep learning models, however, may have their advantages over traditional methods in some\ncircumstances, for example, when facing very large datasets, or when the data is primarily comprised of\ncategorical features (Borisov et al., 2022a).\nAnother important task for tabular data modeling is data synthesis. The ability to synthesize real and high-\nquality data is essential for model development. Data generation is used for augmentation when the data",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2022). Deep learning models, however, may have their advantages over traditional methods in some\ncircumstances, for example, when facing very large datasets, or when the data is primarily comprised of\ncategorical features (Borisov et al., 2022a).\nAnother important task for tabular data modeling is data synthesis. The ability to synthesize real and high-\nquality data is essential for model development. Data generation is used for augmentation when the data"
        }
    },
    {
        "id": "5dc98ed6332c2dd193566ed8ecec13721ba5b04d",
        "content": "Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX:\ntable pre-training via learning a neural SQL executor. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022c. URL https:\n//openreview.net/forum?id=O50443AsCP.\nShangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, Chung-Wei Hsiung, Yi-Chen Hsieh, Yu-Ping",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX:\ntable pre-training via learning a neural SQL executor. In The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022c. URL https:\n//openreview.net/forum?id=O50443AsCP.\nShangching Liu, Shengkun Wang, Tsungyao Chang, Wenqi Lin, Chung-Wei Hsiung, Yi-Chen Hsieh, Yu-Ping"
        }
    },
    {
        "id": "5fbc943699bec6417bf6d49326e52230bc4a865e",
        "content": "on Computational Linguistics, pp. 1978–1988, Barcelona, Spain (Online), December 2020. Interna-\ntional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.179. URL https:\n//aclanthology.org/2020.coling-main.179.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models\nfor tabular data. Advances in Neural Information Processing Systems, 34:18932–18943, 2021.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "on Computational Linguistics, pp. 1978–1988, Barcelona, Spain (Online), December 2020. Interna-\ntional Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.179. URL https:\n//aclanthology.org/2020.coling-main.179.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nYury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models\nfor tabular data. Advances in Neural Information Processing Systems, 34:18932–18943, 2021."
        }
    },
    {
        "id": "6862266977fecd276fae4770dc05c3a2b2aa0c16",
        "content": "2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf.\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. TableGPT:\nFew-shot table-to-text generation with table structure reconstruction and content matching. In Do-\nnia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference\non Computational Linguistics, pp. 1978–1988, Barcelona, Spain (Online), December 2020. Interna-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf.\nHeng Gong, Yawei Sun, Xiaocheng Feng, Bing Qin, Wei Bi, Xiaojiang Liu, and Ting Liu. TableGPT:\nFew-shot table-to-text generation with table structure reconstruction and content matching. In Do-\nnia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference\non Computational Linguistics, pp. 1978–1988, Barcelona, Spain (Online), December 2020. Interna-"
        }
    },
    {
        "id": "70c9af366619b75aee10eff1ae6dc18a97af457c",
        "content": "on an operational, interactive system: Okapi at TREC. Inf. Process. Manag., 31(3):345–360, 1995. doi:\n10.1016/0306-4573(94)00051-4. URL https://doi.org/10.1016/0306-4573(94)00051-4.\nFrancesco Rundo, Francesca Trenta, Agatino Luigi di Stallo, and Sebastiano Battiato. Machine learning for\nquantitative finance applications: A survey. Applied Sciences, 9(24):5574, 2019.\nMaria Sahakyan, Zeyar Aung, and Talal Rahwan. Explainable artificial intelligence for tabular data: A",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "on an operational, interactive system: Okapi at TREC. Inf. Process. Manag., 31(3):345–360, 1995. doi:\n10.1016/0306-4573(94)00051-4. URL https://doi.org/10.1016/0306-4573(94)00051-4.\nFrancesco Rundo, Francesca Trenta, Agatino Luigi di Stallo, and Sebastiano Battiato. Machine learning for\nquantitative finance applications: A survey. Applied Sciences, 9(24):5574, 2019.\nMaria Sahakyan, Zeyar Aung, and Talal Rahwan. Explainable artificial intelligence for tabular data: A"
        }
    },
    {
        "id": "72acc541ac7ed1c028db5c08944140cbb3075b67",
        "content": "v1/2022.findings-acl.41. URL https://aclanthology.org/2022.findings-acl.41.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski,\nHailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree\nBandaru, Jacob Cunningham, Caiming Xiong, and Dragomir R. Radev. Fetaqa: Free-form table question\nanswering. Trans. Assoc. Comput. Linguistics, 10:35–49, 2022. doi: 10.1162/TACL\\_A\\_00446. URL\nhttps://doi.org/10.1162/tacl_a_00446.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "v1/2022.findings-acl.41. URL https://aclanthology.org/2022.findings-acl.41.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski,\nHailey Schoelkopf, Riley Kong, Xiangru Tang, Mutethia Mutuma, Ben Rosand, Isabel Trindade, Renusree\nBandaru, Jacob Cunningham, Caiming Xiong, and Dragomir R. Radev. Fetaqa: Free-form table question\nanswering. Trans. Assoc. Comput. Linguistics, 10:35–49, 2022. doi: 10.1162/TACL\\_A\\_00446. URL\nhttps://doi.org/10.1162/tacl_a_00446."
        }
    },
    {
        "id": "89421e6820f48580e291b68271eaab4d06109a9f",
        "content": "Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davison. Table search using a deep con-\ntextualized language model. In Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR ’20. ACM, July 2020c. doi: 10.1145/3397271.3401044.\nURL http://dx.doi.org/10.1145/3397271.3401044.\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian D. Davison. Table search using a deep con-\ntextualized language model. In Proceedings of the 43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR ’20. ACM, July 2020c. doi: 10.1145/3397271.3401044.\nURL http://dx.doi.org/10.1145/3397271.3401044.\nHeng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen"
        }
    },
    {
        "id": "936efae0ca5b0f772dc6d5062d69640d7110ef32",
        "content": "Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers,\n40",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers,\n40"
        }
    },
    {
        "id": "9c57c3216056b6c10e0d847c6cd2fd04876a0527",
        "content": "Tabular data, commonly known as structured data, refers to data organized into rows and columns, where\neach column represents a specific feature. This subsection discusses the common characteristics and inherited\nchallenges with tabular data:\n1. Heterogeneity: Tabular data can contain different feature types: categorical, numerical, binary,\nand textual. Therefore, features can range from being dense numerical features to sparse or high-\ncardinality categorical features (Borisov et al., 2022a).",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tabular data, commonly known as structured data, refers to data organized into rows and columns, where\neach column represents a specific feature. This subsection discusses the common characteristics and inherited\nchallenges with tabular data:\n1. Heterogeneity: Tabular data can contain different feature types: categorical, numerical, binary,\nand textual. Therefore, features can range from being dense numerical features to sparse or high-\ncardinality categorical features (Borisov et al., 2022a)."
        }
    },
    {
        "id": "a0a891fa392b9a46ac554b2e792fc4dbdce0c3a6",
        "content": "Table-GPT with 350M, 3B, 13B or 175B parameters (based on various versions of OpenAI’s GPT models).\nGraph-based & Tree-based A possible, but less explored serialization method involves converting a\ntable to a graph or tree data structure. However, when working with sequence-to-sequence models, these\nstructures must still be converted back to text. For Zhao et al. (2023a), after converting the table into a",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Table-GPT with 350M, 3B, 13B or 175B parameters (based on various versions of OpenAI’s GPT models).\nGraph-based & Tree-based A possible, but less explored serialization method involves converting a\ntable to a graph or tree data structure. However, when working with sequence-to-sequence models, these\nstructures must still be converted back to text. For Zhao et al. (2023a), after converting the table into a"
        }
    },
    {
        "id": "ac89c6d6a7683d1dca49ee654e64efb1156192af",
        "content": "predict if a car repair claim is fraudulent or not.\nthat they use LLM for time series. However, most of these papers use LLM which is smaller than 1B. We\nwill not discuss these methods here. Please refer to Jin et al. (2023b) for a complete introduction of these\nmethods.\nPreprocessing PromptCast (Xue & Salim, 2022) uses raw value as input the time series data in text format\nand adds a minimal description of the task; as output, it uses the target after it converts it to a sentence.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "predict if a car repair claim is fraudulent or not.\nthat they use LLM for time series. However, most of these papers use LLM which is smaller than 1B. We\nwill not discuss these methods here. Please refer to Jin et al. (2023b) for a complete introduction of these\nmethods.\nPreprocessing PromptCast (Xue & Salim, 2022) uses raw value as input the time series data in text format\nand adds a minimal description of the task; as output, it uses the target after it converts it to a sentence."
        }
    },
    {
        "id": "bf689e86d444aa08b5b857aefbfbd6d858094252",
        "content": "2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nlH1PV42cbF.\nEdward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, and Jimeng Sun. Generating\nmulti-label discrete patient records using generative adversarial networks, 2018.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nlH1PV42cbF.\nEdward Choi, Siddharth Biswal, Bradley Malin, Jon Duke, Walter F. Stewart, and Jimeng Sun. Generating\nmulti-label discrete patient records using generative adversarial networks, 2018.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac"
        }
    },
    {
        "id": "cc9194f8df7db9b084d521c4fc01a71b8142b5c4",
        "content": "et al., 2023a).\n4. Context-based interconnection: In tabular data, features can be correlated. For example, age,\neducation, and alcohol consumption from a demographic table are interconnected: it is hard to get\na doctoral degree at a young age, and there is a minimum legal drinking age. Including correlated\nregressors in regressions lead to biased coefficients, hence, a modeler must be aware of such intricacies\n(Liu et al., 2023d).",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2023a).\n4. Context-based interconnection: In tabular data, features can be correlated. For example, age,\neducation, and alcohol consumption from a demographic table are interconnected: it is hard to get\na doctoral degree at a young age, and there is a minimum legal drinking age. Including correlated\nregressors in regressions lead to biased coefficients, hence, a modeler must be aware of such intricacies\n(Liu et al., 2023d)."
        }
    },
    {
        "id": "e2b856562558f69530104bbc89b83623e245bab6",
        "content": "revised numerical encoding scheme and the use of external models like gradient-boosted decision trees for\n11The code is in https://github.com/kathrinse/be_great\n12The code is in https://github.com/ZhangTP1996/TapTap\n19",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "revised numerical encoding scheme and the use of external models like gradient-boosted decision trees for\n11The code is in https://github.com/kathrinse/be_great\n12The code is in https://github.com/ZhangTP1996/TapTap\n19"
        }
    },
    {
        "id": "eda42ce6786adfb50c4e8a617a8b037d9128144b",
        "content": "USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n72223cc66f63ca1aa59edaec1b3670e6-Abstract-Conference.html.\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin.\nCatboost: unbiased boosting with categorical features, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/\n72223cc66f63ca1aa59edaec1b3670e6-Abstract-Conference.html.\nLiudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin.\nCatboost: unbiased boosting with categorical features, 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,"
        }
    },
    {
        "id": "f26320fcbeda5a8b24349bcb7b58ebf358646cbd",
        "content": "Tat-Seng Chua. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in\nfinance. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pp. 3277–3287, Online, August 2021a. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.acl-long.254. URL https://aclanthology.org/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tat-Seng Chua. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in\nfinance. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pp. 3277–3287, Online, August 2021a. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.acl-long.254. URL https://aclanthology.org/"
        }
    },
    {
        "id": "fafd557f149c2418021307f372d9948ced4b842e",
        "content": "and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL\nhttps://doi.org/10.48550/arXiv.2307.09288.\nMuhammad Umer, Muhammad Awais, and Muhammad Muzammul. Stock market prediction using machine\nlearning (ml) algorithms. ADCAIJ: Advances in Distributed Computing and Artificial Intelligence Journal,\n8(4):97–116, 2019.\nL Vivek Harsha Vardhan and Stanley Kok. Generating privacy-preserving synthetic tabular data using",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/ARXIV.2307.09288. URL\nhttps://doi.org/10.48550/arXiv.2307.09288.\nMuhammad Umer, Muhammad Awais, and Muhammad Muzammul. Stock market prediction using machine\nlearning (ml) algorithms. ADCAIJ: Advances in Distributed Computing and Artificial Intelligence Journal,\n8(4):97–116, 2019.\nL Vivek Harsha Vardhan and Stanley Kok. Generating privacy-preserving synthetic tabular data using"
        }
    },
    {
        "id": "ffba788f865a6a51ffbf581cd7244411800f131e",
        "content": "8(4):97–116, 2019.\nL Vivek Harsha Vardhan and Stanley Kok. Generating privacy-preserving synthetic tabular data using\noblivious variational autoencoders. In Proceedings of the Workshop on Economics of Privacy and Data\nLabor at the 37 th International Conference on Machine Learning (ICML), 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "8(4):97–116, 2019.\nL Vivek Harsha Vardhan and Stanley Kok. Generating privacy-preserving synthetic tabular data using\noblivious variational autoencoders. In Proceedings of the Workshop on Economics of Privacy and Data\nLabor at the 37 th International Conference on Machine Learning (ICML), 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy"
        }
    },
    {
        "id": "132c0a6b8832620846e2e15eb18ef9d85bd29534",
        "content": "sequences and demographic data with text tokens. This approach is efficient and allows integration with high-\nperforming models from various domains but adds complexity due to its non-end-to-end training nature and\nresults in communication between components that is not human-readable. This approach could be adapted\nto LLM for tabular data to improve the encoding of non-text data such as categorical and numerical feature.\n7 Conclusion",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "sequences and demographic data with text tokens. This approach is efficient and allows integration with high-\nperforming models from various domains but adds complexity due to its non-end-to-end training nature and\nresults in communication between components that is not human-readable. This approach could be adapted\nto LLM for tabular data to improve the encoding of non-text data such as categorical and numerical feature.\n7 Conclusion"
        }
    },
    {
        "id": "1dfa70a8c15ed5961d0509716135d34cf031a1b6",
        "content": "abilities of LLMs when working with tabular data for various tasks, such as prediction, table understanding,\nquantitative reasoning, and data generation (Hegselmann et al., 2023; Sui et al., 2023c; Borisov et al., 2023a).\nTabular data stands as one of the pervasive and essential data formats in machine learning (ML), with\nwidespread applications across diverse domains such as finance, medicine, business, agriculture, education,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "abilities of LLMs when working with tabular data for various tasks, such as prediction, table understanding,\nquantitative reasoning, and data generation (Hegselmann et al., 2023; Sui et al., 2023c; Borisov et al., 2023a).\nTabular data stands as one of the pervasive and essential data formats in machine learning (ML), with\nwidespread applications across diverse domains such as finance, medicine, business, agriculture, education,"
        }
    },
    {
        "id": "3f5b79cb90ff0f772d8b72c360d79820b8260d0b",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nhallucinations have critical consequences. Akhtar et al. (2023) found that hallucination led to performance\ndrops in reasoning for LLMs. To address these issues in tabular prediction, Wang et al. (2023c) incorporated\nan audit module that utilizes LLMs to perform self-check and self-correction. They generated pseudo-labels,\nthen used a data audit module which filters the data based on data Shapley scores, leading to a smaller",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nhallucinations have critical consequences. Akhtar et al. (2023) found that hallucination led to performance\ndrops in reasoning for LLMs. To address these issues in tabular prediction, Wang et al. (2023c) incorporated\nan audit module that utilizes LLMs to perform self-check and self-correction. They generated pseudo-labels,\nthen used a data audit module which filters the data based on data Shapley scores, leading to a smaller"
        }
    },
    {
        "id": "4f7f73a52cacdbff496482eeac183cbc50fe281b",
        "content": "Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners, 2020.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners, 2020."
        }
    },
    {
        "id": "5542c118103169242e0f8c674ca594b6a6228c59",
        "content": "each column into a unified combination, which is then matched with an SQL algorithm or module indicated\nby the third type of keyword. Zhang et al. (2023d) opted for a more straightforward approach of tasking\nLLaMa-2 to generate an SQL statement based on a question and table schema. Sun et al. (2023b) finetuned\nPaLM-2 on the Text2SQL task, achieving considerable performance on Spider. OpenTab (Kong et al., 2024)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "each column into a unified combination, which is then matched with an SQL algorithm or module indicated\nby the third type of keyword. Zhang et al. (2023d) opted for a more straightforward approach of tasking\nLLaMa-2 to generate an SQL statement based on a question and table schema. Sun et al. (2023b) finetuned\nPaLM-2 on the Text2SQL task, achieving considerable performance on Spider. OpenTab (Kong et al., 2024)"
        }
    },
    {
        "id": "822ad561657a7c201e2c23c9b68e95fe60ae96bf",
        "content": "data for few-shot classification with large language models, 2023.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. StructGPT: A general\nframework for large language model to reason over structured data. In Houda Bouamor, Juan Pino,\nand Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 9237–9251, Singapore, December 2023. Association for Computational Linguistics. doi:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "data for few-shot classification with large language models, 2023.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Xin Zhao, and Ji-Rong Wen. StructGPT: A general\nframework for large language model to reason over structured data. In Houda Bouamor, Juan Pino,\nand Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing, pp. 9237–9251, Singapore, December 2023. Association for Computational Linguistics. doi:"
        }
    },
    {
        "id": "8d911eadb6c94017fa5bbcbcf733b67d53fa60bb",
        "content": "use of column name and their relationships in few-shot learning settings.\nIn a subsequent study, TABLET (Slack & Singh, 2023) included naturally occurring instructions along\nwith examples for serialization. In this case, where the task is for medical diagnosis, naturally occurring\ninstructions are from consumer-friendly sources, such as government health websites or technical references\nsuch as the Merck Manual. It includes instructions, examples, and test data points. They found that",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "use of column name and their relationships in few-shot learning settings.\nIn a subsequent study, TABLET (Slack & Singh, 2023) included naturally occurring instructions along\nwith examples for serialization. In this case, where the task is for medical diagnosis, naturally occurring\ninstructions are from consumer-friendly sources, such as government health websites or technical references\nsuch as the Merck Manual. It includes instructions, examples, and test data points. They found that"
        }
    },
    {
        "id": "a65ce7db8b462d0c824d40104fdc174d65389e48",
        "content": "ization, and prompt engineering, which have been discussed earlier. Specifically, some LLM-based prediction\nmethods incorporated a statistical summary of the tabular data as part of the input to LLM. Serialization in\nthe prediction task is mostly Text-based (refer to Section 2.1). Prompt engineering includes incorporating\ntask-specific cues and relevant samples into the prompt (refer to Section 2.1). The various preprocessing\nmethods are illustrated in Table 4 and discussed in detail below.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ization, and prompt engineering, which have been discussed earlier. Specifically, some LLM-based prediction\nmethods incorporated a statistical summary of the tabular data as part of the input to LLM. Serialization in\nthe prediction task is mostly Text-based (refer to Section 2.1). Prompt engineering includes incorporating\ntask-specific cues and relevant samples into the prompt (refer to Section 2.1). The various preprocessing\nmethods are illustrated in Table 4 and discussed in detail below."
        }
    },
    {
        "id": "bc583a24056ad88288e57f41a6066fe7cab71cfd",
        "content": "error (SMAPE), the mean absolute scaled error (MSAE), and the overall weighted average (OWA). It demon-\nstrates notable performance in few-shot learning scenarios, where only 5 percent or 10 percent of the data\nare used. This innovative technique underscores the versatility of LLMs in handling complex forecasting\ntasks. For TEST (Sun et al., 2023a), soft prompts are used for fine-tuning. The paper evaluates models",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "error (SMAPE), the mean absolute scaled error (MSAE), and the overall weighted average (OWA). It demon-\nstrates notable performance in few-shot learning scenarios, where only 5 percent or 10 percent of the data\nare used. This innovative technique underscores the versatility of LLMs in handling complex forecasting\ntasks. For TEST (Sun et al., 2023a), soft prompts are used for fine-tuning. The paper evaluates models"
        }
    },
    {
        "id": "c17c3f4654de22287ea5c81bfd8fe7fa8ad7611c",
        "content": "URL https://aclanthology.org/2022.acl-long.78.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir\nRadev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Binding language models\nin symbolic languages. In The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nlH1PV42cbF.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "URL https://aclanthology.org/2022.acl-long.78.\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir\nRadev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. Binding language models\nin symbolic languages. In The Eleventh International Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/pdf?id=\nlH1PV42cbF."
        }
    },
    {
        "id": "f9685c042e976dd4d993c45a9d3833d480e087c9",
        "content": "using LLMs. In the first section, we introduce the characteristics of tabular data, then provide a brief re-\nview of traditional, deep-learning and LLM methods tailored for this area. In Section 2, we introduce key\ntechniques related to the adaptation of tabular data for LLMs. Subsequently, we cover the applications of\nLLMs in prediction tasks (Section 3), data augmentation and enrichment tasks (Section 4), and question an-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "using LLMs. In the first section, we introduce the characteristics of tabular data, then provide a brief re-\nview of traditional, deep-learning and LLM methods tailored for this area. In Section 2, we introduce key\ntechniques related to the adaptation of tabular data for LLMs. Subsequently, we cover the applications of\nLLMs in prediction tasks (Section 3), data augmentation and enrichment tasks (Section 4), and question an-"
        }
    },
    {
        "id": "fca120e01fc0871dc49fe25aa2134b25f0df2702",
        "content": "//aclanthology.org/P17-1167.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn.\nRes., 2022, 2022. URL https://openreview.net/forum?id=jKN1pXi7b0.\nSukriti Jaitly, Tanay Shah, Ashish Shugani, and Razik Singh Grewal. Towards better serialization of tabular\ndata for few-shot classification with large language models, 2023.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "//aclanthology.org/P17-1167.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn.\nRes., 2022, 2022. URL https://openreview.net/forum?id=jKN1pXi7b0.\nSukriti Jaitly, Tanay Shah, Ashish Shugani, and Razik Singh Grewal. Towards better serialization of tabular\ndata for few-shot classification with large language models, 2023."
        }
    },
    {
        "id": "ff77d12ca6f6bbb061783c4dfc9f72a6b7d0644a",
        "content": "Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim.\nData synthesis based on generative adversarial networks. Proceedings of the VLDB Endowment, 11(10):\n1071–1083, June 2018. ISSN 2150-8097. doi: 10.14778/3231751.3231757. URL http://dx.doi.org/\n10.14778/3231751.3231757.\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceed-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim.\nData synthesis based on generative adversarial networks. Proceedings of the VLDB Endowment, 11(10):\n1071–1083, June 2018. ISSN 2150-8097. doi: 10.14778/3231751.3231757. URL http://dx.doi.org/\n10.14778/3231751.3231757.\nPanupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceed-"
        }
    },
    {
        "id": "058c24902afafd1880bf8d66aefe1f787ccc4352",
        "content": "degrades to random guesses.\nThirdly, longer prompts incur higher costs, especially for applications built upon LLM APIs.\nTo address these issues, Herzig et al. (2020); Liu et al. (2022c) proposed methods to truncate the input\nbased on a maximum sequence length. Sui et al. (2023b) introduced predefined certain constraints to meet\nthe LLM call request. Another strategy is to do search and retrieval of only highly relevant tables, rows,\ncolumns or cells which we will discuss later in Section 5.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "degrades to random guesses.\nThirdly, longer prompts incur higher costs, especially for applications built upon LLM APIs.\nTo address these issues, Herzig et al. (2020); Liu et al. (2022c) proposed methods to truncate the input\nbased on a maximum sequence length. Sui et al. (2023b) introduced predefined certain constraints to meet\nthe LLM call request. Another strategy is to do search and retrieval of only highly relevant tables, rows,\ncolumns or cells which we will discuss later in Section 5."
        }
    },
    {
        "id": "137c56a536ab998324e1a99565920d49586a1e70",
        "content": "Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: table-operations\naware fact verification via sentence-table cloze pre-training. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 4971–4983. Association for\nComputational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.331. URL https://doi.org/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, and Xiaoyong Du. PASTA: table-operations\naware fact verification via sentence-table cloze pre-training. In Yoav Goldberg, Zornitsa Kozareva, and Yue\nZhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 4971–4983. Association for\nComputational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.331. URL https://doi.org/"
        }
    },
    {
        "id": "26cb43ed7e2c35c12ecaa2b1f7fea65b1c1e9cdc",
        "content": "2. Tree-based models are not naturally suited for processing sequential data, such as time series while deep\nlearning models such as Recurrent Neural Networks (RNNs) and transformers excel in handling sequential\n3",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2. Tree-based models are not naturally suited for processing sequential data, such as time series while deep\nlearning models such as Recurrent Neural Networks (RNNs) and transformers excel in handling sequential\n3"
        }
    },
    {
        "id": "29cf0774fc6fb80d9f253f7cebdd41b4e4dd8c57",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts. CoRR, abs/2307.03172, 2023b. doi:\n10.48550/ARXIV.2307.03172. URL https://doi.org/10.48550/arXiv.2307.03172.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. Lost in the middle: How language models use long contexts. CoRR, abs/2307.03172, 2023b. doi:\n10.48550/ARXIV.2307.03172. URL https://doi.org/10.48550/arXiv.2307.03172.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. TAPEX:"
        }
    },
    {
        "id": "2d81d732a620b1aa0837e4ca5d1a1a7cef39317e",
        "content": "Table QA For table QA datasets, FetaQA (Nan et al., 2022), WikiTableQuestion (Pasupat & Liang, 2015a),\nHybridQA (Chen et al., 2020b) and SQA (Iyyer et al., 2017) are popular options. Unlike WikiTableQuestions,\nwhich focuses on evaluating a QA system’s ability to understand queries and retrieve short-form answers from\ntabular data, FeTaQA introduces elements that require deeper reasoning and integration of information. This",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Table QA For table QA datasets, FetaQA (Nan et al., 2022), WikiTableQuestion (Pasupat & Liang, 2015a),\nHybridQA (Chen et al., 2020b) and SQA (Iyyer et al., 2017) are popular options. Unlike WikiTableQuestions,\nwhich focuses on evaluating a QA system’s ability to understand queries and retrieve short-form answers from\ntabular data, FeTaQA introduces elements that require deeper reasoning and integration of information. This"
        }
    },
    {
        "id": "2e3c1869f759027b2f434f5c98e6f8fdff46bcd6",
        "content": "Text2SQL Spider (Yu et al., 2018b), Magellan (Das et al., 2015) or WikiSQL (Zhong et al., 2017b), and\nBIRD (Li et al., 2023c) are suitable for training and evaluating models that generate SQL commands. Both\nSpider and WikiSQL have been benchmarked by many existing methods, some shown in Table 7. Compared\nto Spider, WikiSQL is much larger in size.17. The BIRD (BIg Bench for LaRge-scale Database Grounded\nText-to-SQL Evaluation) benchmark contains large tables and complex questions, and it been widely used",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Text2SQL Spider (Yu et al., 2018b), Magellan (Das et al., 2015) or WikiSQL (Zhong et al., 2017b), and\nBIRD (Li et al., 2023c) are suitable for training and evaluating models that generate SQL commands. Both\nSpider and WikiSQL have been benchmarked by many existing methods, some shown in Table 7. Compared\nto Spider, WikiSQL is much larger in size.17. The BIRD (BIg Bench for LaRge-scale Database Grounded\nText-to-SQL Evaluation) benchmark contains large tables and complex questions, and it been widely used"
        }
    },
    {
        "id": "45fc89a220ff5a05318518b1178cbced3d4575c0",
        "content": "Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault\nFévry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and\nAlexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault\nFévry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and\nAlexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth"
        }
    },
    {
        "id": "4913d7337febb074aaca81260c4a5fda69e6fdb1",
        "content": "finance related table question answer, TAT-QA Zhu et al. (2021a) assesses numerical reasoning, involving\noperations like addition, subtraction, and comparison. SciGen (Moosavi et al., 2021) focuses on assess-\ning the arithmetic reasoning capabilities of generation models on complex input structures, such as tables\nfrom scientific articles. TranX (Yin & Neubig, 2018) investigates abstract syntax description language for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "finance related table question answer, TAT-QA Zhu et al. (2021a) assesses numerical reasoning, involving\noperations like addition, subtraction, and comparison. SciGen (Moosavi et al., 2021) focuses on assess-\ning the arithmetic reasoning capabilities of generation models on complex input structures, such as tables\nfrom scientific articles. TranX (Yin & Neubig, 2018) investigates abstract syntax description language for"
        }
    },
    {
        "id": "4c12c7a1914f6e1372f26b87af9e2346eb996742",
        "content": "Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.331. URL https://doi.org/\n10.18653/v1/2022.emnlp-main.331.\nManbir S Gulati and Paul F Roysdon. TabMT: Generating tabular data with masked transformers. In Thirty-\nseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/\nforum?id=qs4swxtIAQ.\nCheng Guo and Felix Berkhahn. Entity embeddings of categorical variables, 2016.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.331. URL https://doi.org/\n10.18653/v1/2022.emnlp-main.331.\nManbir S Gulati and Paul F Roysdon. TabMT: Generating tabular data with masked transformers. In Thirty-\nseventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/\nforum?id=qs4swxtIAQ.\nCheng Guo and Felix Berkhahn. Entity embeddings of categorical variables, 2016."
        }
    },
    {
        "id": "54bc5f0f3a8945b8feb537545b1f139257651529",
        "content": "2022a; Peters et al., 2018b). Along this line, several other Pretrained Language Models (PLM) were\nproposed utilizing a transformer architecture with self-attention mechanisms including BERT and GPT2\n(Ding et al., 2023). The pre-training and fine-tuning paradigm, closely related to transfer learning, allows\nthe model to gain general syntactic and semantic understanding of the text corpus and then be trained",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2022a; Peters et al., 2018b). Along this line, several other Pretrained Language Models (PLM) were\nproposed utilizing a transformer architecture with self-attention mechanisms including BERT and GPT2\n(Ding et al., 2023). The pre-training and fine-tuning paradigm, closely related to transfer learning, allows\nthe model to gain general syntactic and semantic understanding of the text corpus and then be trained"
        }
    },
    {
        "id": "5f781bddc11996260f6905be03088a4a46546b5e",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nis the header (94-97% accuracy). Zhao et al. (2023e) investigated the effects of SOTA Table QA models\non manipulations on the table header, table content and natural language question (phrasing).4 They find\nthat all examined Table QA models (TaPas, TableFormer, TaPEX, OmniTab, GPT3) are not robust under\nadversarial attacks.\n2.3 Prompt Engineering",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nis the header (94-97% accuracy). Zhao et al. (2023e) investigated the effects of SOTA Table QA models\non manipulations on the table header, table content and natural language question (phrasing).4 They find\nthat all examined Table QA models (TaPas, TableFormer, TaPEX, OmniTab, GPT3) are not robust under\nadversarial attacks.\n2.3 Prompt Engineering"
        }
    },
    {
        "id": "61e7ec43016d0a804573b7c8146e57087c497282",
        "content": "TabMT’s architecture is scalable, making it suitable for diverse datasets and demonstrating improved per-\nformance in synthetic data generation tasks.\nThe generation process. To minimize bias from a fixed order of column names, TabMT randomly selects\ncolumn names without replacement during the generation process and subsequently samples the column\nvalues based on the predicted column distribution. TabMT initially sets the masking probability for all",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "TabMT’s architecture is scalable, making it suitable for diverse datasets and demonstrating improved per-\nformance in synthetic data generation tasks.\nThe generation process. To minimize bias from a fixed order of column names, TabMT randomly selects\ncolumn names without replacement during the generation process and subsequently samples the column\nvalues based on the predicted column distribution. TabMT initially sets the masking probability for all"
        }
    },
    {
        "id": "6cc22c32118714f972f2e8a4e2c964df41b27819",
        "content": "practitioners can leverage to describe their methods, find relevant techniques and understand the\ndifference between these techniques. We further subdivide each technique to subsections so that\nresearchers can easily find relevant benchmark techniques and properly categorize their proposed\ntechniques.\n2. A survey and taxonomy of metrics for LLMs’ applications on tabular data. For each\napplication, we categorize and discuss a wide range of metrics that can be used to evaluate the",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "practitioners can leverage to describe their methods, find relevant techniques and understand the\ndifference between these techniques. We further subdivide each technique to subsections so that\nresearchers can easily find relevant benchmark techniques and properly categorize their proposed\ntechniques.\n2. A survey and taxonomy of metrics for LLMs’ applications on tabular data. For each\napplication, we categorize and discuss a wide range of metrics that can be used to evaluate the"
        }
    },
    {
        "id": "8105dfb5046b23f0b839535e925d762bf8143a2e",
        "content": "Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. TUTA: tree-based\ntransformers for generally structured table pre-training. In Feida Zhu, Beng Chin Ooi, and Chunyan Miao\n(eds.), KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual\nEvent, Singapore, August 14-18, 2021, pp. 1780–1790. ACM, 2021. doi: 10.1145/3447548.3467434. URL\nhttps://doi.org/10.1145/3447548.3467434.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. TUTA: tree-based\ntransformers for generally structured table pre-training. In Feida Zhu, Beng Chin Ooi, and Chunyan Miao\n(eds.), KDD ’21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual\nEvent, Singapore, August 14-18, 2021, pp. 1780–1790. ACM, 2021. doi: 10.1145/3447548.3467434. URL\nhttps://doi.org/10.1145/3447548.3467434."
        }
    },
    {
        "id": "ae87fa3088425fe305563ff911d3703a12eda87c",
        "content": "Improved neural networks for tabular data via row attention and contrastive pre-training, 2021.\nDimitris Spathis and Fahim Kawsar. The first step is the hardest: Pitfalls of representing and tokenizing\ntemporal data for large language models, 2023.\nEthan Steinberg, Ken Jung, Jason A Fries, Conor K Corbin, Stephen R Pfohl, and Nigam H Shah. Language\nmodels are an effective representation learning technique for electronic health record data. Journal of\nbiomedical informatics, 113:103637, 2021.\n41",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Improved neural networks for tabular data via row attention and contrastive pre-training, 2021.\nDimitris Spathis and Fahim Kawsar. The first step is the hardest: Pitfalls of representing and tokenizing\ntemporal data for large language models, 2023.\nEthan Steinberg, Ken Jung, Jason A Fries, Conor K Corbin, Stephen R Pfohl, and Nigam H Shah. Language\nmodels are an effective representation learning technique for electronic health record data. Journal of\nbiomedical informatics, 113:103637, 2021.\n41"
        }
    },
    {
        "id": "b79238ae958f5f1253e4c2eec48c798cf01c1c9c",
        "content": "and structured data (Yin et al., 2020a), PTab demonstrated the importance of cross-table training for an\nenhanced representation learning (Liu et al., 2022a), CT-BERT employs masked table modeling (MTM)\nand contrastive learning for cross-table pretraining that outperformed tree-based models (Ye et al., 2023a).\nHowever, previous research primarily focuses on using LM for representation learning, which is quite limited.\n1.3.2 Opportunities for LLMs in tabular data modeling",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and structured data (Yin et al., 2020a), PTab demonstrated the importance of cross-table training for an\nenhanced representation learning (Liu et al., 2022a), CT-BERT employs masked table modeling (MTM)\nand contrastive learning for cross-table pretraining that outperformed tree-based models (Ye et al., 2023a).\nHowever, previous research primarily focuses on using LM for representation learning, which is quite limited.\n1.3.2 Opportunities for LLMs in tabular data modeling"
        }
    },
    {
        "id": "d8b5a3185b0f94c8fb431d82476635be8dfc81be",
        "content": "Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Gpt4table: Can large language\nmodels understand structured table data? a benchmark and empirical study, 2023b.\nYuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, and Dongmei Zhang. Tap4llm: Table\nprovider on sampling, augmenting, and packing semi-structured data for large language model reasoning,\n2023c.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can large language",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Gpt4table: Can large language\nmodels understand structured table data? a benchmark and empirical study, 2023b.\nYuan Sui, Jiaru Zou, Mengyu Zhou, Xinyi He, Lun Du, Shi Han, and Dongmei Zhang. Tap4llm: Table\nprovider on sampling, augmenting, and packing semi-structured data for large language model reasoning,\n2023c.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Table meets llm: Can large language"
        }
    },
    {
        "id": "002e078762102068020ee0c7af8a6146706761e7",
        "content": "standing of semi-structured tabulated text, which involves comprehending both text fragments and their\nimplicit relationships. InfoTabs is particularly useful for studying complex, multi-faceted reasoning over\nsemi-structured, multi-domain, and heterogeneous data. Meanwhile, TabFact (Chen et al., 2020a) consists\nof human-annotated natural language statements about Wikipedia tables. It requires linguistic reasoning\nand symbolic reasoning to get right answer. The dataset is in footnote. 19.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "standing of semi-structured tabulated text, which involves comprehending both text fragments and their\nimplicit relationships. InfoTabs is particularly useful for studying complex, multi-faceted reasoning over\nsemi-structured, multi-domain, and heterogeneous data. Meanwhile, TabFact (Chen et al., 2020a) consists\nof human-annotated natural language statements about Wikipedia tables. It requires linguistic reasoning\nand symbolic reasoning to get right answer. The dataset is in footnote. 19."
        }
    },
    {
        "id": "0ded249746ab39a6cbfd740709f2b0a84b05569e",
        "content": "et al., 2017a). Some methodologies are sketch-based, wherein a natural language question is translated\ninto a sketch. Subsequently, programming language techniques such as type-directed sketch completion and\nautomatic repair are utilized in an iterative manner to refine the initial sketch, ultimately producing the\nfinal query (e.g. SQLizer (Yaghmazadeh et al., 2017)). Another example is SQLNet (Xu et al., 2017) which",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2017a). Some methodologies are sketch-based, wherein a natural language question is translated\ninto a sketch. Subsequently, programming language techniques such as type-directed sketch completion and\nautomatic repair are utilized in an iterative manner to refine the initial sketch, ultimately producing the\nfinal query (e.g. SQLizer (Yaghmazadeh et al., 2017)). Another example is SQLNet (Xu et al., 2017) which"
        }
    },
    {
        "id": "16163eea28e4570273a3c5be1046ff940b03f4da",
        "content": "shot settings, the improvement is the most significant. For Ye et al. (2023b), they obtained higher scores on\nTabFact when using their framework with the PASTA (Gu et al., 2022) model (score 93.00%) as compared\nto the GPT-3 Codex (code-davinci-002) (scored 85.60%). PASTA was pre-trained on a synthesized corpus\nof 1.2 million items from WikiTables for six types of sentence–table cloze tasks. This suggests there remains\nsome benefit in using LMs fine-tuned on tabular tasks.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "shot settings, the improvement is the most significant. For Ye et al. (2023b), they obtained higher scores on\nTabFact when using their framework with the PASTA (Gu et al., 2022) model (score 93.00%) as compared\nto the GPT-3 Codex (code-davinci-002) (scored 85.60%). PASTA was pre-trained on a synthesized corpus\nof 1.2 million items from WikiTables for six types of sentence–table cloze tasks. This suggests there remains\nsome benefit in using LMs fine-tuned on tabular tasks."
        }
    },
    {
        "id": "25214c1da0699bddbb44742f0bc28967d82522cd",
        "content": "2022a; Chung et al., 2022). Solving complex tasks involving multiple steps have been challenging for LLMs.\nBy including intermediate reasoning steps, prompting strategies such as chain-of-thought (CoT) has been\nshown to help unlock the LLM ability to tackle complex arithmetic, commonsense, and symbolic reasoning\ntasks (Wei et al., 2023). These new abilities of LLMs lay the groundwork for exploring their integration into",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2022a; Chung et al., 2022). Solving complex tasks involving multiple steps have been challenging for LLMs.\nBy including intermediate reasoning steps, prompting strategies such as chain-of-thought (CoT) has been\nshown to help unlock the LLM ability to tackle complex arithmetic, commonsense, and symbolic reasoning\ntasks (Wei et al., 2023). These new abilities of LLMs lay the groundwork for exploring their integration into"
        }
    },
    {
        "id": "2dc829681a641ca081c56d192b8918d6348a1d42",
        "content": "Tianyang Liu, Fei Wang, and Muhao Chen. Rethinking tabular data understanding with large language\nmodels, 2023e.\nYanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. Investigating the fairness of large\nlanguage models for predictions on tabular data. Short Version in NeurIPS 2023 Workshop on Socially\nResponsible Language Modelling Research, 2023f.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Tianyang Liu, Fei Wang, and Muhao Chen. Rethinking tabular data understanding with large language\nmodels, 2023e.\nYanchen Liu, Srishti Gautam, Jiaqi Ma, and Himabindu Lakkaraju. Investigating the fairness of large\nlanguage models for predictions on tabular data. Short Version in NeurIPS 2023 Workshop on Socially\nResponsible Language Modelling Research, 2023f.\nYiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,"
        }
    },
    {
        "id": "3a8f1a537e0f609603cae18daddea83fed7b92d3",
        "content": "9Available at https://Github.com/RyanWangZf/MediTab.\n10The dataset is in https://huggingface.co/datasets/yuweiyin/FinBench and the code for FinPT is in https://Github.com/\nYuweiYin/FinPT\n18",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "9Available at https://Github.com/RyanWangZf/MediTab.\n10The dataset is in https://huggingface.co/datasets/yuweiyin/FinBench and the code for FinPT is in https://Github.com/\nYuweiYin/FinPT\n18"
        }
    },
    {
        "id": "442c7a96ab02cf39bd2a711ef2842b29bf8b1a02",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nand target probabilities p ∈ P , they define a function serialize target(t, p) that serializes target classes and\nprobabilities into a sequence formatted as “class t : p , t : p , . . . ”. We give an example for each method\n1 1 2 2\nin 5\nInference-Only Prediction Some work uses LLMs directly for prediction without fine-tuning, we refer\nto these approaches as inference-only prediction. TABLET (Slack & Singh, 2023) utilizes models like Tk-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nand target probabilities p ∈ P , they define a function serialize target(t, p) that serializes target classes and\nprobabilities into a sequence formatted as “class t : p , t : p , . . . ”. We give an example for each method\n1 1 2 2\nin 5\nInference-Only Prediction Some work uses LLMs directly for prediction without fine-tuning, we refer\nto these approaches as inference-only prediction. TABLET (Slack & Singh, 2023) utilizes models like Tk-"
        }
    },
    {
        "id": "48d52cd059412f2a7321b463a002460fd267ab4c",
        "content": "(eds.), Proceedings of the 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, pp. 174–184. ACM, 2023b. doi:\n10.1145/3539618.3591708. URL https://doi.org/10.1145/3539618.3591708.\nPengcheng Yin and Graham Neubig. TRANX: A transition-based neural abstract syntax parser for semantic\nparsing and code generation. In Eduardo Blanco and Wei Lu (eds.), Proceedings of the 2018 Conference on",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(eds.), Proceedings of the 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, pp. 174–184. ACM, 2023b. doi:\n10.1145/3539618.3591708. URL https://doi.org/10.1145/3539618.3591708.\nPengcheng Yin and Graham Neubig. TRANX: A transition-based neural abstract syntax parser for semantic\nparsing and code generation. In Eduardo Blanco and Wei Lu (eds.), Proceedings of the 2018 Conference on"
        }
    },
    {
        "id": "4f2035642240b5fa4422972f7de9291c1b99035d",
        "content": "assigned probabilities to each category. They extend the target into categorical one-hot encoding and then\nuse an external predictor to create the calibrated probability distributions. This replaces the 0/1 one-hot\nencoding while maintaining the final prediction outcome. Formally, given the target classes t ∈ 0, ..., |C|\n7Here is the Github repo for TABLET https://Github.com/dylan-slack/Tablet, TabLLM https://Github.com/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "assigned probabilities to each category. They extend the target into categorical one-hot encoding and then\nuse an external predictor to create the calibrated probability distributions. This replaces the 0/1 one-hot\nencoding while maintaining the final prediction outcome. Formally, given the target classes t ∈ 0, ..., |C|\n7Here is the Github repo for TABLET https://Github.com/dylan-slack/Tablet, TabLLM https://Github.com/"
        }
    },
    {
        "id": "5cb043735f751f7ae5d48d840199b0db449a62f0",
        "content": "Abstract\nRecent breakthroughs in large language modeling have facilitated rigorous exploration of\ntheir application in diverse tasks related to tabular data modeling, such as prediction, tabu-\nlar data synthesis, question answering, and table understanding. Each task presents unique\nchallenges and opportunities. However, there is currently a lack of comprehensive review\nthat summarizes and compares the key techniques, metrics, datasets, models, and optimiza-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Abstract\nRecent breakthroughs in large language modeling have facilitated rigorous exploration of\ntheir application in diverse tasks related to tabular data modeling, such as prediction, tabu-\nlar data synthesis, question answering, and table understanding. Each task presents unique\nchallenges and opportunities. However, there is currently a lack of comprehensive review\nthat summarizes and compares the key techniques, metrics, datasets, models, and optimiza-"
        }
    },
    {
        "id": "603772a1591c0ed7175398f4b7aaabb58aa74fe9",
        "content": "sentences serialization methods as shown in Table 1, followed by fine-tuning a GPT-2 or GPT-2 distill\nmodel. Additionally, a feature order permutation step precedes the use of obtained sentences for LLM\nfine-tuning. REaLTabFormer (Solatorio & Dupriez, 2023) extends GReaT by generating synthetic non-\nrelational and relational tabular data. It uses GReaT (an autoregressive GPT-2 model) to generate a parent\ntable and a sequence-to-sequence model conditioned on the parent table for the relational dataset. The",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "sentences serialization methods as shown in Table 1, followed by fine-tuning a GPT-2 or GPT-2 distill\nmodel. Additionally, a feature order permutation step precedes the use of obtained sentences for LLM\nfine-tuning. REaLTabFormer (Solatorio & Dupriez, 2023) extends GReaT by generating synthetic non-\nrelational and relational tabular data. It uses GReaT (an autoregressive GPT-2 model) to generate a parent\ntable and a sequence-to-sequence model conditioned on the parent table for the relational dataset. The"
        }
    },
    {
        "id": "754724c6174f5edd636f3c4ff9591ede2f787aab",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nTabMT (Gulati & Roysdon, 2023) employs a masked transformer-based architecture. The design allows\nefficient handling of various data types and supports missing data imputation. It leverages a masking mech-\nanism to enhance privacy and data utility, ensuring a balance between data realism and privacy preservation.\nTabMT’s architecture is scalable, making it suitable for diverse datasets and demonstrating improved per-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nTabMT (Gulati & Roysdon, 2023) employs a masked transformer-based architecture. The design allows\nefficient handling of various data types and supports missing data imputation. It leverages a masking mech-\nanism to enhance privacy and data utility, ensuring a balance between data realism and privacy preservation.\nTabMT’s architecture is scalable, making it suitable for diverse datasets and demonstrating improved per-"
        }
    },
    {
        "id": "7b6681abd460fd676cf7836f7c5d1a39bfc420dc",
        "content": "the Association for Computational Linguistics: Human Language Technologies, pp. 512–519, 2021.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions. CoRR, abs/2311.05232, 2023a. doi: 10.48550/\nARXIV.2311.05232. URL https://doi.org/10.48550/arXiv.2311.05232.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the Association for Computational Linguistics: Human Language Technologies, pp. 512–519, 2021.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models:\nPrinciples, taxonomy, challenges, and open questions. CoRR, abs/2311.05232, 2023a. doi: 10.48550/\nARXIV.2311.05232. URL https://doi.org/10.48550/arXiv.2311.05232."
        }
    },
    {
        "id": "7d72eba27ea8ded0d3fce1f0ba805cd4625b8414",
        "content": "Text-to-Vis domains.\n3 LLMs for predictions\nSeveral studies have leveraged LLMs for prediction in tabular data. This section will delve into the existing\nmethodologies and advancements in two categories of tabular data: standard feature-based tabular data and\ntime series data. Time series prediction differs from normal feature-based tabular data since the predictive\npower heavily relies on the past. For each category, we divide it into different steps which include pre-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Text-to-Vis domains.\n3 LLMs for predictions\nSeveral studies have leveraged LLMs for prediction in tabular data. This section will delve into the existing\nmethodologies and advancements in two categories of tabular data: standard feature-based tabular data and\ntime series data. Time series prediction differs from normal feature-based tabular data since the predictive\npower heavily relies on the past. For each category, we divide it into different steps which include pre-"
        }
    },
    {
        "id": "930ef8b2d985a189acacbaefe54bc3a23e3fc4e2",
        "content": "with prior knowledge, it is reasonable to hypothesize about the synergistic or antagonistic effects between\ntraining and inference data, potentially leading to unforeseen behaviors in such scenarios (Jung & van der\nPlas, 2024). Another unexplored aspects relates to the order invariant nature of tabular data. while language\nmodels are inherently order-variant, with word order significantly impacting predictions and contextual",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "with prior knowledge, it is reasonable to hypothesize about the synergistic or antagonistic effects between\ntraining and inference data, potentially leading to unforeseen behaviors in such scenarios (Jung & van der\nPlas, 2024). Another unexplored aspects relates to the order invariant nature of tabular data. while language\nmodels are inherently order-variant, with word order significantly impacting predictions and contextual"
        }
    },
    {
        "id": "9a6e1eea09e9a5aeb71dd00164e761a48ef34dcc",
        "content": "arXiv:2210.08964, 2022.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural\nlanguage. Proc. ACM Program. Lang., 1(OOPSLA), oct 2017. doi: 10.1145/3133887. URL https:\n//doi.org/10.1145/3133887.\nJiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z. Chen, and Jian Wu. T2g-former: Organizing tabular\nfeatures into relation graphs promotes heterogeneous feature interaction, 2023.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "arXiv:2210.08964, 2022.\nNavid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural\nlanguage. Proc. ACM Program. Lang., 1(OOPSLA), oct 2017. doi: 10.1145/3133887. URL https:\n//doi.org/10.1145/3133887.\nJiahuan Yan, Jintai Chen, Yixuan Wu, Danny Z. Chen, and Jian Wu. T2g-former: Organizing tabular\nfeatures into relation graphs promotes heterogeneous feature interaction, 2023."
        }
    },
    {
        "id": "a3f0101bf2998391b41a0157b52c48a255b3fa99",
        "content": "table understanding. We delve into the essential steps required for tabular data to be ingested by LLM,\ncovering serialization, table manipulation, and prompt engineering. Additionally, we systematically compare\ndatasets, methodologies, metrics and models for each task, emphasizing the principal challenges and recent\nadvancements in understanding, inferring, and generating tabular data. We provide recommendations for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "table understanding. We delve into the essential steps required for tabular data to be ingested by LLM,\ncovering serialization, table manipulation, and prompt engineering. Additionally, we systematically compare\ndatasets, methodologies, metrics and models for each task, emphasizing the principal challenges and recent\nadvancements in understanding, inferring, and generating tabular data. We provide recommendations for"
        }
    },
    {
        "id": "a3f46644f0b9ed27eb2de16ce7c824d7946e998c",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang,\nYuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model, 2023.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang,\nYuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model, 2023.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang"
        }
    },
    {
        "id": "e79a7bb711550917fe842dc1bfd95f603bfc965f",
        "content": "parsing and code generation. In Eduardo Blanco and Wei Lu (eds.), Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pp. 7–12, Brussels, Belgium,\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2002. URL https:\n//aclanthology.org/D18-2002.\nPengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint under-\nstanding of textual and tabular data, 2020a.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "parsing and code generation. In Eduardo Blanco and Wei Lu (eds.), Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pp. 7–12, Brussels, Belgium,\nNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2002. URL https:\n//aclanthology.org/D18-2002.\nPengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint under-\nstanding of textual and tabular data, 2020a."
        }
    },
    {
        "id": "fa5ee08ee8ef785ba1698c3883aaf05fcc759724",
        "content": "base model by most papers we surveyed.\nMetric MAE is the most common metric. Another popular metric is the Continuous Ranked Probability\nScore (CRPS) as it captures distributional qualities, allowing for comparison of models that generate samples\nwithout likelihoods. CRPS is considered an improvement over MAE as it does not ignore the structures in\ndata like correlations between time steps. The Symmetric Mean Absolute Percentage Error (SMAPE)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "base model by most papers we surveyed.\nMetric MAE is the most common metric. Another popular metric is the Continuous Ranked Probability\nScore (CRPS) as it captures distributional qualities, allowing for comparison of models that generate samples\nwithout likelihoods. CRPS is considered an improvement over MAE as it does not ignore the structures in\ndata like correlations between time steps. The Symmetric Mean Absolute Percentage Error (SMAPE)"
        }
    },
    {
        "id": "12d3c578dde33b82ddacc3a57b015b6281b17c96",
        "content": "Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceed-\nings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,\nACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pp. 1470–1480. The Association for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceed-\nings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International\nJoint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,\nACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pp. 1470–1480. The Association for"
        }
    },
    {
        "id": "1dbf048cb17e7f0ee2745938456e878e1733ee9d",
        "content": "incorporates similar examples to help the LLMs understand the desired output. Sui et al. (2023b) observed\nsignificant performance drops performance, of overall accuracy decrease of 30.38% on all tasks, when changing\ntheir prompts from a 1-shot to a 0-shot setting. In terms of choosing appropriate examples, Narayan et al.\n(2022) found their manually curated examples to outperform randomly selected examples by an average of",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "incorporates similar examples to help the LLMs understand the desired output. Sui et al. (2023b) observed\nsignificant performance drops performance, of overall accuracy decrease of 30.38% on all tasks, when changing\ntheir prompts from a 1-shot to a 0-shot setting. In terms of choosing appropriate examples, Narayan et al.\n(2022) found their manually curated examples to outperform randomly selected examples by an average of"
        }
    },
    {
        "id": "1e2d822f67b325dd638d5e3f51e572246a8f46fa",
        "content": "Andrew Carroll, Cory Y McLean, and Nicholas A Furlotte. Multimodal llms for health grounded in\nindividual-specific data. In Workshop on Machine Learning for Multimodal Healthcare Data, pp. 86–102.\nSpringer, 2023.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in\nneural information processing systems, 13, 2000.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Andrew Carroll, Cory Y McLean, and Nicholas A Furlotte. Multimodal llms for health grounded in\nindividual-specific data. In Workshop on Machine Learning for Multimodal Healthcare Data, pp. 86–102.\nSpringer, 2023.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in\nneural information processing systems, 13, 2000.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor"
        }
    },
    {
        "id": "35275eaaacce30e271b1fa36ba55e142c3541540",
        "content": "Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pp. 4511–4523. Association for Computational Linguistics, 2019b. doi:\n10.18653/V1/P19-1443. URL https://doi.org/10.18653/v1/p19-1443.\n45",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August\n2, 2019, Volume 1: Long Papers, pp. 4511–4523. Association for Computational Linguistics, 2019b. doi:\n10.18653/V1/P19-1443. URL https://doi.org/10.18653/v1/p19-1443.\n45"
        }
    },
    {
        "id": "44d0dfaf83efa44e961bbb6b0af6be35ea36a34a",
        "content": "structured data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2309–2324, Online,\nJuly 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.210. URL https:\n//aclanthology.org/2020.acl-main.210.\nStefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David A. Sontag.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "structured data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2309–2324, Online,\nJuly 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.210. URL https:\n//aclanthology.org/2020.acl-main.210.\nStefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David A. Sontag."
        }
    },
    {
        "id": "44e08020663700c63df56d975ff60af37f3044c1",
        "content": "application-specific, thus we discuss it later, in Sections 3 and 5.\nFigure 4: Key techniques in using LLMs for tabular data. The dotted line indicates steps that are optional.\n2.1 Serialization\nSince LLMs are sequence-to-sequence models, in order to feed tabular data as inputs into an LLM, we have\nto convert the structured tabular data into a text format (Sui et al., 2023b; Jaitly et al., 2023).\nText-based Table 1 describes the common text-based serialization methods in the literature. A straight-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "application-specific, thus we discuss it later, in Sections 3 and 5.\nFigure 4: Key techniques in using LLMs for tabular data. The dotted line indicates steps that are optional.\n2.1 Serialization\nSince LLMs are sequence-to-sequence models, in order to feed tabular data as inputs into an LLM, we have\nto convert the structured tabular data into a text format (Sui et al., 2023b; Jaitly et al., 2023).\nText-based Table 1 describes the common text-based serialization methods in the literature. A straight-"
        }
    },
    {
        "id": "46173b6931fe33d6221b5cdb1db23ba028179f6c",
        "content": "tasks. For TEST (Sun et al., 2023a), soft prompts are used for fine-tuning. The paper evaluates models\nlike Bert, GPT-2 (Brown et al., 2020), ChatGLM (Zeng et al., 2023), and LLaMa Touvron et al. (2023a),\nusing metrics like classification accuracy and RMSE. However, the result shows that this method is not as\nefficient and accurate as training a small task-oriented model. In general, currently, LLaMa is used as the\nbase model by most papers we surveyed.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tasks. For TEST (Sun et al., 2023a), soft prompts are used for fine-tuning. The paper evaluates models\nlike Bert, GPT-2 (Brown et al., 2020), ChatGLM (Zeng et al., 2023), and LLaMa Touvron et al. (2023a),\nusing metrics like classification accuracy and RMSE. However, the result shows that this method is not as\nefficient and accurate as training a small task-oriented model. In general, currently, LLaMa is used as the\nbase model by most papers we surveyed."
        }
    },
    {
        "id": "469ce7d6ccc6051ee6ad797788071c54ffee92f6",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nrepresentation enables PLMs to accept inputs from diverse tables, thus enabling cross-table training. Also,\ndue to the lack of locality in tabular data, models need to be invariant to permutations of the columns (Ye\net al., 2023a). In this fashion, TABERT was proposed as a PLM trained on both natural language sentence\nand structured data (Yin et al., 2020a), PTab demonstrated the importance of cross-table training for an",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nrepresentation enables PLMs to accept inputs from diverse tables, thus enabling cross-table training. Also,\ndue to the lack of locality in tabular data, models need to be invariant to permutations of the columns (Ye\net al., 2023a). In this fashion, TABERT was proposed as a PLM trained on both natural language sentence\nand structured data (Yin et al., 2020a), PTab demonstrated the importance of cross-table training for an"
        }
    },
    {
        "id": "46ac6842275adf6b9c8f2e61f9c013be9d99ca57",
        "content": "mean absolute error. RMSE stands for root-mean-square error. F1 score is calculated from the precision\nand recall of the test, where the precision is the number of true positive results divided by the number of all\nsamples predicted to be positive, including those not identified correctly, and the recall is the number of true\npositive results divided by the number of all samples that should have been identified as positive. CRPS is",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "mean absolute error. RMSE stands for root-mean-square error. F1 score is calculated from the precision\nand recall of the test, where the precision is the number of true positive results divided by the number of all\nsamples predicted to be positive, including those not identified correctly, and the recall is the number of true\npositive results divided by the number of all samples that should have been identified as positive. CRPS is"
        }
    },
    {
        "id": "555a4f9304fcad648c8114e1273d62987c85194b",
        "content": "parsing for text-to-sql. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI\nConference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelli-\ngence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pp. 13067–13075. AAAI Press, 2023a.\ndoi: 10.1609/AAAI.V37I11.26535. URL https://doi.org/10.1609/aaai.v37i11.26535.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "parsing for text-to-sql. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI\nConference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of\nArtificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelli-\ngence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pp. 13067–13075. AAAI Press, 2023a.\ndoi: 10.1609/AAAI.V37I11.26535. URL https://doi.org/10.1609/aaai.v37i11.26535."
        }
    },
    {
        "id": "64f74e55260c19186d044192e87f02c49b17e7c0",
        "content": "ARXIV.2311.05232. URL https://doi.org/10.48550/arXiv.2311.05232.\nXin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling\nusing contextual embeddings, 2020.\nZezhou Huang, Pavan Kalyan Damalapati, and Eugene Wu. Data ambiguity strikes back: How documenta-\ntion improves gpt’s text-to-sql. CoRR, abs/2310.18742, 2023b. doi: 10.48550/ARXIV.2310.18742. URL\nhttps://doi.org/10.48550/arXiv.2310.18742.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ARXIV.2311.05232. URL https://doi.org/10.48550/arXiv.2311.05232.\nXin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling\nusing contextual embeddings, 2020.\nZezhou Huang, Pavan Kalyan Damalapati, and Eugene Wu. Data ambiguity strikes back: How documenta-\ntion improves gpt’s text-to-sql. CoRR, abs/2310.18742, 2023b. doi: 10.48550/ARXIV.2310.18742. URL\nhttps://doi.org/10.48550/arXiv.2310.18742."
        }
    },
    {
        "id": "6699e8259c9c8f2f331198e6836e09549f1598e7",
        "content": "of candidate sequences, then selected final response by ensembling the derived response based on plurality\nvoting.\nChen (2023) investigated the effects of both CoT and SC on QA and FV tasks. When investigating the\nexplainability of LLM’s predictions, Dinh et al. (2022) experimented with a multi-turn approach of asking\nGPT3 to explain its own prediction from the previous round, and guided the explanation response using\nCoT by adding the line “Let’s think logically. This is because”.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "of candidate sequences, then selected final response by ensembling the derived response based on plurality\nvoting.\nChen (2023) investigated the effects of both CoT and SC on QA and FV tasks. When investigating the\nexplainability of LLM’s predictions, Dinh et al. (2022) experimented with a multi-turn approach of asking\nGPT3 to explain its own prediction from the previous round, and guided the explanation response using\nCoT by adding the line “Let’s think logically. This is because”."
        }
    },
    {
        "id": "6838f048cfad5d6619f0434495923eb039b22230",
        "content": "context-aware representation learning was introduced by pretraining the model on large-scale unannotated\ncorpora using bidirectional LSTM that takes context into consideration (e.g., ELMo (Peters et al., 2018a)),\nwhich shows significant performance boost in various natural language processing (NLP) tasks (Wang et al.,\n2022a; Peters et al., 2018b). Along this line, several other Pretrained Language Models (PLM) were",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "context-aware representation learning was introduced by pretraining the model on large-scale unannotated\ncorpora using bidirectional LSTM that takes context into consideration (e.g., ELMo (Peters et al., 2018a)),\nwhich shows significant performance boost in various natural language processing (NLP) tasks (Wang et al.,\n2022a; Peters et al., 2018b). Along this line, several other Pretrained Language Models (PLM) were"
        }
    },
    {
        "id": "7f2d79d91fbd4e4a18fc17d722ee85811a8bd703",
        "content": "dimensionality (created by the one-hot encoding of categorical data).\n3. The emergent capabilities, such as step-by-step reasoning through CoT, have transformed LM from\nlanguage modeling to a more general task-solving tool. Research is needed to test the limit of LLM’s\nemergent abilities on tabular data modeling.\n1.4 Contribution\nThe key contributions of this work are as follows:\n1. A formal break down of key techniques for LLMs’ applications on tabular data We",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "dimensionality (created by the one-hot encoding of categorical data).\n3. The emergent capabilities, such as step-by-step reasoning through CoT, have transformed LM from\nlanguage modeling to a more general task-solving tool. Research is needed to test the limit of LLM’s\nemergent abilities on tabular data modeling.\n1.4 Contribution\nThe key contributions of this work are as follows:\n1. A formal break down of key techniques for LLMs’ applications on tabular data We"
        }
    },
    {
        "id": "817dd3e44945f545318e75772ec408c5a205dbaa",
        "content": "dicted Feature meaning produce year is 2017. The repair claim of the\ncar is\nDataset Summary + LLM Pro- Manikandan et al. (2023) Larger car is always more expensive. This is\ncessed Feature + Task a 2017 Land Rover. Therefore, this car repair\nclaim is (Fraudulent or Not Fraudulent):\nLatex Format of features value + Jaitly et al. (2023) Is this car repair claim fraudulent? Yes or No?\nTask\nExpert Task Understanding + Slack & Singh (2023) Identify if the car repair claim is fraudulent.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "dicted Feature meaning produce year is 2017. The repair claim of the\ncar is\nDataset Summary + LLM Pro- Manikandan et al. (2023) Larger car is always more expensive. This is\ncessed Feature + Task a 2017 Land Rover. Therefore, this car repair\nclaim is (Fraudulent or Not Fraudulent):\nLatex Format of features value + Jaitly et al. (2023) Is this car repair claim fraudulent? Yes or No?\nTask\nExpert Task Understanding + Slack & Singh (2023) Identify if the car repair claim is fraudulent."
        }
    },
    {
        "id": "850d6dea2ea6508e937bce6a0e243658eea9b688",
        "content": "et al., 2022) NLG Table Report and et al., 2023g), 50.00 (Zhao et al., 2023a)\nWikipedia\nToTTo (Parikh 120000 NLG Table Sentence Wikipedia BLEU: 53.21 (Sui et al., 2023c), 20.77\net al., 2020a) (Zhang et al., 2023g)\nFEVEROUS 28800 Classifi- Claim, Label Wikipedia Exact Match Accuracy: 77.22 (Chen,\nAly et al. (2021) cation Table 2023), 73.77 (Zhang et al., 2023g), 66.51\n(Sui et al., 2023c)\nTabFact (Chen 16573 NLI Table, Label Wikipedia Exact Match Accuracy: 93.00 (Ye",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2022) NLG Table Report and et al., 2023g), 50.00 (Zhao et al., 2023a)\nWikipedia\nToTTo (Parikh 120000 NLG Table Sentence Wikipedia BLEU: 53.21 (Sui et al., 2023c), 20.77\net al., 2020a) (Zhang et al., 2023g)\nFEVEROUS 28800 Classifi- Claim, Label Wikipedia Exact Match Accuracy: 77.22 (Chen,\nAly et al. (2021) cation Table 2023), 73.77 (Zhang et al., 2023g), 66.51\n(Sui et al., 2023c)\nTabFact (Chen 16573 NLI Table, Label Wikipedia Exact Match Accuracy: 93.00 (Ye"
        }
    },
    {
        "id": "9505e9884c5ec0cf78f16dfe4c3130e6429899c5",
        "content": "Dragomir Radev. RobuT: A systematic study of table QA robustness against human-annotated ad-\nversarial perturbations. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceed-\nings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers), pp. 6064–6081, Toronto, Canada, July 2023e. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-long.334. URL https://aclanthology.org/2023.acl-long.334.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Dragomir Radev. RobuT: A systematic study of table QA robustness against human-annotated ad-\nversarial perturbations. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceed-\nings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers), pp. 6064–6081, Toronto, Canada, July 2023e. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-long.334. URL https://aclanthology.org/2023.acl-long.334."
        }
    },
    {
        "id": "cd386355b2135863c368984bb25d187c41edf244",
        "content": "inference (NLI), Text2SQL tasks, and more. Many earlier methods fine-tune BERT (Devlin et al., 2019)\nto become table encoders for table-related tasks, like TAPAS (Herzig et al., 2020), TABERT (Yin et al.,\n2020a), TURL (Deng et al., 2022a), TUTA (Wang et al., 2021) and TABBIE (Iida et al., 2021). For\nexample, TAPAS extended BERT’s masked language model objective to structured data by incorporating\nadditional embeddings designed to capture tabular structure. It also integrates two classification layers",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "inference (NLI), Text2SQL tasks, and more. Many earlier methods fine-tune BERT (Devlin et al., 2019)\nto become table encoders for table-related tasks, like TAPAS (Herzig et al., 2020), TABERT (Yin et al.,\n2020a), TURL (Deng et al., 2022a), TUTA (Wang et al., 2021) and TABBIE (Iida et al., 2021). For\nexample, TAPAS extended BERT’s masked language model objective to structured data by incorporating\nadditional embeddings designed to capture tabular structure. It also integrates two classification layers"
        }
    },
    {
        "id": "e95451f54092d46795e96f443cd8c1fe5e44e4a2",
        "content": "Model grafting The performance of LLM for tabular data modeling could be improved through model\ngrafting. Model grafting involves mapping non-text data into the same token embedding space as text using\nspecialized encoders, as exemplified by the HeLM model (Belyaeva et al., 2023), which integrates spirogram\nsequences and demographic data with text tokens. This approach is efficient and allows integration with high-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Model grafting The performance of LLM for tabular data modeling could be improved through model\ngrafting. Model grafting involves mapping non-text data into the same token embedding space as text using\nspecialized encoders, as exemplified by the HeLM model (Belyaeva et al., 2023), which integrates spirogram\nsequences and demographic data with text tokens. This approach is efficient and allows integration with high-"
        }
    },
    {
        "id": "ef02253e56343b0c400f77951281bff9db479b2f",
        "content": "Lehner. Building the dresden web table corpus: A classification approach. In Ioan Raicu, Omer F.\nRana, and Rajkumar Buyya (eds.), 2nd IEEE/ACM International Symposium on Big Data Computing,\nBDC 2015, Limassol, Cyprus, December 7-10, 2015, pp. 41–50. IEEE Computer Society, 2015. doi:\n10.1109/BDC.2015.30. URL https://doi.org/10.1109/BDC.2015.30.\nRaul Castro Fernandez, Aaron J. Elmore, Michael J. Franklin, Sanjay Krishnan, and Chenhao Tan. How",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Lehner. Building the dresden web table corpus: A classification approach. In Ioan Raicu, Omer F.\nRana, and Rajkumar Buyya (eds.), 2nd IEEE/ACM International Symposium on Big Data Computing,\nBDC 2015, Limassol, Cyprus, December 7-10, 2015, pp. 41–50. IEEE Computer Society, 2015. doi:\n10.1109/BDC.2015.30. URL https://doi.org/10.1109/BDC.2015.30.\nRaul Castro Fernandez, Aaron J. Elmore, Michael J. Franklin, Sanjay Krishnan, and Chenhao Tan. How"
        }
    },
    {
        "id": "01d651d0b502f757ec9535bbbbb285aab0b7525d",
        "content": "expressions, (4) masking correlated columns (E.g. “Ranking” and “Total Points” can be inferred from one another), and (5)\nintroducing new columns that are derived from existing columns. For the question itself, they perturbed questions at the\nword-level or sentence-level.\n11",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "expressions, (4) masking correlated columns (E.g. “Ranking” and “Total Points” can be inferred from one another), and (5)\nintroducing new columns that are derived from existing columns. For the question itself, they perturbed questions at the\nword-level or sentence-level.\n11"
        }
    },
    {
        "id": "17269f575e3ef6ae1d29459729643eabb7876dbf",
        "content": "demonstrates the ability to comprehend and generate human language text (Liu et al., 2023g).1\nBefore LLMs, researchers have been investigating ways to integrate tabular data with neural network for\nNLP and data management tasks (Badaro et al., 2023). Today, researchers are keen to investigate the\nabilities of LLMs when working with tabular data for various tasks, such as prediction, table understanding,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "demonstrates the ability to comprehend and generate human language text (Liu et al., 2023g).1\nBefore LLMs, researchers have been investigating ways to integrate tabular data with neural network for\nNLP and data management tasks (Badaro et al., 2023). Today, researchers are keen to investigate the\nabilities of LLMs when working with tabular data for various tasks, such as prediction, table understanding,"
        }
    },
    {
        "id": "2f6bfb6bf9406df9b5e091e04bd4bddcb7133916",
        "content": "arXiv.2312.15395.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances\nin Neural Information Processing Systems, 35:1950–1965, 2022b.\n37",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "arXiv.2312.15395.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances\nin Neural Information Processing Systems, 35:1950–1965, 2022b.\n37"
        }
    },
    {
        "id": "467c5ea4b726ee9b2c1552d26566c107f980bd4e",
        "content": "for real-world tabular data, 2021.\nElita Lobo, Oktie Hassanzadeh, Nhan Pham, Nandana Mihindukulasooriya, Dharmashankar Subramanian,\nand Horst Samulowitz. Matching table metadata with business glossaries using large language models,\n2023.\nHui Luan and Chin-Chung Tsai. A review of using machine learning approaches for precision education.\nEducational Technology & Society, 24(1):250–266, 2021.\nHaoran Luo, Fan Cheng, Heng Yu, and Yuqi Yi. Sdtr: Soft decision tree regressor for tabular data. IEEE",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "for real-world tabular data, 2021.\nElita Lobo, Oktie Hassanzadeh, Nhan Pham, Nandana Mihindukulasooriya, Dharmashankar Subramanian,\nand Horst Samulowitz. Matching table metadata with business glossaries using large language models,\n2023.\nHui Luan and Chin-Chung Tsai. A review of using machine learning approaches for precision education.\nEducational Technology & Society, 24(1):250–266, 2021.\nHaoran Luo, Fan Cheng, Heng Yu, and Yuqi Yi. Sdtr: Soft decision tree regressor for tabular data. IEEE"
        }
    },
    {
        "id": "47e3f37926df1be3b411568cace66393c08d6894",
        "content": "and Dongmei Zhang. HiTab: A hierarchical table dataset for question answering and natural language\ngeneration. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1094–1110,\nDublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.78.\nURL https://aclanthology.org/2022.acl-long.78.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and Dongmei Zhang. HiTab: A hierarchical table dataset for question answering and natural language\ngeneration. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1094–1110,\nDublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.78.\nURL https://aclanthology.org/2022.acl-long.78."
        }
    },
    {
        "id": "5122d7757e3de60fd025df9c0606b267f497270e",
        "content": "buckets: (1) to decompose a challenging task into manageable sub-tasks, (2) to update the model outputs\nbased on new user inputs, and (3) to work-around specific constraints or to resolve errors.\nIntermediate, sub-tasks This section overlaps with concepts around CoT and SC discussed earlier in\nSection 2.3. In a nutshell, since the reasoning task might be complex, LLMs might require guidance to\ndecompose the task into manageable sub-tasks. For example, to improve downstream tabular reasoning, Sui",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "buckets: (1) to decompose a challenging task into manageable sub-tasks, (2) to update the model outputs\nbased on new user inputs, and (3) to work-around specific constraints or to resolve errors.\nIntermediate, sub-tasks This section overlaps with concepts around CoT and SC discussed earlier in\nSection 2.3. In a nutshell, since the reasoning task might be complex, LLMs might require guidance to\ndecompose the task into manageable sub-tasks. For example, to improve downstream tabular reasoning, Sui"
        }
    },
    {
        "id": "59aae8bbb098c213e0e1f1466802589e74ca7841",
        "content": "tences using templates (2023); Gong et al. (2020); Dinh et al.\n(2022); Jaitly et al. (2023)\nTable 1: Text-based serialization methods.\n2020b), TURL (Deng et al., 2022a), TUTA (Wang et al., 2021), TABBIE (Iida et al., 2021) and UTP (Chen\net al., 2023b). Cong et al. (2023) discuss the pros and cons of the learned table representations of a few of\nthese encoders. For LLMs with >1B parameters, there are UniTabPT (Sarkar & Lausen, 2023) with 3B",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tences using templates (2023); Gong et al. (2020); Dinh et al.\n(2022); Jaitly et al. (2023)\nTable 1: Text-based serialization methods.\n2020b), TURL (Deng et al., 2022a), TUTA (Wang et al., 2021), TABBIE (Iida et al., 2021) and UTP (Chen\net al., 2023b). Cong et al. (2023) discuss the pros and cons of the learned table representations of a few of\nthese encoders. For LLMs with >1B parameters, there are UniTabPT (Sarkar & Lausen, 2023) with 3B"
        }
    },
    {
        "id": "5c0571e74fd24a3de46b9ab5f8bcd655c2171d5c",
        "content": "Shuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot, single-\ndomain, and cross-domain settings. CoRR, abs/2305.11853, 2023. doi: 10.48550/ARXIV.2305.11853. URL\nhttps://doi.org/10.48550/arXiv.2305.11853.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Shuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot, single-\ndomain, and cross-domain settings. CoRR, abs/2305.11853, 2023. doi: 10.48550/ARXIV.2305.11853. URL\nhttps://doi.org/10.48550/arXiv.2305.11853.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie."
        }
    },
    {
        "id": "800ffe3e3d99e9558d8464bb8d17292eac2c5061",
        "content": "Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\nA survey on evaluation of large language models. ACM Trans. Intell. Syst. Technol., jan 2024. ISSN\n2157-6904. doi: 10.1145/3641289. URL https://doi.org/10.1145/3641289.\nJintai Chen, Kuanlun Liao, Yao Wan, Danny Z. Chen, and Jian Wu. Danets: Deep abstract networks for\ntabular data classification and regression, 2022a.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie.\nA survey on evaluation of large language models. ACM Trans. Intell. Syst. Technol., jan 2024. ISSN\n2157-6904. doi: 10.1145/3641289. URL https://doi.org/10.1145/3641289.\nJintai Chen, Kuanlun Liao, Yao Wan, Danny Z. Chen, and Jian Wu. Danets: Deep abstract networks for\ntabular data classification and regression, 2022a."
        }
    },
    {
        "id": "8eaec3e38a17922f36b5929e8fcd11fc1468f657",
        "content": "processing, 1997.\nNabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated llm: Synergy of llms\nand data curation for tabular augmentation in ultra low-data regimes. arXiv preprint arXiv:2312.12112,\n2023.\nDeven Santosh Shah, H Andrew Schwartz, and Dirk Hovy. Predictive biases in natural language process-\ning models: A conceptual framework and overview. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 5248–5264, 2020.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "processing, 1997.\nNabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated llm: Synergy of llms\nand data curation for tabular augmentation in ultra low-data regimes. arXiv preprint arXiv:2312.12112,\n2023.\nDeven Santosh Shah, H Andrew Schwartz, and Dirk Hovy. Predictive biases in natural language process-\ning models: A conceptual framework and overview. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 5248–5264, 2020."
        }
    },
    {
        "id": "a4bb5561af95fc730c408907dc12c612bb1476b4",
        "content": "Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao\nChen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S. Lasecki, and\nDragomir R. Radev. Cosql: A conversational text-to-sql challenge towards cross-domain natural language\ninterfaces to databases. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao\nChen, Yuwen Zhang, Shreya Dixit, Vincent Zhang, Caiming Xiong, Richard Socher, Walter S. Lasecki, and\nDragomir R. Radev. Cosql: A conversational text-to-sql challenge towards cross-domain natural language\ninterfaces to databases. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International"
        }
    },
    {
        "id": "af3d416d4cf07e54f344453f063c4de30c88bb02",
        "content": "7Here is the Github repo for TABLET https://Github.com/dylan-slack/Tablet, TabLLM https://Github.com/\nclinicalml/TabLLM and LIFT https://Github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning\n14",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "7Here is the Github repo for TABLET https://Github.com/dylan-slack/Tablet, TabLLM https://Github.com/\nclinicalml/TabLLM and LIFT https://Github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning\n14"
        }
    },
    {
        "id": "ca7548450b56fda4df2f39778874fc865b1dca84",
        "content": "Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, Zihao Wu, Lin Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen,\nTianming Liu, and Bao Ge. Summary of chatgpt-related research and perspective towards the future of\nlarge language models. Meta-Radiology, 1(2):100017, September 2023g. ISSN 2950-1628. doi: 10.1016/\nj.metrad.2023.100017. URL http://dx.doi.org/10.1016/j.metrad.2023.100017.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li,\nMengshen He, Zhengliang Liu, Zihao Wu, Lin Zhao, Dajiang Zhu, Xiang Li, Ning Qiang, Dingang Shen,\nTianming Liu, and Bao Ge. Summary of chatgpt-related research and perspective towards the future of\nlarge language models. Meta-Radiology, 1(2):100017, September 2023g. ISSN 2950-1628. doi: 10.1016/\nj.metrad.2023.100017. URL http://dx.doi.org/10.1016/j.metrad.2023.100017."
        }
    },
    {
        "id": "e3f6a5ac22d99c8b8832e6261577b0f0ef2e624c",
        "content": "tions in the prompt about the person the LLM should portray as it completes a task. For example, Zhao\net al. (2023a) experimented with the prompt “Suppose you are an expert in statistical analysis.”.\n2.4 End-to-end systems\nSince LLMs can generate any text-based output, apart from generating human-readable responses, it could\nalso generate code readable by other programs. Abraham et al. (2022) designed a model that converts",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tions in the prompt about the person the LLM should portray as it completes a task. For example, Zhao\net al. (2023a) experimented with the prompt “Suppose you are an expert in statistical analysis.”.\n2.4 End-to-end systems\nSince LLMs can generate any text-based output, apart from generating human-readable responses, it could\nalso generate code readable by other programs. Abraham et al. (2022) designed a model that converts"
        }
    },
    {
        "id": "ec7a3e981910029ab7d71dbe6abedc8e91469329",
        "content": "Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, and Xiaobo Zhang.\nLarge language models are complex table parsers. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023, pp. 14786–14802. Association for Computational Linguistics, 2023a. URL\nhttps://aclanthology.org/2023.emnlp-main.914.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Bowen Zhao, Changkai Ji, Yuejie Zhang, Wen He, Yingwen Wang, Qing Wang, Rui Feng, and Xiaobo Zhang.\nLarge language models are complex table parsers. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023, pp. 14786–14802. Association for Computational Linguistics, 2023a. URL\nhttps://aclanthology.org/2023.emnlp-main.914."
        }
    },
    {
        "id": "f36b6a0d5b60649ab781e866677616cf4db8893f",
        "content": "such as the Merck Manual. It includes instructions, examples, and test data points. They found that\nthese instructions significantly enhance zero-shot F1 performance. However, experiments from TABLET\nrevealed that LLMs tend to ignore instructions, even with examples, leading to prediction failures. Along\n5GitHub repository link https://Github.com/clinicalml/TabLLM/tree/main/datasets\n6Link to the pre-trained data https://Github.com/Kaggle/kaggle-api\n13",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "such as the Merck Manual. It includes instructions, examples, and test data points. They found that\nthese instructions significantly enhance zero-shot F1 performance. However, experiments from TABLET\nrevealed that LLMs tend to ignore instructions, even with examples, leading to prediction failures. Along\n5GitHub repository link https://Github.com/clinicalml/TabLLM/tree/main/datasets\n6Link to the pre-trained data https://Github.com/Kaggle/kaggle-api\n13"
        }
    },
    {
        "id": "f5ed4d22ca536c1d883fa33f455c9e35c6e912e9",
        "content": "(Sui et al., 2023c)\nTabFact (Chen 16573 NLI Table, Label Wikipedia Exact Match Accuracy: 93.00 (Ye\net al., 2020a) State- et al., 2023b), 90.71 (Sarkar & Lausen,\nment 2023), 87.60 (Jiang et al., 2023), 82.55\n(Zhang et al., 2023g), 78.80 (Chen,\n2023), 62.67 (Sui et al., 2023c)\nSpider (Yu 1020 Text2- Table, SQL Human an- Execution Accuracy: 87.60 (Li et al.,\net al., 2018b) SQL Question notation 2024), 86.60 (Gao et al., 2024), 85.30\n(Pourreza & Rafiei, 2023), 82.30 (Dong",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(Sui et al., 2023c)\nTabFact (Chen 16573 NLI Table, Label Wikipedia Exact Match Accuracy: 93.00 (Ye\net al., 2020a) State- et al., 2023b), 90.71 (Sarkar & Lausen,\nment 2023), 87.60 (Jiang et al., 2023), 82.55\n(Zhang et al., 2023g), 78.80 (Chen,\n2023), 62.67 (Sui et al., 2023c)\nSpider (Yu 1020 Text2- Table, SQL Human an- Execution Accuracy: 87.60 (Li et al.,\net al., 2018b) SQL Question notation 2024), 86.60 (Gao et al., 2024), 85.30\n(Pourreza & Rafiei, 2023), 82.30 (Dong"
        }
    },
    {
        "id": "048259cb0c5e2a538331d2d75feb87878fefad50",
        "content": "problem in tabular data modeling, how to mitigate hallucinations, how to find better representations\nof numerical data, how to improve capacity, how to form standard benchmarks, how to improve model\ninterpretability, how to create an integrated workflow, how to design better fine-tuning strategies\nand finally, how to improve the performance of downstream applications.\n2 Key techniques for LLMs’ applications on tabular data",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "problem in tabular data modeling, how to mitigate hallucinations, how to find better representations\nof numerical data, how to improve capacity, how to form standard benchmarks, how to improve model\ninterpretability, how to create an integrated workflow, how to design better fine-tuning strategies\nand finally, how to improve the performance of downstream applications.\n2 Key techniques for LLMs’ applications on tabular data"
        }
    },
    {
        "id": "174184e07b134b088d071e23d0632763efa7d90c",
        "content": "consolidation, enrichment, and refinement, have the potential to streamline user experience. These methods\nstill require extensive preprocessing, which makes it hard to inference. The development of a unified pipeline\nthat incorporates these models, along with auto data prepossessing and serialization to established platforms\nsuch as Hugging Face, warrants further exploration.\nFine-tuning strategy design Designing appropriate tasks and learning strategies for LLMs is extensively",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "consolidation, enrichment, and refinement, have the potential to streamline user experience. These methods\nstill require extensive preprocessing, which makes it hard to inference. The development of a unified pipeline\nthat incorporates these models, along with auto data prepossessing and serialization to established platforms\nsuch as Hugging Face, warrants further exploration.\nFine-tuning strategy design Designing appropriate tasks and learning strategies for LLMs is extensively"
        }
    },
    {
        "id": "2087a0905a083614b42d0eed2df394dec19a93e7",
        "content": "Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/\nhash/b1efde53be364a73914f58805a001731-Abstract-Conference.html.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,\nUSA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/\nhash/b1efde53be364a73914f58805a001731-Abstract-Conference.html.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-"
        }
    },
    {
        "id": "24f41d8e81c99aa549cba6f8e45722d52b036a07",
        "content": "cardinality categorical features (Borisov et al., 2022a).\n2. Sparsity: Real-world applications, such as clinical trials, epidemiological research, fraud detection,\netc., often deal with imbalanced class labels and missing values, which results in long-tailed distri-\nbution in the training samples (Sauber-Cole & Khoshgoftaar, 2022).\n3. Dependency on pre-processing: Data pre-processing is crucial and application-dependent when work-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "cardinality categorical features (Borisov et al., 2022a).\n2. Sparsity: Real-world applications, such as clinical trials, epidemiological research, fraud detection,\netc., often deal with imbalanced class labels and missing values, which results in long-tailed distri-\nbution in the training samples (Sauber-Cole & Khoshgoftaar, 2022).\n3. Dependency on pre-processing: Data pre-processing is crucial and application-dependent when work-"
        }
    },
    {
        "id": "2df738cc378b24496450a3197f5b24700de61cb4",
        "content": "in training, ease of tuning, and ease of interpretation. However, they have limitations compared to deep\nlearning models: 1. Tree-based models can be sensitive to feature engineering especially with categorical\nfeatures while deep learning can learn representation implicitly during training (Goodfellow et al., 2016).\n2. Tree-based models are not naturally suited for processing sequential data, such as time series while deep",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "in training, ease of tuning, and ease of interpretation. However, they have limitations compared to deep\nlearning models: 1. Tree-based models can be sensitive to feature engineering especially with categorical\nfeatures while deep learning can learn representation implicitly during training (Goodfellow et al., 2016).\n2. Tree-based models are not naturally suited for processing sequential data, such as time series while deep"
        }
    },
    {
        "id": "33485beba55ef256e1310fe96b866f9ad939d867",
        "content": "et al., 2018b) SQL Question notation 2024), 86.60 (Gao et al., 2024), 85.30\n(Pourreza & Rafiei, 2023), 82.30 (Dong\net al., 2023), 79.90 (Li et al., 2023a),\n78.00 (Rai et al., 2023), 77.80 (Jiang\net al., 2023), 77.60 (Li et al., 2023b),\n76.80 (Chang & Fosler-Lussier, 2023)\nWikiSQL 24241 Text2- Table, SQL, An- Human An- Execution Accuracy: 90.86 (Sarkar &\n(Zhong et al., SQL Question swer notated Lausen, 2023), 65.60 (Jiang et al.,\n2017b) 2023), 50.48 (Zhang et al., 2023g)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2018b) SQL Question notation 2024), 86.60 (Gao et al., 2024), 85.30\n(Pourreza & Rafiei, 2023), 82.30 (Dong\net al., 2023), 79.90 (Li et al., 2023a),\n78.00 (Rai et al., 2023), 77.80 (Jiang\net al., 2023), 77.60 (Li et al., 2023b),\n76.80 (Chang & Fosler-Lussier, 2023)\nWikiSQL 24241 Text2- Table, SQL, An- Human An- Execution Accuracy: 90.86 (Sarkar &\n(Zhong et al., SQL Question swer notated Lausen, 2023), 65.60 (Jiang et al.,\n2017b) 2023), 50.48 (Zhang et al., 2023g)"
        }
    },
    {
        "id": "4b80e601a25058c933cd6afd7ab0552bba0bf33c",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Evaluating and enhancing structural\nunderstanding capabilities of large language models on tables via input designs. CoRR, abs/2305.13062,\n2023a. doi: 10.48550/ARXIV.2305.13062. URL https://doi.org/10.48550/arXiv.2305.13062.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Gpt4table: Can large language",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Evaluating and enhancing structural\nunderstanding capabilities of large language models on tables via input designs. CoRR, abs/2305.13062,\n2023a. doi: 10.48550/ARXIV.2305.13062. URL https://doi.org/10.48550/arXiv.2305.13062.\nYuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, and Dongmei Zhang. Gpt4table: Can large language"
        }
    },
    {
        "id": "56f47f7ec89391a4b97c13b0132fbf7ce044e7df",
        "content": "decompose a task by performing step-by-step thinking, resulting in better reasoning. Program-of-Thoughts\n(Chen et al., 2022b) guides the LLMs using code-related comments like “Let’s write a program step-by-step...”.\nZhao et al. (2023d) explored CoT and PoT strategies for the numerical QA task. Yang et al. (2023) prompt\nthe LLMs with one shot CoT demonstration example to generate a reasoning and answer. Subsequently,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "decompose a task by performing step-by-step thinking, resulting in better reasoning. Program-of-Thoughts\n(Chen et al., 2022b) guides the LLMs using code-related comments like “Let’s write a program step-by-step...”.\nZhao et al. (2023d) explored CoT and PoT strategies for the numerical QA task. Yang et al. (2023) prompt\nthe LLMs with one shot CoT demonstration example to generate a reasoning and answer. Subsequently,"
        }
    },
    {
        "id": "693e0a4f4a2dd1cd627ea307a3e50be95fe08e7f",
        "content": "Singh, 2023) fits a simple model such as a one-layer rule set model or prototype with the 10 most important\nfeatures on the task’s full training data. It then serializes the logic into text using a template and revises the\ntemplates using GPT3. Based on their experiments, it was found that contrary to the naturally occurring\ninstructions, LLM-generated instructions do not significantly improve performance.\nTarget Augmentation LLMs can solve complex tasks through text generation, however, the output is not",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Singh, 2023) fits a simple model such as a one-layer rule set model or prototype with the 10 most important\nfeatures on the task’s full training data. It then serializes the logic into text using a template and revises the\ntemplates using GPT3. Based on their experiments, it was found that contrary to the naturally occurring\ninstructions, LLM-generated instructions do not significantly improve performance.\nTarget Augmentation LLMs can solve complex tasks through text generation, however, the output is not"
        }
    },
    {
        "id": "773176cda03ca33c19ca87596377023f59a1451a",
        "content": "datasets with varied training set sizes (from 2k - 140k) and feature sizes (from 9 - 120) 10.\nRecommendation Prediction CTRL (Li et al., 2023e) proposes a novel method for Click Through Rate\n(CTR) prediction by converting tabular data into text using human-designed prompts, making it understand-\nable for language models. The model treats tabular data and generated textual data as separate modalities,\nfeeding them into a collaborative CTR model and a pre-trained language model such as ChatGLM (Zeng",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "datasets with varied training set sizes (from 2k - 140k) and feature sizes (from 9 - 120) 10.\nRecommendation Prediction CTRL (Li et al., 2023e) proposes a novel method for Click Through Rate\n(CTR) prediction by converting tabular data into text using human-designed prompts, making it understand-\nable for language models. The model treats tabular data and generated textual data as separate modalities,\nfeeding them into a collaborative CTR model and a pre-trained language model such as ChatGLM (Zeng"
        }
    },
    {
        "id": "b65c714ff1b0ab31c2fb1a504b99bc4204d1799c",
        "content": "from scientific articles. TranX (Yin & Neubig, 2018) investigates abstract syntax description language for\nthe target representations, enabling high accuracy and generalizability across different types of meaning\nrepresentations.20.\nPretraining For pretraining on large datasets for table understanding, we recommend to use TaBERT (Yin\net al., 2020b) and TAPAS (Herzig et al., 2020). Dataset in Tapas has 6.2 million tables and is useful for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "from scientific articles. TranX (Yin & Neubig, 2018) investigates abstract syntax description language for\nthe target representations, enabling high accuracy and generalizability across different types of meaning\nrepresentations.20.\nPretraining For pretraining on large datasets for table understanding, we recommend to use TaBERT (Yin\net al., 2020b) and TAPAS (Herzig et al., 2020). Dataset in Tapas has 6.2 million tables and is useful for"
        }
    },
    {
        "id": "ba67b1f1a5956eafd761992e9096497b34a6b0b3",
        "content": "to LLM for tabular data to improve the encoding of non-text data such as categorical and numerical feature.\n7 Conclusion\nThis survey represents the first comprehensive investigation into the utilization of LLMs for modeling het-\nerogeneous tabular data across various tasks, including prediction, data synthesis, question answering and\ntable understanding. We delve into the essential steps required for tabular data to be ingested by LLM,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to LLM for tabular data to improve the encoding of non-text data such as categorical and numerical feature.\n7 Conclusion\nThis survey represents the first comprehensive investigation into the utilization of LLMs for modeling het-\nerogeneous tabular data across various tasks, including prediction, data synthesis, question answering and\ntable understanding. We delve into the essential steps required for tabular data to be ingested by LLM,"
        }
    },
    {
        "id": "be3dc7f2cd03a13b16d6a2fc4ae5680c01e80614",
        "content": "cation prediction and RMSE is mostly commonly used metric for regression 3\n3.3 Time Series Forecasting\nCompared to prediction on feature-based tabular data with numerical and categorical features, time series\nprediction pays more attention to numerical features and temporal relations. Thus, serialization and target\naugmentation are more relevant to how to best represent numerical features. Many papers have claimed\n15",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "cation prediction and RMSE is mostly commonly used metric for regression 3\n3.3 Time Series Forecasting\nCompared to prediction on feature-based tabular data with numerical and categorical features, time series\nprediction pays more attention to numerical features and temporal relations. Thus, serialization and target\naugmentation are more relevant to how to best represent numerical features. Many papers have claimed\n15"
        }
    },
    {
        "id": "c514335bf3cae025335e7b887ba55f9cf21a6302",
        "content": "each row represented as a “47”}}\ndictionary of keys (column\nnames) and values\nData Ma- Dataframe as a list of lists, [[‘’,‘name’,‘age’] Singha et al. (2023)\ntrix where the firm item is the col- [0, ‘helen’, 47]]\numn header\nMarkdown Rows are line-separated, | | name | age | Singha et al. (2023); Liu et al. (2023e);\ncolumns are separated by “|” |:–-|:–––––|–––-:| Zhang et al. (2023d); Ye et al. (2023b);\n2 |0 |helen | 47| Zhao et al. (2023d); Sui et al. (2023b)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "each row represented as a “47”}}\ndictionary of keys (column\nnames) and values\nData Ma- Dataframe as a list of lists, [[‘’,‘name’,‘age’] Singha et al. (2023)\ntrix where the firm item is the col- [0, ‘helen’, 47]]\numn header\nMarkdown Rows are line-separated, | | name | age | Singha et al. (2023); Liu et al. (2023e);\ncolumns are separated by “|” |:–-|:–––––|–––-:| Zhang et al. (2023d); Ye et al. (2023b);\n2 |0 |helen | 47| Zhao et al. (2023d); Sui et al. (2023b)"
        }
    },
    {
        "id": "c807673c3cb071aa859a24dba2706910c758ec28",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nTable and Conversation QA HybriDialogue (Nakamura et al., 2022) includes conversations grounded\non both Wikipedia text and tables. This addresses a significant challenge in current dialogue systems:\nconversing on topics with information distributed across different modalities, specifically text and tables. 15\nTable Classification FEVEROUS (Aly et al., 2021) focuses on both unstructured text and structured",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nTable and Conversation QA HybriDialogue (Nakamura et al., 2022) includes conversations grounded\non both Wikipedia text and tables. This addresses a significant challenge in current dialogue systems:\nconversing on topics with information distributed across different modalities, specifically text and tables. 15\nTable Classification FEVEROUS (Aly et al., 2021) focuses on both unstructured text and structured"
        }
    },
    {
        "id": "cfceb3fd8b49bb5a5b46ef53ddf27aeac20cbf0c",
        "content": "of synthetic data, 3) privacy preservation – DCR score, representing the median Distance to the Closest\nRecord (DCR), to evaluate the privacy level of the original data, (Note: the similarity-based DCR score\nprovides an average metric for the system but does not offer information about individual privacy guaran-\ntees (Ganev & De Cristofaro, 2023)) and 4) Performance on downstream tasks – like machine learning",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "of synthetic data, 3) privacy preservation – DCR score, representing the median Distance to the Closest\nRecord (DCR), to evaluate the privacy level of the original data, (Note: the similarity-based DCR score\nprovides an average metric for the system but does not offer information about individual privacy guaran-\ntees (Ganev & De Cristofaro, 2023)) and 4) Performance on downstream tasks – like machine learning"
        }
    },
    {
        "id": "de93b3431d58b0c09a02f4eaf1d3b2f483b31fee",
        "content": "LLM based method for tabular data.\nModel interpretability Like many deep learning algorithms, LLMs suffer from a lack of interpretability.\nFor LLM based method in tabular data, only a few systems expose a justification of their model output, such\nas TabLLM (Hegselmann et al., 2023). One direction is to use the Shapley values to derive interpretations.\nShapley values have been used to evaluate the prompt for LLMs (Liu et al., 2023a). It could also be useful",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "LLM based method for tabular data.\nModel interpretability Like many deep learning algorithms, LLMs suffer from a lack of interpretability.\nFor LLM based method in tabular data, only a few systems expose a justification of their model output, such\nas TabLLM (Hegselmann et al., 2023). One direction is to use the Shapley values to derive interpretations.\nShapley values have been used to evaluate the prompt for LLMs (Liu et al., 2023a). It could also be useful"
        }
    },
    {
        "id": "f0bb49fd4cbea045e35108ceea42dcb21d24d20f",
        "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer,\n2023.\nDaking Rai, Bailin Wang, Yilun Zhou, and Ziyu Yao. Improving generalization in language model-based\ntext-to-sql semantic parsing: Two simple semantic boundary-based techniques. In Anna Rogers, Jordan L.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer,\n2023.\nDaking Rai, Bailin Wang, Yilun Zhou, and Ziyu Yao. Improving generalization in language model-based\ntext-to-sql semantic parsing: Two simple semantic boundary-based techniques. In Anna Rogers, Jordan L."
        }
    },
    {
        "id": "01a5c5ee10c397bbd014d069e3c5b20439020d32",
        "content": "//doi.org/10.48550/arXiv.2312.01678.\nHengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos,\nHuzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion in\nlatent space. arXiv preprint arXiv:2310.09656, 2023c.\nHengyuan Zhang, Peng Chang, and Zongcheng Ji. Bridging the gap: Deciphering tabular data using large\nlanguage model. CoRR, abs/2308.11891, 2023d. doi: 10.48550/ARXIV.2308.11891. URL https://",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "//doi.org/10.48550/arXiv.2312.01678.\nHengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos,\nHuzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion in\nlatent space. arXiv preprint arXiv:2310.09656, 2023c.\nHengyuan Zhang, Peng Chang, and Zongcheng Ji. Bridging the gap: Deciphering tabular data using large\nlanguage model. CoRR, abs/2308.11891, 2023d. doi: 10.48550/ARXIV.2308.11891. URL https://"
        }
    },
    {
        "id": "0411b9f324bb45c1f8976ea82932c510afe7865d",
        "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.),\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Min-\nneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.),\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Min-\nneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423."
        }
    },
    {
        "id": "07696ff1d9b84bd4fb134858c2b637138d2d10e3",
        "content": "distance metric (E.g. Euclidean or negative cosine similarity) of the question and example embedding, and\nkNN algorithm to select k closest examples from Q; (3) masked question similarity selection: similar to\n(2), but beforehand masking domain-specific information (the table names, column names and values) in\nthe question; (4) query similarity selection: select k examples similar to target SQL query s∗, which relies",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "distance metric (E.g. Euclidean or negative cosine similarity) of the question and example embedding, and\nkNN algorithm to select k closest examples from Q; (3) masked question similarity selection: similar to\n(2), but beforehand masking domain-specific information (the table names, column names and values) in\nthe question; (4) query similarity selection: select k examples similar to target SQL query s∗, which relies"
        }
    },
    {
        "id": "11e6f9319c699a7cc2b00d61e119f73f0b1d6504",
        "content": "Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya"
        }
    },
    {
        "id": "226c5248b8452c26b1bc369a2e636e82683be6de",
        "content": "Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sam-\npat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta\nBaral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+ nlp tasks, 2022b.\nZhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. TUTA: tree-based",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sam-\npat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta\nBaral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+ nlp tasks, 2022b.\nZhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. TUTA: tree-based"
        }
    },
    {
        "id": "2271f7ff05a7f754f47606deab047eb790e3c17b",
        "content": "representation learning. SIGMOD Rec., 51(1):33–40, 2022a. doi: 10.1145/3542700.3542709. URL\nhttps://doi.org/10.1145/3542700.3542709.\nYang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam, and Tat-Seng Chua. PACIFIC: towards proactive\nconversational question answering over tabular and textual data in finance. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "representation learning. SIGMOD Rec., 51(1):33–40, 2022a. doi: 10.1145/3542700.3542709. URL\nhttps://doi.org/10.1145/3542700.3542709.\nYang Deng, Wenqiang Lei, Wenxuan Zhang, Wai Lam, and Tat-Seng Chua. PACIFIC: towards proactive\nconversational question answering over tabular and textual data in finance. In Yoav Goldberg, Zornitsa\nKozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural"
        }
    },
    {
        "id": "24fc17fe615b8a6ebdbb86a35f61ce7aa803f833",
        "content": "survey of models and applications. Transactions of the Association for Computational Linguistics, 11:\n227–249, 2023. doi: 10.1162/tacl_a_00544. URL https://aclanthology.org/2023.tacl-1.14.\nMrinal Kanti Baowaly, Chia-Ching Lin, Chao-Lin Liu, and Kuan-Ta Chen. Synthesizing electronic health\nrecords using improved generative adversarial networks. Journal of the American Medical Informatics\nAssociation, 26(3):228–241, 2019.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "survey of models and applications. Transactions of the Association for Computational Linguistics, 11:\n227–249, 2023. doi: 10.1162/tacl_a_00544. URL https://aclanthology.org/2023.tacl-1.14.\nMrinal Kanti Baowaly, Chia-Ching Lin, Chao-Lin Liu, and Kuan-Ta Chen. Synthesizing electronic health\nrecords using improved generative adversarial networks. Journal of the American Medical Informatics\nAssociation, 26(3):228–241, 2019."
        }
    },
    {
        "id": "296f3599de237136872fc75e55b5b0c567120067",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nRetrieval-augmented generation (RAG) Retrieval-augmented generation (RAG) relies on the intu-\nition that the LLMs are general models, but can be guided to a domain-specific answer if the user includes the\nrelevant context within the prompts. By incorporating tables as part of the prompts, most papers described\nin this survey can be attributed as RAG systems. A particular challenge in RAG is to extract the most",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nRetrieval-augmented generation (RAG) Retrieval-augmented generation (RAG) relies on the intu-\nition that the LLMs are general models, but can be guided to a domain-specific answer if the user includes the\nrelevant context within the prompts. By incorporating tables as part of the prompts, most papers described\nin this survey can be attributed as RAG systems. A particular challenge in RAG is to extract the most"
        }
    },
    {
        "id": "3184c1b845c82402715612ffb57989daa96a3fda",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nFigure 1: Overview of LLM on Tabular Data: the paper discusses application of LLM for prediction, data\ngeneration, and table understanding tasks\none-hot encoding), or it may introduce synthetic ordering (e.g. with ordinal encoding) (Borisov\net al., 2023a).\n4. Context-based interconnection: In tabular data, features can be correlated. For example, age,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nFigure 1: Overview of LLM on Tabular Data: the paper discusses application of LLM for prediction, data\ngeneration, and table understanding tasks\none-hot encoding), or it may introduce synthetic ordering (e.g. with ordinal encoding) (Borisov\net al., 2023a).\n4. Context-based interconnection: In tabular data, features can be correlated. For example, age,"
        }
    },
    {
        "id": "4b127821d578c56c566c70debbfe006ed36b3d05",
        "content": "Task\nExpert Task Understanding + Slack & Singh (2023) Identify if the car repair claim is fraudulent.\nLabel + Task An older car is more likely to have a fraudu-\nlent repair claim. Features Car Brand: Land\nRover Year: 2017. Answer with one of the fol-\nlowing: Yes | No\nDataset description + Feature Wang et al. (2023a) The dataset is about fraud repair claims. Car\nmeaning + Feature Value + Task Brand is the brand of car. The year is the age\nwhen the car is produced. The features are:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Task\nExpert Task Understanding + Slack & Singh (2023) Identify if the car repair claim is fraudulent.\nLabel + Task An older car is more likely to have a fraudu-\nlent repair claim. Features Car Brand: Land\nRover Year: 2017. Answer with one of the fol-\nlowing: Yes | No\nDataset description + Feature Wang et al. (2023a) The dataset is about fraud repair claims. Car\nmeaning + Feature Value + Task Brand is the brand of car. The year is the age\nwhen the car is produced. The features are:"
        }
    },
    {
        "id": "63aa365e79a4cf418d3a84fb39c3a79f1f92569e",
        "content": "2023) incorporates the use of Verbalizer (Cui et al., 2022) to map the answer to a valid class. To calculate\nAUCROC or AUCPR, the probability of the output is necessary. Thus, Verbalizer proves advantageous for\nclosed-source models by enabling the assignment of output probability. UniPredict (Wang et al., 2023a) has\nthe most complicated target augmentation. They transform the target label into a set of probabilities for",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2023) incorporates the use of Verbalizer (Cui et al., 2022) to map the answer to a valid class. To calculate\nAUCROC or AUCPR, the probability of the output is necessary. Thus, Verbalizer proves advantageous for\nclosed-source models by enabling the assignment of output probability. UniPredict (Wang et al., 2023a) has\nthe most complicated target augmentation. They transform the target label into a set of probabilities for"
        }
    },
    {
        "id": "8dc825c99d71918696a6aa5b6a23e591e6e64192",
        "content": "returns an answer. In more involved architectures, the system might be connected to external databases\nor programs. Most of the times, the knowledge base might not fit in the context length or memory of the\nLLM. Therefore, unique challenges to tabular QA for LLMs include: query intent disambiguation, search\n22https://huggingface.co/NumbersStation\n23For the scope of our paper, we do not consider images, videos and audio inputs.\n25",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "returns an answer. In more involved architectures, the system might be connected to external databases\nor programs. Most of the times, the knowledge base might not fit in the context length or memory of the\nLLM. Therefore, unique challenges to tabular QA for LLMs include: query intent disambiguation, search\n22https://huggingface.co/NumbersStation\n23For the scope of our paper, we do not consider images, videos and audio inputs.\n25"
        }
    },
    {
        "id": "e14b39bcaf9a2584450ab139d1cd1cb13b642c50",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul Groth, and H. V. Jagadish. Observatory: Characterizing\nembeddings of relational tables. CoRR, abs/2310.07736, 2023. doi: 10.48550/ARXIV.2310.07736. URL\nhttps://doi.org/10.48550/arXiv.2310.07736.\nGanqu Cui, Shengding Hu, Ning Ding, Longtao Huang, and Zhiyuan Liu. Prototypical verbalizer for prompt-\nbased few-shot tuning, 2022.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul Groth, and H. V. Jagadish. Observatory: Characterizing\nembeddings of relational tables. CoRR, abs/2310.07736, 2023. doi: 10.48550/ARXIV.2310.07736. URL\nhttps://doi.org/10.48550/arXiv.2310.07736.\nGanqu Cui, Shengding Hu, Ning Ding, Longtao Huang, and Zhiyuan Liu. Prototypical verbalizer for prompt-\nbased few-shot tuning, 2022."
        }
    },
    {
        "id": "16015644f6a4b9d9dbd634f2f53e68670425f62a",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nWenhu Chen. Large language models are few(1)-shot table reasoners. In Andreas Vlachos and Isabelle\nAugenstein (eds.), Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik,\nCroatia, May 2-6, 2023, pp. 1090–1100. Association for Computational Linguistics, 2023. doi: 10.18653/\nV1/2023.FINDINGS-EACL.83. URL https://doi.org/10.18653/v1/2023.findings-eacl.83.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nWenhu Chen. Large language models are few(1)-shot table reasoners. In Andreas Vlachos and Isabelle\nAugenstein (eds.), Findings of the Association for Computational Linguistics: EACL 2023, Dubrovnik,\nCroatia, May 2-6, 2023, pp. 1090–1100. Association for Computational Linguistics, 2023. doi: 10.18653/\nV1/2023.FINDINGS-EACL.83. URL https://doi.org/10.18653/v1/2023.findings-eacl.83."
        }
    },
    {
        "id": "1966e6118940864d72fba397913f2f94f4d64a37",
        "content": "Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao. Ct-bert: Learning\nbetter tabular representations through cross-table pre-training, 2023a.\n44",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao. Ct-bert: Learning\nbetter tabular representations through cross-table pre-training, 2023a.\n44"
        }
    },
    {
        "id": "1b841c7c40e75a45d37779c555b6b29d649fdad9",
        "content": "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural\nlanguage using reinforcement learning, 2017a.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural\nlanguage using reinforcement learning. CoRR, abs/1709.00103, 2017b.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\nTat-Seng Chua. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural\nlanguage using reinforcement learning, 2017a.\nVictor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural\nlanguage using reinforcement learning. CoRR, abs/1709.00103, 2017b.\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and\nTat-Seng Chua. TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in"
        }
    },
    {
        "id": "639d4b3033f7aa64e8de4e13b5f94b88604fe5bf",
        "content": "Table 3: Prediction methods. Resource is high if it has to finetune a model with size ≥ 1B even if it is\nPEFT. Used Model include all models used in the paper which includes serialization, preprocessing and\nmodel finetuning. ACC stands for accuracy. AUC stands for Area under the ROC Curve. MAE stands for\nmean absolute error. RMSE stands for root-mean-square error. F1 score is calculated from the precision",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Table 3: Prediction methods. Resource is high if it has to finetune a model with size ≥ 1B even if it is\nPEFT. Used Model include all models used in the paper which includes serialization, preprocessing and\nmodel finetuning. ACC stands for accuracy. AUC stands for Area under the ROC Curve. MAE stands for\nmean absolute error. RMSE stands for root-mean-square error. F1 score is calculated from the precision"
        }
    },
    {
        "id": "6475294ffe2a085c975c22bbd904c8dd398fd5d0",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nHao Fu, Yao; Peng and Tushar Khot. How does gpt obtain its ability? tracing emer-\ngent abilities of language models to their sources. Yao Fu’s Notion, Dec 2022. URL\nhttps://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-\nof-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1.\nGeorgi Ganev and Emiliano De Cristofaro. On the inadequacy of similarity-based privacy metrics: Recon-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nHao Fu, Yao; Peng and Tushar Khot. How does gpt obtain its ability? tracing emer-\ngent abilities of language models to their sources. Yao Fu’s Notion, Dec 2022. URL\nhttps://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-\nof-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1.\nGeorgi Ganev and Emiliano De Cristofaro. On the inadequacy of similarity-based privacy metrics: Recon-"
        }
    },
    {
        "id": "64950142db9f969ed82ffe273ec893d44dcff041",
        "content": "lightweight collaborative model for downstream tasks. The approach outperforms all the SOTA baselines\nincluding semantic and collaborative models over three datasets by a significant margin, showing superior\nprediction capabilities and proving the effectiveness of the paradigm of combining collaborative and semantic\nsignals. However, the code for this method is not available. They use LogLoss and AUC to evaluate the\nmethod.\n4 LLMs for tabular data generation",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "lightweight collaborative model for downstream tasks. The approach outperforms all the SOTA baselines\nincluding semantic and collaborative models over three datasets by a significant margin, showing superior\nprediction capabilities and proving the effectiveness of the paradigm of combining collaborative and semantic\nsignals. However, the code for this method is not available. They use LogLoss and AUC to evaluate the\nmethod.\n4 LLMs for tabular data generation"
        }
    },
    {
        "id": "710a6c339eb3ac0e7b61ac3c8dd96c65f04f1eb9",
        "content": "adversarial attacks.\n2.3 Prompt Engineering\nA prompt is an input text that is fed into an LLM. Designing an effective prompt is a non-trivial task, and\nmany research topics have branched out from prompt engineering alone. In this subsection, we cover the\npopular techniques in prompt engineering, and how researchers have used them for tasks involving tables.\nPrompt format The simplest format is concatenating task description with the serialized table as string.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "adversarial attacks.\n2.3 Prompt Engineering\nA prompt is an input text that is fed into an LLM. Designing an effective prompt is a non-trivial task, and\nmany research topics have branched out from prompt engineering alone. In this subsection, we cover the\npopular techniques in prompt engineering, and how researchers have used them for tasks involving tables.\nPrompt format The simplest format is concatenating task description with the serialized table as string."
        }
    },
    {
        "id": "7b9f303203c492e1648698dcc9ce358defbff993",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\ndependencies. 3. Tree-based models sometimes struggle to generalize to unseen data particularly if the\ntraining data is not representative of the entire distribution, while deep learning methods may generalize\nbetter to diverse datasets with their ability to learn intricate representations (Goodfellow et al., 2016).\nFor deep learning methods in tabular data prediction, the methodologies can be broadly grouped into the",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\ndependencies. 3. Tree-based models sometimes struggle to generalize to unseen data particularly if the\ntraining data is not representative of the entire distribution, while deep learning methods may generalize\nbetter to diverse datasets with their ability to learn intricate representations (Goodfellow et al., 2016).\nFor deep learning methods in tabular data prediction, the methodologies can be broadly grouped into the"
        }
    },
    {
        "id": "80a49c83ca84b4f0054243b77095516067951cd6",
        "content": "AIT-QA: Question answering dataset over complex tables in the airline industry. In Anastassia Loukina,\nRashmi Gangadharaiah, and Bonan Min (eds.), Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track,\npp. 305–314, Hybrid: Seattle, Washington + Online, July 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.naacl-industry.34. URL https://aclanthology.org/2022.naacl-industry.34.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "AIT-QA: Question answering dataset over complex tables in the airline industry. In Anastassia Loukina,\nRashmi Gangadharaiah, and Bonan Min (eds.), Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track,\npp. 305–314, Hybrid: Seattle, Washington + Online, July 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.naacl-industry.34. URL https://aclanthology.org/2022.naacl-industry.34."
        }
    },
    {
        "id": "99814f3abe49fb096e00ae2e5025209da17b4121",
        "content": "tional Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.398. URL https://doi.org/10.18653/v1/\n2020.acl-main.398.\nJonathan Herzig, Thomas Mueller, Syrine Krichene, and Julian Eisenschlos. Open domain question answering\nover tables via dense retrieval. In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp. 512–519, 2021.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tional Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.398. URL https://doi.org/10.18653/v1/\n2020.acl-main.398.\nJonathan Herzig, Thomas Mueller, Syrine Krichene, and Julian Eisenschlos. Open domain question answering\nover tables via dense retrieval. In Proceedings of the 2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, pp. 512–519, 2021."
        }
    },
    {
        "id": "9bae1073dca676d916e7c444eaaabbd081c3077f",
        "content": "tees (Ganev & De Cristofaro, 2023)) and 4) Performance on downstream tasks – like machine learning\nefficiency (MLE) and missing value imputation. MLE is to compare the testing accuracy on real data when\ntrained on synthetic ones. Additionally, the quality of data generation can be assessed through its perfor-\nmance on missing value imputation, that is, guessing the missing value(s) of a tuple, when the rest of the\nattributes are given.\n5 LLMs for table understanding",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tees (Ganev & De Cristofaro, 2023)) and 4) Performance on downstream tasks – like machine learning\nefficiency (MLE) and missing value imputation. MLE is to compare the testing accuracy on real data when\ntrained on synthetic ones. Additionally, the quality of data generation can be assessed through its perfor-\nmance on missing value imputation, that is, guessing the missing value(s) of a tuple, when the rest of the\nattributes are given.\n5 LLMs for table understanding"
        }
    },
    {
        "id": "9dca0baaeb975ca5cfdc15aad231afea8338b504",
        "content": "Fine-tuning strategy design Designing appropriate tasks and learning strategies for LLMs is extensively\nexplored. LLMs demonstrate emergent abilities such as in-context learning, instruction following, and step-\nby-step reasoning. However, these capabilities may not be fully evident in certain tabular data prediction\nand table understanding tasks, depending on the model used. LLMs are sensitive to various serialization",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Fine-tuning strategy design Designing appropriate tasks and learning strategies for LLMs is extensively\nexplored. LLMs demonstrate emergent abilities such as in-context learning, instruction following, and step-\nby-step reasoning. However, these capabilities may not be fully evident in certain tabular data prediction\nand table understanding tasks, depending on the model used. LLMs are sensitive to various serialization"
        }
    },
    {
        "id": "be761a6043b815cde144e1d8d201155ca522e52e",
        "content": "top four tables, then selecting the set that appears most frequently among the ten sets. To further filter\nthe columns, all columns are ranked by relevance to the question by specifying that ChatGPT match the\ncolumn names against with the question words or the foreign key should be placed ahead to assist in more\naccurate recall results. Similarly, SC method is used. cTBLS Sundar & Heck (2023) designed a three-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "top four tables, then selecting the set that appears most frequently among the ten sets. To further filter\nthe columns, all columns are ranked by relevance to the question by specifying that ChatGPT match the\ncolumn names against with the question words or the foreign key should be placed ahead to assist in more\naccurate recall results. Similarly, SC method is used. cTBLS Sundar & Heck (2023) designed a three-"
        }
    },
    {
        "id": "e84680a7e7a6c2b77f638c183adb5fcadf118e1b",
        "content": "Prokhorenkova, 2021). 3. Attention-based methods. These models incorporate attention mechanisms for\nfeature selection and reasoning (TabNet (Arik & Pfister, 2020)), feature encoding (TransTab (Wang & Sun,\n2022), TabTransformer (Huang et al., 2020), TP-BERTa (Yan et al., 2024b)), feature interaction modeling\n(DANet (Chen et al., 2022a), T2G-Former (Yan et al., 2023), ExcelFormer (Chen et al., 2023a), ARM-net",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Prokhorenkova, 2021). 3. Attention-based methods. These models incorporate attention mechanisms for\nfeature selection and reasoning (TabNet (Arik & Pfister, 2020)), feature encoding (TransTab (Wang & Sun,\n2022), TabTransformer (Huang et al., 2020), TP-BERTa (Yan et al., 2024b)), feature interaction modeling\n(DANet (Chen et al., 2022a), T2G-Former (Yan et al., 2023), ExcelFormer (Chen et al., 2023a), ARM-net"
        }
    },
    {
        "id": "f0d9922e1ac6496632a1f868011e26a53d4395fd",
        "content": "disease prediction tasks. Additionally, this approach, with an extended sequence length, is also suitable\nfor patients who were not hospitalized. LLM has also been combined with Vertical models to do medical\nprediction Yan et al. (2024a), showcasing remarkable performance even without any manual labels.\nFinancial Prediction FinPT (Yin et al., 2023) presents an LLM-based approach to financial risk prediction.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "disease prediction tasks. Additionally, this approach, with an extended sequence length, is also suitable\nfor patients who were not hospitalized. LLM has also been combined with Vertical models to do medical\nprediction Yan et al. (2024a), showcasing remarkable performance even without any manual labels.\nFinancial Prediction FinPT (Yin et al., 2023) presents an LLM-based approach to financial risk prediction."
        }
    },
    {
        "id": "fea3180bedc5ee8ff341cb1816683137261f6ae6",
        "content": "(2022) found their manually curated examples to outperform randomly selected examples by an average of\n14.7 F1 points. For Chen (2023), increasing from 1-shot to 2-shot can often benefit the model, however,\nfurther increases did not help.\nChain-of-Thought and Self-consistency Chain-of-Thought (CoT) (Wei et al., 2022c) induces LLMs to\ndecompose a task by performing step-by-step thinking, resulting in better reasoning. Program-of-Thoughts",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(2022) found their manually curated examples to outperform randomly selected examples by an average of\n14.7 F1 points. For Chen (2023), increasing from 1-shot to 2-shot can often benefit the model, however,\nfurther increases did not help.\nChain-of-Thought and Self-consistency Chain-of-Thought (CoT) (Wei et al., 2022c) induces LLMs to\ndecompose a task by performing step-by-step thinking, resulting in better reasoning. Program-of-Thoughts"
        }
    },
    {
        "id": "2b14c88f1249c417d322bf8cffb94d36ec845ac0",
        "content": "et al. (2023b) also found that markup languages, specifically HTML, outperformed X-separated formats for\nGPT3.5 and GPT4. Their hypothesis is that the GPT models were trained on a significant amount of web\ndata and thus, probably exposed the LLMs to more HTML and XML formats when interpreting tables.\nApart from manual templates, Hegselmann et al. (2023) also used LLMs (Fine-tuned BLOOM on ToTTo\n(Parikh et al., 2020b), T0++ (Sanh et al., 2022), GPT-3 (Ouyang et al., 2022)) to generate descriptions of",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al. (2023b) also found that markup languages, specifically HTML, outperformed X-separated formats for\nGPT3.5 and GPT4. Their hypothesis is that the GPT models were trained on a significant amount of web\ndata and thus, probably exposed the LLMs to more HTML and XML formats when interpreting tables.\nApart from manual templates, Hegselmann et al. (2023) also used LLMs (Fine-tuned BLOOM on ToTTo\n(Parikh et al., 2020b), T0++ (Sanh et al., 2022), GPT-3 (Ouyang et al., 2022)) to generate descriptions of"
        }
    },
    {
        "id": "4bd1fdf8180f73cdc85aa9f0e038eaaa36335e74",
        "content": "et al., 2023) creates multiple inputs through the serialization step. The AdaBoost algorithm then creates an\nensemble of summary-based weak learners. While non-fine-tuned LLMs struggle with continuous attributes,\nsummary boosting is effective with smaller datasets. Furthermore, its performance is enhanced using GPT-\ngenerated descriptions by leveraging existing model knowledge, underscoring the potential of LLMs in new",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2023) creates multiple inputs through the serialization step. The AdaBoost algorithm then creates an\nensemble of summary-based weak learners. While non-fine-tuned LLMs struggle with continuous attributes,\nsummary boosting is effective with smaller datasets. Furthermore, its performance is enhanced using GPT-\ngenerated descriptions by leveraging existing model knowledge, underscoring the potential of LLMs in new"
        }
    },
    {
        "id": "516a7300a6db8a087406e67f3b224a4fbf2123dc",
        "content": "Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models, 2022.\n32",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models, 2022.\n32"
        }
    },
    {
        "id": "5785cd696a957aaa40bc79a8f518594561ad01c0",
        "content": "data like correlations between time steps. The Symmetric Mean Absolute Percentage Error (SMAPE)\nmeasures the accuracy based on percentage errors, the Mean Absolute Scaled Error (MASE) is a scale-\nindependent error metric normalized by the in-sample mean absolute error of a naive benchmark model, and\nthe Overall Weighted Average (OWA) is a combined metric that averages the ranks of SMAPE and MASE\nto compare the performance of different methods. Among those metrics, MAE and RMSE are used by at",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "data like correlations between time steps. The Symmetric Mean Absolute Percentage Error (SMAPE)\nmeasures the accuracy based on percentage errors, the Mean Absolute Scaled Error (MASE) is a scale-\nindependent error metric normalized by the in-sample mean absolute error of a naive benchmark model, and\nthe Overall Weighted Average (OWA) is a combined metric that averages the ranks of SMAPE and MASE\nto compare the performance of different methods. Among those metrics, MAE and RMSE are used by at"
        }
    },
    {
        "id": "858c09fb55adee288771961c98baa6394a08dc60",
        "content": "informative, and free-form answers. NQ-TABLES Herzig et al. (2021) is larger than the previously mentioned\ntable. Its advantage lies in its emphasis on open-domain questions, which can be answered using structured\ntable data.14.\n14Official sites for Table QA datasets: FetaQA (https://github.com/Yale-LILY/FeTaQA), WikiTableQuestions\n(https://ppasupat.github.io/WikiTableQuestions/), HybridQA (https://github.com/wenhuchen/HybridQA), SQA (https:\n22",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "informative, and free-form answers. NQ-TABLES Herzig et al. (2021) is larger than the previously mentioned\ntable. Its advantage lies in its emphasis on open-domain questions, which can be answered using structured\ntable data.14.\n14Official sites for Table QA datasets: FetaQA (https://github.com/Yale-LILY/FeTaQA), WikiTableQuestions\n(https://ppasupat.github.io/WikiTableQuestions/), HybridQA (https://github.com/wenhuchen/HybridQA), SQA (https:\n22"
        }
    },
    {
        "id": "8b75d9d0cfb4a88270585109ebace6cb87e91bf9",
        "content": "the highest semantic similarity to the question, surpassing methods involving no sampling, or clustering,\nrandom, even sampling, or content snapshots. Dong et al. (2023) used ChatGPT to rank tables based on\ntheir relevance to the question using SC: they generate ten sets of retrieval results, each set containing the\ntop four tables, then selecting the set that appears most frequently among the ten sets. To further filter",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the highest semantic similarity to the question, surpassing methods involving no sampling, or clustering,\nrandom, even sampling, or content snapshots. Dong et al. (2023) used ChatGPT to rank tables based on\ntheir relevance to the question using SC: they generate ten sets of retrieval results, each set containing the\ntop four tables, then selecting the set that appears most frequently among the ten sets. To further filter"
        }
    },
    {
        "id": "8e423457a5f839c9f2ded39d7c5694925dee9919",
        "content": "(Parikh et al., 2020b), T0++ (Sanh et al., 2022), GPT-3 (Ouyang et al., 2022)) to generate descriptions of\na table as sentences, blurring the line between a text-based and embedding-based serialization methodology.\n3Same name, different group of authors.\n9",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "(Parikh et al., 2020b), T0++ (Sanh et al., 2022), GPT-3 (Ouyang et al., 2022)) to generate descriptions of\na table as sentences, blurring the line between a text-based and embedding-based serialization methodology.\n3Same name, different group of authors.\n9"
        }
    },
    {
        "id": "9607fd9a2bbb8379078beb7588d858cf12ce74b4",
        "content": "and table understanding tasks, depending on the model used. LLMs are sensitive to various serialization\nand prompt engineering methods (Hegselmann et al., 2023), which is the primary way to adapt LLM to\nunseen tasks. As a future direction, researchers and practitioners need to carefully design tasks and learning\nstrategies tailored to tabular data in order to achieve an optimal performance.\nModel grafting The performance of LLM for tabular data modeling could be improved through model",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and table understanding tasks, depending on the model used. LLMs are sensitive to various serialization\nand prompt engineering methods (Hegselmann et al., 2023), which is the primary way to adapt LLM to\nunseen tasks. As a future direction, researchers and practitioners need to carefully design tasks and learning\nstrategies tailored to tabular data in order to achieve an optimal performance.\nModel grafting The performance of LLM for tabular data modeling could be improved through model"
        }
    },
    {
        "id": "9724f32bba1bebda967a708a58257c9229035b7b",
        "content": "Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinfor-\nmatics, 36(4):1234–1240, September 2019. ISSN 1367-4811. doi: 10.1093/bioinformatics/btz682. URL\nhttp://dx.doi.org/10.1093/bioinformatics/btz682.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. RESDSQL: decoupling schema linking and skeleton\nparsing for text-to-sql. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinfor-\nmatics, 36(4):1234–1240, September 2019. ISSN 1367-4811. doi: 10.1093/bioinformatics/btz682. URL\nhttp://dx.doi.org/10.1093/bioinformatics/btz682.\nHaoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. RESDSQL: decoupling schema linking and skeleton\nparsing for text-to-sql. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI"
        }
    },
    {
        "id": "a47f2c35dea60acdf0ab56d0095ad8a31af56a12",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nthe system further leverages multitask learning on domain-specific datasets, generates pseudo-labels for\nadditional data, and refines them using Shapley scores. Pretraining on the refined dataset is followed by\nfine-tuning using the original data. The resulting model supports both zero-shot and few-shot learning for\nnew datasets. GPT-3.5 accessed via OpenAI’s API facilitates data consolidation and augmentation, while",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nthe system further leverages multitask learning on domain-specific datasets, generates pseudo-labels for\nadditional data, and refines them using Shapley scores. Pretraining on the refined dataset is followed by\nfine-tuning using the original data. The resulting model supports both zero-shot and few-shot learning for\nnew datasets. GPT-3.5 accessed via OpenAI’s API facilitates data consolidation and augmentation, while"
        }
    },
    {
        "id": "b2e547e049a4692af76e26e9c8a1cd52fd0453ea",
        "content": "doi: 10.18653/v1/2020.emnlp-main.89. URL https://aclanthology.org/2020.emnlp-main.89.\nAnkur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\nDipanjan Das. Totto: A controlled table-to-text generation dataset. CoRR, abs/2004.14373, 2020b. URL\nhttps://arxiv.org/abs/2004.14373.\nNoseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "doi: 10.18653/v1/2020.emnlp-main.89. URL https://aclanthology.org/2020.emnlp-main.89.\nAnkur P. Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\nDipanjan Das. Totto: A controlled table-to-text generation dataset. CoRR, abs/2004.14373, 2020b. URL\nhttps://arxiv.org/abs/2004.14373.\nNoseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim."
        }
    },
    {
        "id": "fee6d5bdb10ec871254745132fb7a287088fb0d3",
        "content": "et al., 2022) allows for more standardized and comparable evaluation across different NLG models and tasks,\npotentially leading to more reliable and consistent benchmarking in the field. The dataset is in footnote. 18.\nTable NLI InfoTabs (Gupta et al., 2020) uses Wikipedia info-boxes and is designed to facilitate under-\nstanding of semi-structured tabulated text, which involves comprehending both text fragments and their",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "et al., 2022) allows for more standardized and comparable evaluation across different NLG models and tasks,\npotentially leading to more reliable and consistent benchmarking in the field. The dataset is in footnote. 18.\nTable NLI InfoTabs (Gupta et al., 2020) uses Wikipedia info-boxes and is designed to facilitate under-\nstanding of semi-structured tabulated text, which involves comprehending both text fragments and their"
        }
    },
    {
        "id": "03560ee53a265c2adcd442ff9cf2b2fc73570e2b",
        "content": "10.48550/ARXIV.2302.09302. URL https://doi.org/10.48550/arXiv.2302.09302.\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pp. 785–794,\nNew York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/\n2939672.2939785. URL https://doi.org/10.1145/2939672.2939785.\n31",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "10.48550/ARXIV.2302.09302. URL https://doi.org/10.48550/arXiv.2302.09302.\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’16, pp. 785–794,\nNew York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342322. doi: 10.1145/\n2939672.2939785. URL https://doi.org/10.1145/2939672.2939785.\n31"
        }
    },
    {
        "id": "4f5dd430dabe7f1561a0f6176c102b66658c67b1",
        "content": "Finetuning or No finetuning? There are some larger models that fine-tune on various tabular tasks,\nsome including QA and FV tasks, mentioned in Section 2.1 under embeddings-based serialization. Li et al.\n(2023d) found that fine-tuning always helps to improve performance across various tabular tasks. In zero-\nshot settings, the improvement is the most significant. For Ye et al. (2023b), they obtained higher scores on",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Finetuning or No finetuning? There are some larger models that fine-tune on various tabular tasks,\nsome including QA and FV tasks, mentioned in Section 2.1 under embeddings-based serialization. Li et al.\n(2023d) found that fine-tuning always helps to improve performance across various tabular tasks. In zero-\nshot settings, the improvement is the most significant. For Ye et al. (2023b), they obtained higher scores on"
        }
    },
    {
        "id": "518b1421af1497f8d2281f039d30f63ea8cb4914",
        "content": "additional embeddings designed to capture tabular structure. It also integrates two classification layers\nto facilitate the selection of cells and predict the corresponding aggregation operator. A particular table\nQA task, Text2SQL, involves translating natural language question into structured query language (SQL).\nEarlier research conducted semantic parsing through hand-crafted features and grammar rules (Pasupat\n4",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "additional embeddings designed to capture tabular structure. It also integrates two classification layers\nto facilitate the selection of cells and predict the corresponding aggregation operator. A particular table\nQA task, Text2SQL, involves translating natural language question into structured query language (SQL).\nEarlier research conducted semantic parsing through hand-crafted features and grammar rules (Pasupat\n4"
        }
    },
    {
        "id": "56b69e2b5643c01b2ba9acc4e08914e66075097a",
        "content": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunk-\numar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis\nKaramanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima\nDoshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj\nVarshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sam-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunk-\numar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis\nKaramanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima\nDoshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj\nVarshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sam-"
        }
    },
    {
        "id": "68041ba13b522dafeea1c1c179c5bbc770126d85",
        "content": "table and a sequence-to-sequence model conditioned on the parent table for the relational dataset. The\nmodel implements target masking to prevent data copying and introduces statistical methods to detect\noverfitting. It demonstrates superior performance in capturing relational structures and achieves state-of-\nthe-art results in predictive tasks without needing fine-tuning. Following the similar paradigm, Zhang et al.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "table and a sequence-to-sequence model conditioned on the parent table for the relational dataset. The\nmodel implements target masking to prevent data copying and introduces statistical methods to detect\noverfitting. It demonstrates superior performance in capturing relational structures and achieves state-of-\nthe-art results in predictive tasks without needing fine-tuning. Following the similar paradigm, Zhang et al."
        }
    },
    {
        "id": "6ca34ff4865bf4eb912d6ecae0d244d946eb268b",
        "content": "solving tabular prediction problems in the electronic health record, 2023.\nNafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych. Scigen: a dataset for reasoning-\naware text generation from scientific tables. In Thirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=\nJul-uX7EV_I.\nKai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen, and William Yang Wang. HybriDialogue: An",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "solving tabular prediction problems in the electronic health record, 2023.\nNafise Sadat Moosavi, Andreas Rücklé, Dan Roth, and Iryna Gurevych. Scigen: a dataset for reasoning-\naware text generation from scientific tables. In Thirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=\nJul-uX7EV_I.\nKai Nakamura, Sharon Levy, Yi-Lin Tuan, Wenhu Chen, and William Yang Wang. HybriDialogue: An"
        }
    },
    {
        "id": "830b729bcc1aab937eca54c78e07440ace3fdd0a",
        "content": "Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https:\n//proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Ad-\nvances in Neural Information Processing Systems 30: Annual Conference on Neural Information Pro-\ncessing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL https:\n//proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html."
        }
    },
    {
        "id": "a4720a22135490a77609d1fcb851fa23d36e3a51",
        "content": "neural text-to-sql generation, 2018a.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning\nYao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale human-labeled dataset\nfor complex and cross-domain semantic parsing and text-to-SQL task. In Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "neural text-to-sql generation, 2018a.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning\nYao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale human-labeled dataset\nfor complex and cross-domain semantic parsing and text-to-SQL task. In Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods"
        }
    },
    {
        "id": "b14e1c736874f1b02e42dff751df91d847fb2b21",
        "content": "learning dynamics and two novel curation metrics, namely confidence and uncertainty. These metrics help\nto filter out undesirable generated samples during the training process of a classifier, aiming to produce\nhigh-quality synthetic data. Specifically, both metrics are calculated for each sample, utilizing the classifier\ntrained on these samples. Additionally, CLLM distinguishes itself by not requiring any fine-tuning of LLMs.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "learning dynamics and two novel curation metrics, namely confidence and uncertainty. These metrics help\nto filter out undesirable generated samples during the training process of a classifier, aiming to produce\nhigh-quality synthetic data. Specifically, both metrics are calculated for each sample, utilizing the classifier\ntrained on these samples. Additionally, CLLM distinguishes itself by not requiring any fine-tuning of LLMs."
        }
    },
    {
        "id": "b726b7ae00f13fecb5cc0367dae6b811d737a193",
        "content": "At the end, we will briefly cover domain-specific prediction methods using LLMs.\n3.1 Datasets\nFor task-specific fine-tuning, most datasets used for the prediction task are from UCI ML, OpenML, or a\ncombo of 9 datasets created by Manikandan et al. (2023). Details of the datasets are in Table 2. OpenML\nhas the highest number of datasets, but the size of the largest dataset is only 5600 rows. Half of the datasets",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "At the end, we will briefly cover domain-specific prediction methods using LLMs.\n3.1 Datasets\nFor task-specific fine-tuning, most datasets used for the prediction task are from UCI ML, OpenML, or a\ncombo of 9 datasets created by Manikandan et al. (2023). Details of the datasets are in Table 2. OpenML\nhas the highest number of datasets, but the size of the largest dataset is only 5600 rows. Half of the datasets"
        }
    },
    {
        "id": "bdc523c748fa1e123f76705560c64b3db321d8bb",
        "content": "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-\npanjan Das. ToTTo: A controlled table-to-text generation dataset. In Bonnie Webber, Trevor Cohn, Yulan\nHe, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1173–1186, Online, November 2020a. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-main.89. URL https://aclanthology.org/2020.emnlp-main.89.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Di-\npanjan Das. ToTTo: A controlled table-to-text generation dataset. In Bonnie Webber, Trevor Cohn, Yulan\nHe, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pp. 1173–1186, Online, November 2020a. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-main.89. URL https://aclanthology.org/2020.emnlp-main.89."
        }
    },
    {
        "id": "c2bd73d192fe4a3805879fd230a79f79be75b162",
        "content": "tabular data, FeTaQA introduces elements that require deeper reasoning and integration of information. This\nincludes generating free-form text answers that involve the retrieval, inference, and integration of multiple\ndiscontinuous facts from structured knowledge sources like tables. This requires the model to generate long,\ninformative, and free-form answers. NQ-TABLES Herzig et al. (2021) is larger than the previously mentioned",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "tabular data, FeTaQA introduces elements that require deeper reasoning and integration of information. This\nincludes generating free-form text answers that involve the retrieval, inference, and integration of multiple\ndiscontinuous facts from structured knowledge sources like tables. This requires the model to generate long,\ninformative, and free-form answers. NQ-TABLES Herzig et al. (2021) is larger than the previously mentioned"
        }
    },
    {
        "id": "f3251af49fed57c4fb9810a87f454a769acb8c41",
        "content": "by the size of the window and the tokenization method of numerical feature, preventing it from further\nimprovement.\nFine-tuning Fine-tuning the model for time series prediction is more commonly seen in current research.\nPromptCast (Xue & Salim, 2022) tried the method of inference-only prediction or fine-tuning on task-specific\ndatasets. It shows that larger models always perform better. Time LLM (Jin et al., 2023a) presents a novel",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "by the size of the window and the tokenization method of numerical feature, preventing it from further\nimprovement.\nFine-tuning Fine-tuning the model for time series prediction is more commonly seen in current research.\nPromptCast (Xue & Salim, 2022) tried the method of inference-only prediction or fine-tuning on task-specific\ndatasets. It shows that larger models always perform better. Time LLM (Jin et al., 2023a) presents a novel"
        }
    },
    {
        "id": "f9a81ffebfe363a5d1da93e339e33ef436c6906f",
        "content": "Georgi Ganev and Emiliano De Cristofaro. On the inadequacy of similarity-based privacy metrics: Recon-\nstruction attacks against\" truly anonymous synthetic data”. arXiv preprint arXiv:2312.05114, 2023.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql\nempowered by large language models: A benchmark evaluation. Proc. VLDB Endow., 17(5):1132–1145,\n2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Georgi Ganev and Emiliano De Cristofaro. On the inadequacy of similarity-based privacy metrics: Recon-\nstruction attacks against\" truly anonymous synthetic data”. arXiv preprint arXiv:2312.05114, 2023.\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql\nempowered by large language models: A benchmark evaluation. Proc. VLDB Endow., 17(5):1132–1145,\n2024. URL https://www.vldb.org/pvldb/vol17/p1132-gao.pdf."
        }
    },
    {
        "id": "fef4c08307398ad5276d380f75e776f7d5c286c2",
        "content": "Inference and Text2SQL. We further provided recommended datasets based on tasks and their\nGitHub link. Practitioners and researchers can look at the section and find relevant dataset eas-\nily. We share publicly-available datasets here: https://github.com/tanfiona/LLM-on-Tabular-Data-\nPrediction-Table-Understanding-Data-Generation\n4. A survey and taxonomy of techniques for LLMs’ applications on tabular data. For each",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Inference and Text2SQL. We further provided recommended datasets based on tasks and their\nGitHub link. Practitioners and researchers can look at the section and find relevant dataset eas-\nily. We share publicly-available datasets here: https://github.com/tanfiona/LLM-on-Tabular-Data-\nPrediction-Table-Understanding-Data-Generation\n4. A survey and taxonomy of techniques for LLMs’ applications on tabular data. For each"
        }
    },
    {
        "id": "02930ba7651b18d0d1022c2af80a67c98d32bcbe",
        "content": "task instructions, and input statistics as a prefix. TEST (Sun et al., 2023a) introduces an embedding layer\ntailored for LLMs, using exponentially dilated causal convolution networks for time series processing. The\nembedding is generated through contrastive learning with unique positive pairs and aligning text and time\nseries tokens using similarity measures. Serialization involves two QA templates, treating multivariate time\nseries as univariate series for sequential template filling.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "task instructions, and input statistics as a prefix. TEST (Sun et al., 2023a) introduces an embedding layer\ntailored for LLMs, using exponentially dilated causal convolution networks for time series processing. The\nembedding is generated through contrastive learning with unique positive pairs and aligning text and time\nseries tokens using similarity measures. Serialization involves two QA templates, treating multivariate time\nseries as univariate series for sequential template filling."
        }
    },
    {
        "id": "04a389f5f5ee24ac41033a9020e694fcf6f42637",
        "content": "that summarizes and compares the key techniques, metrics, datasets, models, and optimiza-\ntion approaches in this research domain. This survey aims to address this gap by consolidat-\ning recent progress in these areas, offering a thorough survey and taxonomy of the datasets,\nmetrics, and methodologies utilized. It identifies strengths, limitations, unexplored territo-\nries, and gaps in the existing literature, while providing some insights for future research",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "that summarizes and compares the key techniques, metrics, datasets, models, and optimiza-\ntion approaches in this research domain. This survey aims to address this gap by consolidat-\ning recent progress in these areas, offering a thorough survey and taxonomy of the datasets,\nmetrics, and methodologies utilized. It identifies strengths, limitations, unexplored territo-\nries, and gaps in the existing literature, while providing some insights for future research"
        }
    },
    {
        "id": "05d3b4f7ad7cf289d5504f9ab97a5f693ea71ba5",
        "content": "ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pp. 1470–1480. The Association for\nComputer Linguistics, 2015a. doi: 10.3115/V1/P15-1142. URL https://doi.org/10.3115/v1/p15-1142.\n39",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pp. 1470–1480. The Association for\nComputer Linguistics, 2015a. doi: 10.3115/V1/P15-1142. URL https://doi.org/10.3115/v1/p15-1142.\n39"
        }
    },
    {
        "id": "0eeeeda9c3b14a9ac59a5d465b6447b0ab579637",
        "content": "ctr prediction, 2023e.\nYikuan Li, Shishir Rao, José Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy,\nYajie Zhu, Kazem Rahimi, and Gholamreza Salimi-Khorshidi. Behrt: transformer for electronic health\nrecords. Scientific reports, 10(1):7155, 2020a.\nZheng Li, Yue Zhao, and Jialin Fu. Sync: A copula based framework for generating synthetic data from\naggregated sources, 2020b.\nZhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ctr prediction, 2023e.\nYikuan Li, Shishir Rao, José Roberto Ayala Solares, Abdelaali Hassaine, Rema Ramakrishnan, Dexter Canoy,\nYajie Zhu, Kazem Rahimi, and Gholamreza Salimi-Khorshidi. Behrt: transformer for electronic health\nrecords. Scientific reports, 10(1):7155, 2020a.\nZheng Li, Yue Zhao, and Jialin Fu. Sync: A copula based framework for generating synthetic data from\naggregated sources, 2020b.\nZhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru Hu, Bin Zhang, Yuxiao Ye,"
        }
    },
    {
        "id": "2c5b604a7aea96a83b48f1c1e8fa74d3beabf146",
        "content": "Target Augmentation LLMs can solve complex tasks through text generation, however, the output is not\nalways controllable (Dinh et al., 2022). As a result, mapping the textual output from LLMs to a target label\nfor prediction tasks is essential. This is called target augmentation (Wang et al., 2023a). A straightforward\nbut labor-intensive way is manual labeling, as was used by Serilize-LM (Jaitly et al., 2023). To be more",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Target Augmentation LLMs can solve complex tasks through text generation, however, the output is not\nalways controllable (Dinh et al., 2022). As a result, mapping the textual output from LLMs to a target label\nfor prediction tasks is essential. This is called target augmentation (Wang et al., 2023a). A straightforward\nbut labor-intensive way is manual labeling, as was used by Serilize-LM (Jaitly et al., 2023). To be more"
        }
    },
    {
        "id": "5acbe1985b346d972b43f096a387a91463997bb6",
        "content": "2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022c. URL http://papers.nips.cc/\npaper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\nLei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data\nusing conditional gan, 2019.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022c. URL http://papers.nips.cc/\npaper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.\nLei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data\nusing conditional gan, 2019."
        }
    },
    {
        "id": "65f13d58b8d20872046c25a8558ade5be6708ed8",
        "content": "forum?id=qs4swxtIAQ.\nCheng Guo and Felix Berkhahn. Entity embeddings of categorical variables, 2016.\nHuifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: A factorization-machine\nbased neural network for ctr prediction, 2017.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. INFOTABS: Inference on tables as semi-\nstructured data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "forum?id=qs4swxtIAQ.\nCheng Guo and Felix Berkhahn. Entity embeddings of categorical variables, 2016.\nHuifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: A factorization-machine\nbased neural network for ctr prediction, 2017.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek Srikumar. INFOTABS: Inference on tables as semi-\nstructured data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings"
        }
    },
    {
        "id": "6726d273251fe560161d66bdd74ef4dd54c049e9",
        "content": "of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November\n3-7, 2019, pp. 1962–1979. Association for Computational Linguistics, 2019a. doi: 10.18653/V1/D19-1204.\nURL https://doi.org/10.18653/v1/D19-1204.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November\n3-7, 2019, pp. 1962–1979. Association for Computational Linguistics, 2019a. doi: 10.18653/V1/D19-1204.\nURL https://doi.org/10.18653/v1/D19-1204.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene Li,"
        }
    },
    {
        "id": "73047c352148f8b792c956a4b5e9ae27dfd6da00",
        "content": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn.\nRes., 2022, 2022b. URL https://openreview.net/forum?id=yzkSU5zdwD.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Trans. Mach. Learn.\nRes., 2022, 2022b. URL https://openreview.net/forum?id=yzkSU5zdwD.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,"
        }
    },
    {
        "id": "9606e3da9d455b87569e04544e79a97128982de7",
        "content": "neapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://aclanthology.org/N19-1423.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language\nmodels. Nature Machine Intelligence, 5(3):220–235, 2023.\nTuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dim-",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "neapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.\nURL https://aclanthology.org/N19-1423.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained language\nmodels. Nature Machine Intelligence, 5(3):220–235, 2023.\nTuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dim-"
        }
    },
    {
        "id": "9c52d0981e4c35f1310f5720101591fe37ad26f7",
        "content": "power heavily relies on the past. For each category, we divide it into different steps which include pre-\nprocessing, fine-tuning, and target augmentation. Preprocessing explains how different prediction methods\ngenerate input to the language model. Preprocessing includes serialization, table manipulation, and prompt\nengineering. Target augmentation maps the textual output from LLMs to a target label for prediction tasks.\nAt the end, we will briefly cover domain-specific prediction methods using LLMs.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "power heavily relies on the past. For each category, we divide it into different steps which include pre-\nprocessing, fine-tuning, and target augmentation. Preprocessing explains how different prediction methods\ngenerate input to the language model. Preprocessing includes serialization, table manipulation, and prompt\nengineering. Target augmentation maps the textual output from LLMs to a target label for prediction tasks.\nAt the end, we will briefly cover domain-specific prediction methods using LLMs."
        }
    },
    {
        "id": "9c9d4632728a634e8ee1833ffa058ccf6925e0c8",
        "content": "data modeling, GBDT algorithms, including XGBoost, LightGBM, and CatBoost (Prokhorenkova et al.,\n2019), still outperform deep-learning methods in most datasets with additional benefits in fast training time,\nhigh interpretability, and easy optimization (Shwartz-Ziv & Armon, 2022; Gorishniy et al., 2021; Grinsztajn\net al., 2022). Deep learning models, however, may have their advantages over traditional methods in some",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "data modeling, GBDT algorithms, including XGBoost, LightGBM, and CatBoost (Prokhorenkova et al.,\n2019), still outperform deep-learning methods in most datasets with additional benefits in fast training time,\nhigh interpretability, and easy optimization (Shwartz-Ziv & Armon, 2022; Gorishniy et al., 2021; Grinsztajn\net al., 2022). Deep learning models, however, may have their advantages over traditional methods in some"
        }
    },
    {
        "id": "a37a5164dbfa0f8b065943a33099994e007ece3f",
        "content": "multilingual fine-tuning, 2024.\nArlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular\ndatasets, 2021.\nYannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael\nGlass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, and Soumen Chakrabarti.\nAIT-QA: Question answering dataset over complex tables in the airline industry. In Anastassia Loukina,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "multilingual fine-tuning, 2024.\nArlind Kadra, Marius Lindauer, Frank Hutter, and Josif Grabocka. Well-tuned simple nets excel on tabular\ndatasets, 2021.\nYannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael\nGlass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, and Soumen Chakrabarti.\nAIT-QA: Question answering dataset over complex tables in the airline industry. In Anastassia Loukina,"
        }
    },
    {
        "id": "b73cb6aaa997e097a731fd693a618ea3c5febb2e",
        "content": "48.80 (Chen, 2023), 35.01 (Zhang et al.,\n2023g)\nHybridQA 13000 QA Table Answer Wikipedia Exact Match Accuracy: 39.38 (Zhang\n(Chen et al., Question et al., 2023g), 25.14 (Sui et al., 2023c)\n2020b)\nSQA (Iyyer 982 QA Table Answer WikiTable- Exact Match Accuracy: 71.23 (Sarkar\net al., 2017) Question Question & Lausen, 2023), 33.45 (Sui et al.,\n2023c)\nHiTAB (Cheng 3597 QA/ Question, Answer Statistical Execution Accuracy: 64.71 (Zhang\net al., 2022) NLG Table Report and et al., 2023g), 50.00 (Zhao et al., 2023a)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "48.80 (Chen, 2023), 35.01 (Zhang et al.,\n2023g)\nHybridQA 13000 QA Table Answer Wikipedia Exact Match Accuracy: 39.38 (Zhang\n(Chen et al., Question et al., 2023g), 25.14 (Sui et al., 2023c)\n2020b)\nSQA (Iyyer 982 QA Table Answer WikiTable- Exact Match Accuracy: 71.23 (Sarkar\net al., 2017) Question Question & Lausen, 2023), 33.45 (Sui et al.,\n2023c)\nHiTAB (Cheng 3597 QA/ Question, Answer Statistical Execution Accuracy: 64.71 (Zhang\net al., 2022) NLG Table Report and et al., 2023g), 50.00 (Zhao et al., 2023a)"
        }
    },
    {
        "id": "c9b795077f1b6e82db1b02d9fa73504376e72f67",
        "content": "meaning + Feature Value + Task Brand is the brand of car. The year is the age\nwhen the car is produced. The features are:\nCar Brand is Land Rover. The year is 2017.\nPredict if this car repair claim is fraudulent by\nYes for fraudulent, No for not fraudulent\nTable 4: Method and Example for different preprocessing for general predictive tasks. The example is to\npredict if a car repair claim is fraudulent or not.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "meaning + Feature Value + Task Brand is the brand of car. The year is the age\nwhen the car is produced. The features are:\nCar Brand is Land Rover. The year is 2017.\nPredict if this car repair claim is fraudulent by\nYes for fraudulent, No for not fraudulent\nTable 4: Method and Example for different preprocessing for general predictive tasks. The example is to\npredict if a car repair claim is fraudulent or not."
        }
    },
    {
        "id": "03667b9685d9bad505a64a1f8592977af2878a5f",
        "content": "attributes like make, color, and body type are now combined into a single,richer sentence. It leverages\ncovariance to identify the most relevant features and either label them critically or add a sentence to ex-\nplain the most important features. Finally, they converted tabular data into LaTeX code format. This\nLaTeX representation of the table was then used as the input for fine-tuning our LLM by just passing a row\nrepresentation preceded by hline tag without any headers.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "attributes like make, color, and body type are now combined into a single,richer sentence. It leverages\ncovariance to identify the most relevant features and either label them critically or add a sentence to ex-\nplain the most important features. Finally, they converted tabular data into LaTeX code format. This\nLaTeX representation of the table was then used as the input for fine-tuning our LLM by just passing a row\nrepresentation preceded by hline tag without any headers."
        }
    },
    {
        "id": "3bb3e6c4af0e205b05c142bd0562ff71f2478447",
        "content": "to incorporate semantic knowledge from column names, converting heterogenous tabular data into textual\n6",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "to incorporate semantic knowledge from column names, converting heterogenous tabular data into textual\n6"
        }
    },
    {
        "id": "582236685525d7b704f663946c6b7a58bd40a4fc",
        "content": "also generate code readable by other programs. Abraham et al. (2022) designed a model that converts\nnatural language queries to structured queries, which can be run against a database or a spreadsheet. Liu\net al. (2023e) designed a system where the LLM could interact with Python to execute commands, process\ndata, and scrutinize results (within a Pandas DataFrame), iteratively over a maximum of five iterations.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "also generate code readable by other programs. Abraham et al. (2022) designed a model that converts\nnatural language queries to structured queries, which can be run against a database or a spreadsheet. Liu\net al. (2023e) designed a system where the LLM could interact with Python to execute commands, process\ndata, and scrutinize results (within a Pandas DataFrame), iteratively over a maximum of five iterations."
        }
    },
    {
        "id": "63e4e1e899be990fbe3195840d78692cdaf0a734",
        "content": "ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/\npdf?id=6ruVLB727MC.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/\npdf?id=6ruVLB727MC.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching"
        }
    },
    {
        "id": "7ac658344ae28e08d7ad28162d3a982c079af823",
        "content": "new datasets. GPT-3.5 accessed via OpenAI’s API facilitates data consolidation and augmentation, while\nUnifiedQA-v2-T5 (Khashabi et al., 2022) is employed for sanity checks. Additionally, Meditab utilizes a pre-\ntrained BioBert classifier (Lee et al., 2019). The system undergoes thorough evaluation across supervised,\nfew-shot, and zero-shot learning scenarios within the medical domain, demonstrating superior performance",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "new datasets. GPT-3.5 accessed via OpenAI’s API facilitates data consolidation and augmentation, while\nUnifiedQA-v2-T5 (Khashabi et al., 2022) is employed for sanity checks. Additionally, Meditab utilizes a pre-\ntrained BioBert classifier (Lee et al., 2019). The system undergoes thorough evaluation across supervised,\nfew-shot, and zero-shot learning scenarios within the medical domain, demonstrating superior performance"
        }
    },
    {
        "id": "8a69685f8ac12c9da4a3e7fc5c7350aaf2d96029",
        "content": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen\nAnderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain,\nXiaobing Liu, and Hemal Shah. Wide & deep learning for recommender systems, 2016.\nZhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou,\nand Dongmei Zhang. HiTab: A hierarchical table dataset for question answering and natural language",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen\nAnderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain,\nXiaobing Liu, and Hemal Shah. Wide & deep learning for recommender systems, 2016.\nZhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou,\nand Dongmei Zhang. HiTab: A hierarchical table dataset for question answering and natural language"
        }
    },
    {
        "id": "c8ac1a00550906a3d50e7fbf4d7c3273d741585b",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nthis fashion, more studies tested a more complex serialization and prompt engineering method rather than\na simple concatenation of feature and value for serialization.\nThe schema-based prompt engineering usually includes background information about the dataset, a task\ndescription, a summary, and example data points. Summary Boosting(Manikandan et al., 2023) serializes",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nthis fashion, more studies tested a more complex serialization and prompt engineering method rather than\na simple concatenation of feature and value for serialization.\nThe schema-based prompt engineering usually includes background information about the dataset, a task\ndescription, a summary, and example data points. Summary Boosting(Manikandan et al., 2023) serializes"
        }
    },
    {
        "id": "a0aaea48bfdcda91db846f5de045baba004a42ba",
        "content": "Working around constraints or error de-bugging Zhao et al. (2023a) used multi-turn prompts to\nwork around cases where the tables exceed the API input limit. In other cases, especially if the generated\nLLM output is code, an iterative process of feeding errors back to the LLM can help the LLM generate\ncorrect code. Zhang et al. (2023d) did so to improve SQL query generation.\n5.3.4 Output evaluation and format",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Working around constraints or error de-bugging Zhao et al. (2023a) used multi-turn prompts to\nwork around cases where the tables exceed the API input limit. In other cases, especially if the generated\nLLM output is code, an iterative process of feeding errors back to the LLM can help the LLM generate\ncorrect code. Zhang et al. (2023d) did so to improve SQL query generation.\n5.3.4 Output evaluation and format"
        }
    },
    {
        "id": "d3e70788a5ffe46f69a6d0c29b13d4d8a1b86ef7",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nMing Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang,\nHaifeng Chen, Xiaoli Li, Shirui Pan, Vincent S. Tseng, Yu Zheng, Lei Chen, and Hui Xiong. Large models\nfor time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196, 2023b.\nNengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question answering:\nRecent advances, 2022.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nMing Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang,\nHaifeng Chen, Xiaoli Li, Shirui Pan, Vincent S. Tseng, Yu Zheng, Lei Chen, and Hui Xiong. Large models\nfor time series and spatio-temporal data: A survey and outlook. arXiv preprint arXiv:2310.10196, 2023b.\nNengzheng Jin, Joanna Siebert, Dongfang Li, and Qingcai Chen. A survey on table question answering:\nRecent advances, 2022."
        }
    },
    {
        "id": "0ca9ead5ca918cacd10cc0c8a0aefce4144cbbef",
        "content": "3. Dependency on pre-processing: Data pre-processing is crucial and application-dependent when work-\ning with tabular data. For numerical values, common techniques include data normalization or\nscaling, categorical value encoding, missing value imputation, and outlier removal. For categorical\nvalues, common techniques include label encoding or one-hot encoding. Improper pre-processing\nmay lead to information loss, a sparse matrix, and it may introduce multi-collinearity (e.g. with",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "3. Dependency on pre-processing: Data pre-processing is crucial and application-dependent when work-\ning with tabular data. For numerical values, common techniques include data normalization or\nscaling, categorical value encoding, missing value imputation, and outlier removal. For categorical\nvalues, common techniques include label encoding or one-hot encoding. Improper pre-processing\nmay lead to information loss, a sparse matrix, and it may introduce multi-collinearity (e.g. with"
        }
    },
    {
        "id": "1530ace972f0c993fa55a6e0b2f11b5ce6332353",
        "content": "the output sequence generated by the adapted LLM function and the reference output sequence generated\nfrom target augmentation (represented by serialized target). To make LLMs applicable to multiple tabular\ndatasets at the same time, Generative Tabular Learning (GTL) was proposed by Zhang et al. (2023a). It\nincludes two parts: 1) the first part specifies the task background and description with optionally some",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the output sequence generated by the adapted LLM function and the reference output sequence generated\nfrom target augmentation (represented by serialized target). To make LLMs applicable to multiple tabular\ndatasets at the same time, Generative Tabular Learning (GTL) was proposed by Zhang et al. (2023a). It\nincludes two parts: 1) the first part specifies the task background and description with optionally some"
        }
    },
    {
        "id": "3d826eeaf7a0a425e4808e26c1fc983e9a5ffc08",
        "content": "why the LLMs are worse off at serializing tables to sentences is due to the tendency for LLMs to hallucinate:\nLLMs respond with unrelated expressions, adding new data, or return incorrect feature values.\n2.2 Table Manipulations\nTable manipulations refer to operations and transformations performed on tabular data, typically stored in\ndatabases or spreadsheets. These manipulations involve actions such as filtering, sorting, joining, aggregating,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "why the LLMs are worse off at serializing tables to sentences is due to the tendency for LLMs to hallucinate:\nLLMs respond with unrelated expressions, adding new data, or return incorrect feature values.\n2.2 Table Manipulations\nTable manipulations refer to operations and transformations performed on tabular data, typically stored in\ndatabases or spreadsheets. These manipulations involve actions such as filtering, sorting, joining, aggregating,"
        }
    },
    {
        "id": "47bb5ebccceb826f7a9d8f3a00f38bcbc4fe5e16",
        "content": "Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pp. 3911–3921, Brussels, Belgium, October-November 2018b. Association\nfor Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan\nLi, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pp. 3911–3921, Brussels, Belgium, October-November 2018b. Association\nfor Computational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan\nLi, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander R. Fabbri, Zifan Li, Luyao"
        }
    },
    {
        "id": "7b942a97b50e1dea2db5a9bf7971fc42656c3c3f",
        "content": "2021)). In boosting ensemble methods, base learners are learned sequentially to reduce previous learner’s\nerror until there is no significant improvement, making it relatively stable and accurate than a single learner\n(Chen & Guestrin, 2016). Traditional tree-based models are known for its high performance, efficiency\nin training, ease of tuning, and ease of interpretation. However, they have limitations compared to deep",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2021)). In boosting ensemble methods, base learners are learned sequentially to reduce previous learner’s\nerror until there is no significant improvement, making it relatively stable and accurate than a single learner\n(Chen & Guestrin, 2016). Traditional tree-based models are known for its high performance, efficiency\nin training, ease of tuning, and ease of interpretation. However, they have limitations compared to deep"
        }
    },
    {
        "id": "81e2a30f29848564046d2b672ca4fbde32fe6013",
        "content": "complex table reasoning tasks, especially when combined with prompt engineering tricks like CoT. In this\n//www.microsoft.com/en-us/download/details.aspx?id=54253), and NQ-Tables (https://github.com/google-research-\ndatasets/natural-questions).\n15Official site for HybriDialogue: https://github.com/entitize/HybridDialogue\n16Official site for FEVEROUS: https://fever.ai/dataset/feverous.html. Official site for Dresden Web Tables: https:\n//ppasupat.github.io/WikiTableQuestions/.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "complex table reasoning tasks, especially when combined with prompt engineering tricks like CoT. In this\n//www.microsoft.com/en-us/download/details.aspx?id=54253), and NQ-Tables (https://github.com/google-research-\ndatasets/natural-questions).\n15Official site for HybriDialogue: https://github.com/entitize/HybridDialogue\n16Official site for FEVEROUS: https://fever.ai/dataset/feverous.html. Official site for Dresden Web Tables: https:\n//ppasupat.github.io/WikiTableQuestions/."
        }
    },
    {
        "id": "87e0ec33a07b4370335b11a03a7dd0fb3de4b31b",
        "content": "insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and\naddress the prevailing challenges in the field.\nReferences\nAbhijith Neil Abraham, Fariz Rahman, and Damanpreet Kaur. Tablequery: Querying tabular data with\nnatural language. CoRR, abs/2202.00454, 2022. URL https://arxiv.org/abs/2202.00454.\nMubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, and Elena",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and\naddress the prevailing challenges in the field.\nReferences\nAbhijith Neil Abraham, Fariz Rahman, and Damanpreet Kaur. Tablequery: Querying tabular data with\nnatural language. CoRR, abs/2202.00454, 2022. URL https://arxiv.org/abs/2202.00454.\nMubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, and Elena"
        }
    },
    {
        "id": "dfad2b22e8945eee53e05d65bcf1003f35980c29",
        "content": "data such as clinical notes, has high interconnection between variables, contains missing data and noisy\nsignals. The LM based model could capture the long-term dependencies among events such as diabetes and\ndeal with unstructured data such as clinical notes. Thus, LM based models (McMaster et al., 2023; Steinberg\net al., 2021; Rasmy et al., 2021; Li et al., 2020a) perform better than XGBoost. However, these models only",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "data such as clinical notes, has high interconnection between variables, contains missing data and noisy\nsignals. The LM based model could capture the long-term dependencies among events such as diabetes and\ndeal with unstructured data such as clinical notes. Thus, LM based models (McMaster et al., 2023; Steinberg\net al., 2021; Rasmy et al., 2021; Li et al., 2020a) perform better than XGBoost. However, these models only"
        }
    },
    {
        "id": "ff39a9dc393362141828a0492205e2e176b0b21a",
        "content": "and finally, how to improve the performance of downstream applications.\n2 Key techniques for LLMs’ applications on tabular data\nWhile conducting our survey, we noticed a few common components in modeling tabular data with LLMs\nacross tasks. We discuss common techniques, like serialization, table manipulations, prompt engineering,\nand building end-to-end systems in this section. Fine-tuning LLMs is also popular, but it tends to be\napplication-specific, thus we discuss it later, in Sections 3 and 5.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "and finally, how to improve the performance of downstream applications.\n2 Key techniques for LLMs’ applications on tabular data\nWhile conducting our survey, we noticed a few common components in modeling tabular data with LLMs\nacross tasks. We discuss common techniques, like serialization, table manipulations, prompt engineering,\nand building end-to-end systems in this section. Fine-tuning LLMs is also popular, but it tends to be\napplication-specific, thus we discuss it later, in Sections 3 and 5."
        }
    },
    {
        "id": "0b185a5b2de2e8fa0285787e208b1df4ac0bc7cf",
        "content": "representation (i.e. word embedding) of each word so that similar words are mapped close to each other in\nthe embedding space (e.g. Word2Vec); thus, the model can generalize well to unseen sequences, as well as it\navoids the curse of dimensionality (Bengio et al., 2000). Later, rather than learning a static word embedding,\ncontext-aware representation learning was introduced by pretraining the model on large-scale unannotated",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "representation (i.e. word embedding) of each word so that similar words are mapped close to each other in\nthe embedding space (e.g. Word2Vec); thus, the model can generalize well to unseen sequences, as well as it\navoids the curse of dimensionality (Bengio et al., 2000). Later, rather than learning a static word embedding,\ncontext-aware representation learning was introduced by pretraining the model on large-scale unannotated"
        }
    },
    {
        "id": "4885e2566248627bd4d35e95aa509696bbc1aa41",
        "content": "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,"
        }
    },
    {
        "id": "4d7d9fd9c18c518a7d7d7f9af902cfef08db946f",
        "content": "Mubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, and Elena\nSimperl. Exploring the numerical reasoning capabilities of language models: A comprehensive analysis\non tabular data. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for\nComputational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 15391–15405. Association\nfor Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp.1028.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Mubashara Akhtar, Abhilash Reddy Shankarampeta, Vivek Gupta, Arpit Patil, Oana Cocarascu, and Elena\nSimperl. Exploring the numerical reasoning capabilities of language models: A comprehensive analysis\non tabular data. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for\nComputational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 15391–15405. Association\nfor Computational Linguistics, 2023. URL https://aclanthology.org/2023.findings-emnlp.1028."
        }
    },
    {
        "id": "9335cd5a316c8c731c974518919726f268b011a5",
        "content": "//ppasupat.github.io/WikiTableQuestions/.\n17Official site for Spider: https://drive.usercontent.google.com/download?id=1iRDVHLr4mX2wQKSgA9J8Pire73Jahh0m&export=\ndownload&authuser=0. Official site for WikiSQL: https://github.com/salesforce/WikiSQL.\n18Official site for ToTTo: https://github.com/google-research-datasets/ToTTo. Official site for HiTAB: https://\ngithub.com/microsoft/HiTab\n19Official site for InfoTabs: https://infotabs.github.io/. Official site for TabFact: https://tabfact.github.io/",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "//ppasupat.github.io/WikiTableQuestions/.\n17Official site for Spider: https://drive.usercontent.google.com/download?id=1iRDVHLr4mX2wQKSgA9J8Pire73Jahh0m&export=\ndownload&authuser=0. Official site for WikiSQL: https://github.com/salesforce/WikiSQL.\n18Official site for ToTTo: https://github.com/google-research-datasets/ToTTo. Official site for HiTAB: https://\ngithub.com/microsoft/HiTab\n19Official site for InfoTabs: https://infotabs.github.io/. Official site for TabFact: https://tabfact.github.io/"
        }
    },
    {
        "id": "9b065fe398ea840d33e8747f34becec607d99d22",
        "content": "the LLMs with one shot CoT demonstration example to generate a reasoning and answer. Subsequently,\nthey included the reasoning texts, indicated by special “<CoT>” token, as part of inputs to fine-tune smaller\nmodels to generate the final answer.\nSelf-consistency (SC) (Wang et al., 2023b) leverages the intuition that a complex reasoning problem typically\nadmits multiple different ways of thinking leading to its unique correct answer. SC samples a diverse set",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the LLMs with one shot CoT demonstration example to generate a reasoning and answer. Subsequently,\nthey included the reasoning texts, indicated by special “<CoT>” token, as part of inputs to fine-tune smaller\nmodels to generate the final answer.\nSelf-consistency (SC) (Wang et al., 2023b) leverages the intuition that a complex reasoning problem typically\nadmits multiple different ways of thinking leading to its unique correct answer. SC samples a diverse set"
        }
    },
    {
        "id": "c5b91ca8ee1390a4a360a8af3e5d65b872b8bbef",
        "content": "3. A survey and taxonomy of datasets for LLMs’ applications on tabular data. For\neach application, we identify datasets that are commonly used for benchmark. For table un-\nderstanding and question answering, we further categorize datasets by their downstream appli-\ncations: Question Answering, Natural Language Generation, Classification, Natural Language\nInference and Text2SQL. We further provided recommended datasets based on tasks and their",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "3. A survey and taxonomy of datasets for LLMs’ applications on tabular data. For\neach application, we identify datasets that are commonly used for benchmark. For table un-\nderstanding and question answering, we further categorize datasets by their downstream appli-\ncations: Question Answering, Natural Language Generation, Classification, Natural Language\nInference and Text2SQL. We further provided recommended datasets based on tasks and their"
        }
    },
    {
        "id": "178be23528eea550627750d06c2b9c373e42bc4d",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nMethodology Method Example\nFeature name + Feature Value + Dinh et al. (2022); Hegsel- Car Brand is Land Rover. Year is 2017. Re-\nPredicted Feature Name mann et al. (2023) pair claim is\nTask Background + Feature Zhang et al. (2023a) The task is about fraud repair claim predic-\nmeaning + Feature Value + Pre- tion. The brand of car is Land Rover. The\ndicted Feature meaning produce year is 2017. The repair claim of the\ncar is",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nMethodology Method Example\nFeature name + Feature Value + Dinh et al. (2022); Hegsel- Car Brand is Land Rover. Year is 2017. Re-\nPredicted Feature Name mann et al. (2023) pair claim is\nTask Background + Feature Zhang et al. (2023a) The task is about fraud repair claim predic-\nmeaning + Feature Value + Pre- tion. The brand of car is Land Rover. The\ndicted Feature meaning produce year is 2017. The repair claim of the\ncar is"
        }
    },
    {
        "id": "a5ffa9aba9a88b6774caa3c69b1651757efb1476",
        "content": "the most complicated target augmentation. They transform the target label into a set of probabilities for\neach class via a function called “augment”. Formally, for a target T in an arbitrary dataset D, they define a\nfunction augment(T ) = (C, P ), where C are new categories of targets with semantic meaning and P are the\nassigned probabilities to each category. They extend the target into categorical one-hot encoding and then",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "the most complicated target augmentation. They transform the target label into a set of probabilities for\neach class via a function called “augment”. Formally, for a target T in an arbitrary dataset D, they define a\nfunction augment(T ) = (C, P ), where C are new categories of targets with semantic meaning and P are the\nassigned probabilities to each category. They extend the target into categorical one-hot encoding and then"
        }
    },
    {
        "id": "c581ba33de00dfbc93b54ab62bfeabcbb88020c6",
        "content": "Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye,\nYali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao.\nTablegpt: Towards unifying tables, nature language and commands into one GPT. CoRR, abs/2307.08674,\n2023. doi: 10.48550/ARXIV.2307.08674. URL https://doi.org/10.48550/arXiv.2307.08674.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Liangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi Huang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou, Miao Wang, Wufang Zhu, Guoshan Lu, Chao Ye,\nYali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng, Jie Xu, Haobo Wang, Gang Chen, and Junbo Zhao.\nTablegpt: Towards unifying tables, nature language and commands into one GPT. CoRR, abs/2307.08674,\n2023. doi: 10.48550/ARXIV.2307.08674. URL https://doi.org/10.48550/arXiv.2307.08674."
        }
    },
    {
        "id": "e27c491037d5fd84502db649ec556c0d3978d49e",
        "content": "Text-to-SQL Evaluation) benchmark contains large tables and complex questions, and it been widely used\nby the community.\nTable NLG ToTTo (Parikh et al., 2020a) aims to create natural yet faithful descriptions to the source\ntable. It is rich in size and can be used to benchmark table conditional text generation task. HiTAB (Cheng\net al., 2022) allows for more standardized and comparable evaluation across different NLG models and tasks,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Text-to-SQL Evaluation) benchmark contains large tables and complex questions, and it been widely used\nby the community.\nTable NLG ToTTo (Parikh et al., 2020a) aims to create natural yet faithful descriptions to the source\ntable. It is rich in size and can be used to benchmark table conditional text generation task. HiTAB (Cheng\net al., 2022) allows for more standardized and comparable evaluation across different NLG models and tasks,"
        }
    },
    {
        "id": "02b6f4b5cd9d733b6a39f3baf8cc7d961469d977",
        "content": "correct code. Zhang et al. (2023d) did so to improve SQL query generation.\n5.3.4 Output evaluation and format\nIf the QA output is a number or category, F1 or Accuracy evaluation metrics are common. If evaluating\nopen-ended responses, apart from using typical measures for like ROUGE and BLEU, some papers also\nhire annotators to evaluate the Informativeness, Coherence and Fluency of the LLM responses Zhang et al.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "correct code. Zhang et al. (2023d) did so to improve SQL query generation.\n5.3.4 Output evaluation and format\nIf the QA output is a number or category, F1 or Accuracy evaluation metrics are common. If evaluating\nopen-ended responses, apart from using typical measures for like ROUGE and BLEU, some papers also\nhire annotators to evaluate the Informativeness, Coherence and Fluency of the LLM responses Zhang et al."
        }
    },
    {
        "id": "0dba426b91e7512a3883071bfd0d666fd335a262",
        "content": "HTML code reflecting tables, etc). Alternatively, the table could be converted into X-separated values, where\nX could be any reasonable delimiter like comma or tab. Some papers convert the tables into human-readable\nsentences using templates based on the column headers and cell values. The most common approach based\non our survey is the Markdown format.\nEmbedding-based Many papers also employ table encoders, which were fine-tuned from PLMs, to encode",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "HTML code reflecting tables, etc). Alternatively, the table could be converted into X-separated values, where\nX could be any reasonable delimiter like comma or tab. Some papers convert the tables into human-readable\nsentences using templates based on the column headers and cell values. The most common approach based\non our survey is the Markdown format.\nEmbedding-based Many papers also employ table encoders, which were fine-tuned from PLMs, to encode"
        }
    },
    {
        "id": "f6fe91bc2efc925ad14d07df227d9a98f7c0a8cd",
        "content": "reflection: querying the LLM with the input template “What is the {column}? {x}” to check if the answer\nmatches the original values. If the answers do not match, the descriptions are corrected by re-prompting\nthe LLM. However, this method requires iterative efforts and is hard to deploy in real world application.\nAn interesting future direction would be to explore efficient and practical way to deal with hallucination in\nLLM based method for tabular data.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "reflection: querying the LLM with the input template “What is the {column}? {x}” to check if the answer\nmatches the original values. If the answers do not match, the descriptions are corrected by re-prompting\nthe LLM. However, this method requires iterative efforts and is hard to deploy in real world application.\nAn interesting future direction would be to explore efficient and practical way to deal with hallucination in\nLLM based method for tabular data."
        }
    },
    {
        "id": "3f1578687d2f92e6b272223d1512022593ebf14a",
        "content": "a sequence of intermediate command operations. For example, an LLM needs to first check if the task\nrequires retrieval, mathematical reasoning, table manipulations, and/or the questions cannot be answered\nif the instructions are too vague. They constructed a dataset of command chain instructions to fine-tune\nLLMs to generate these commands. Deng et al. (2022b) proposed the QA task be split into three subtasks:",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "a sequence of intermediate command operations. For example, an LLM needs to first check if the task\nrequires retrieval, mathematical reasoning, table manipulations, and/or the questions cannot be answered\nif the instructions are too vague. They constructed a dataset of command chain instructions to fine-tune\nLLMs to generate these commands. Deng et al. (2022b) proposed the QA task be split into three subtasks:"
        }
    },
    {
        "id": "59695ccb3c9ee659f79d1a29ec4873d39be52457",
        "content": "data generation are mostly based on Copulas (Patki et al., 2016; Li et al., 2020b) and Bayesian networks\n(Zhang et al., 2017; Madl et al., 2023) while recent advancement in generative models such as Variational\nAutoencoders (VAEs) (Ma et al., 2020; Darabi & Elor, 2021; Vardhan & Kok, 2020; Liu et al., 2023d; Xu\net al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018; Choi et al., 2018; Baowaly et al.,",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "data generation are mostly based on Copulas (Patki et al., 2016; Li et al., 2020b) and Bayesian networks\n(Zhang et al., 2017; Madl et al., 2023) while recent advancement in generative models such as Variational\nAutoencoders (VAEs) (Ma et al., 2020; Darabi & Elor, 2021; Vardhan & Kok, 2020; Liu et al., 2023d; Xu\net al., 2023b)), generative adversarial networks (GANs) (Park et al., 2018; Choi et al., 2018; Baowaly et al.,"
        }
    },
    {
        "id": "7111fa80c647fcc6d03baa68b0d10085974613ec",
        "content": "Specific Prefix Manikandan et al. (2023); Please answer with category 1, category 2, ...\nSlack & Singh (2023)\nPredict probability and Wang et al. (2023a) {category1: probability1} => Calibrated\nrecalibrate by XGBoost\nTable 5: Target Augmentation methods, papers that used them, and examples\n3.4 Applications of Prediction using LLM\nMedical Prediction Medical data such as electronic health records (EHR) is a rich and complex source of",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Specific Prefix Manikandan et al. (2023); Please answer with category 1, category 2, ...\nSlack & Singh (2023)\nPredict probability and Wang et al. (2023a) {category1: probability1} => Calibrated\nrecalibrate by XGBoost\nTable 5: Target Augmentation methods, papers that used them, and examples\n3.4 Applications of Prediction using LLM\nMedical Prediction Medical data such as electronic health records (EHR) is a rich and complex source of"
        }
    },
    {
        "id": "85bb927902df47a65e181b9d5e8e42f0f86969fa",
        "content": "2019)), or methods focusing on combining feature transformation with deep neural networks (Wide&Deep\n(Cheng et al., 2016; Guo & Berkhahn, 2016), DeepFM (Guo et al., 2017), DNN2LR (Liu et al., 2021)). 2.\nDifferentiable trees. Inspired by the performance of ensembled trees, this line of methods seeks to make\ntrees differentiable by smoothing the decision function (NODE (Popov et al., 2019), SDTR (Luo et al.,\n2021), Net-DNF (Katzir et al., 2020)). Another subcategory of methods combine tree-based models with",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "2019)), or methods focusing on combining feature transformation with deep neural networks (Wide&Deep\n(Cheng et al., 2016; Guo & Berkhahn, 2016), DeepFM (Guo et al., 2017), DNN2LR (Liu et al., 2021)). 2.\nDifferentiable trees. Inspired by the performance of ensembled trees, this line of methods seeks to make\ntrees differentiable by smoothing the decision function (NODE (Popov et al., 2019), SDTR (Luo et al.,\n2021), Net-DNF (Katzir et al., 2020)). Another subcategory of methods combine tree-based models with"
        }
    },
    {
        "id": "b6ce5f92cdb0a859509a364ab3d6612cdf1bab1f",
        "content": "Processing, pp. 9237–9251, Singapore, December 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.574. URL https://aclanthology.org/2023.emnlp-main.574.\nMing Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan\nLiang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language\nmodels. arXiv preprint arXiv:2310.01728, 2023a.\n35",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Processing, pp. 9237–9251, Singapore, December 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.574. URL https://aclanthology.org/2023.emnlp-main.574.\nMing Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan\nLiang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language\nmodels. arXiv preprint arXiv:2310.01728, 2023a.\n35"
        }
    },
    {
        "id": "2da32857cf06578b8335fc9b76c53ed7bd1123cc",
        "content": "classification task, data generation, and table understanding.\nTabular prediction refers to classification and regression tasks. For tabular prediction, despite advancements\nin the field, traditional tree-based ensemble methods such as gradient-boosted decision trees (GBDT) remain\nthe state-of-the-art (SOTA) for classification task on tabular data (Borisov et al., 2022a; Gorishniy et al.,\n2021)). In boosting ensemble methods, base learners are learned sequentially to reduce previous learner’s",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "classification task, data generation, and table understanding.\nTabular prediction refers to classification and regression tasks. For tabular prediction, despite advancements\nin the field, traditional tree-based ensemble methods such as gradient-boosted decision trees (GBDT) remain\nthe state-of-the-art (SOTA) for classification task on tabular data (Borisov et al., 2022a; Gorishniy et al.,\n2021)). In boosting ensemble methods, base learners are learned sequentially to reduce previous learner’s"
        }
    },
    {
        "id": "437cfd0d2cf0c58578690b169d36da305b2ec20b",
        "content": "CoT by adding the line “Let’s think logically. This is because”.\n4For table headers, they explored synonym and abbreviation replacement perturbations. For table content, they explored\nfive perturbations: (1) row shuffling, (2) column shuffling, (3) extending column names content into semantically equivalent\nexpressions, (4) masking correlated columns (E.g. “Ranking” and “Total Points” can be inferred from one another), and (5)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "CoT by adding the line “Let’s think logically. This is because”.\n4For table headers, they explored synonym and abbreviation replacement perturbations. For table content, they explored\nfive perturbations: (1) row shuffling, (2) column shuffling, (3) extending column names content into semantically equivalent\nexpressions, (4) masking correlated columns (E.g. “Ranking” and “Total Points” can be inferred from one another), and (5)"
        }
    },
    {
        "id": "e4daa3b5ce3f10581fd75ef3c91c1fb57ebb7712",
        "content": "PaLM-2 on the Text2SQL task, achieving considerable performance on Spider. OpenTab (Kong et al., 2024)\ndeveloped an LLM-based framework for the open-domain table QA tasks, incorporating a SQL generation\nCoder module. The top scoring models for the Spider today are Dong et al. (2023); Gao et al. (2024);\nPourreza & Rafiei (2023), all building off OpenAI’s GPT models. SQL generation is popular in the industry,\nwith many open-source fine-tuned models available22.",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "PaLM-2 on the Text2SQL task, achieving considerable performance on Spider. OpenTab (Kong et al., 2024)\ndeveloped an LLM-based framework for the open-domain table QA tasks, incorporating a SQL generation\nCoder module. The top scoring models for the Spider today are Dong et al. (2023); Gao et al. (2024);\nPourreza & Rafiei (2023), all building off OpenAI’s GPT models. SQL generation is popular in the industry,\nwith many open-source fine-tuned models available22."
        }
    },
    {
        "id": "3ac7b2533cb487b938ad11b472137e042effe2cd",
        "content": "Text-based Table 1 describes the common text-based serialization methods in the literature. A straight-\nforward way would be to directly input a programming-language readable data structure (E.g. Pandas\nDataFrame Loader for Python, line-separated JSON-file format, Data Matrix represented by a list of lists,\nHTML code reflecting tables, etc). Alternatively, the table could be converted into X-separated values, where",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Text-based Table 1 describes the common text-based serialization methods in the literature. A straight-\nforward way would be to directly input a programming-language readable data structure (E.g. Pandas\nDataFrame Loader for Python, line-separated JSON-file format, Data Matrix represented by a list of lists,\nHTML code reflecting tables, etc). Alternatively, the table could be converted into X-separated values, where"
        }
    },
    {
        "id": "7950b4457f9ec694102fd3be6a750beb4358ad02",
        "content": "regressors in regressions lead to biased coefficients, hence, a modeler must be aware of such intricacies\n(Liu et al., 2023d).\n5. Order invariant: In tabular data, samples and features can be sorted. However, as opposed to text-\nbased and image-based data that is intrinsically tied to the position of the word/token or pixel in the\ntext or image, tabular data are relatively order-invariant. Therefore, position-based methodologies",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "regressors in regressions lead to biased coefficients, hence, a modeler must be aware of such intricacies\n(Liu et al., 2023d).\n5. Order invariant: In tabular data, samples and features can be sorted. However, as opposed to text-\nbased and image-based data that is intrinsically tied to the position of the word/token or pixel in the\ntext or image, tabular data are relatively order-invariant. Therefore, position-based methodologies"
        }
    },
    {
        "id": "a72be9c68916c695ee962e8fe718cb8b60b2c19d",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\nand retrieval, output types and format, and multi-turn settings where iterative calls between programs are\nneeded. We describe these components further in this section.\n5.3.1 Query intent disambiguation\nZha et al. (2023) introduced the concept of Chain-of-command (CoC), that translates user inputs into\na sequence of intermediate command operations. For example, an LLM needs to first check if the task",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\nand retrieval, output types and format, and multi-turn settings where iterative calls between programs are\nneeded. We describe these components further in this section.\n5.3.1 Query intent disambiguation\nZha et al. (2023) introduced the concept of Chain-of-command (CoC), that translates user inputs into\na sequence of intermediate command operations. For example, an LLM needs to first check if the task"
        }
    },
    {
        "id": "713e87d3d0105d6f39086044acc773c995c7ba33",
        "content": "AWS\nYanjun Qi yanjunqi@amazon.com\nAWS\nUniversity of Virginia\nScott Nickleach nickleac@amazon.com\nAmazon\nDiego Socolinsky sclinsky@amazon.com\nAWS\nSrinivasan Sengamedu sengamed@amazon.com\nAmazon\nChristos Faloutsos faloutso@amazon.com\nCarnegie Mellon University\nAmazon\nReviewed on OpenReview: https: // openreview .net/ forum? id= IZnrCGF9WI&noteId= nWxFR40vnD\nAbstract\nRecent breakthroughs in large language modeling have facilitated rigorous exploration of",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "AWS\nYanjun Qi yanjunqi@amazon.com\nAWS\nUniversity of Virginia\nScott Nickleach nickleac@amazon.com\nAmazon\nDiego Socolinsky sclinsky@amazon.com\nAWS\nSrinivasan Sengamedu sengamed@amazon.com\nAmazon\nChristos Faloutsos faloutso@amazon.com\nCarnegie Mellon University\nAmazon\nReviewed on OpenReview: https: // openreview .net/ forum? id= IZnrCGF9WI&noteId= nWxFR40vnD\nAbstract\nRecent breakthroughs in large language modeling have facilitated rigorous exploration of"
        }
    },
    {
        "id": "00d04c5bd7814befd684ca03e7b6d478c765c8a0",
        "content": "entails extracting essential features to populate their respective slots. Unlike the previous supervised end-\nto-end models, TableQuery is a NL2SQL model pretrained on QA on free text that obviates the necessity of\nloading the entire dataset into memory and serializing databases.\nFigure 2: Tabular data characteristics and machine learning models for tabular data prediction, data syn-\nthesis and table understanding like question answering before LLMs.\n1.3 Overview of large language models (LLMs)",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "entails extracting essential features to populate their respective slots. Unlike the previous supervised end-\nto-end models, TableQuery is a NL2SQL model pretrained on QA on free text that obviates the necessity of\nloading the entire dataset into memory and serializing databases.\nFigure 2: Tabular data characteristics and machine learning models for tabular data prediction, data syn-\nthesis and table understanding like question answering before LLMs.\n1.3 Overview of large language models (LLMs)"
        }
    },
    {
        "id": "3b0faec9963f707742d9901bdd6c7ed123098ab2",
        "content": "different dimensions: 1) Low-order statistics – column-wise density and pair-wise column correlation, es-\ntimating individual column density and the relational dynamics between pairs of columns, 2) High-order\nmetrics – the calculation of α-precision and β-recall scores that measure the overall fidelity and diversity\nof synthetic data, 3) privacy preservation – DCR score, representing the median Distance to the Closest",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "different dimensions: 1) Low-order statistics – column-wise density and pair-wise column correlation, es-\ntimating individual column density and the relational dynamics between pairs of columns, 2) High-order\nmetrics – the calculation of α-precision and β-recall scores that measure the overall fidelity and diversity\nof synthetic data, 3) privacy preservation – DCR score, representing the median Distance to the Closest"
        }
    },
    {
        "id": "4871d2eab177866f5614d5ba931af72266d90a94",
        "content": "for Computational Linguistics, 2020b. doi: 10.18653/V1/2020.FINDINGS-EMNLP.91. URL https:\n//doi.org/10.18653/v1/2020.findings-emnlp.91.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disen-\ntangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022b. doi:\n10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "for Computational Linguistics, 2020b. doi: 10.18653/V1/2020.FINDINGS-EMNLP.91. URL https:\n//doi.org/10.18653/v1/2020.findings-emnlp.91.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disen-\ntangling computation from reasoning for numerical reasoning tasks. CoRR, abs/2211.12588, 2022b. doi:\n10.48550/ARXIV.2211.12588. URL https://doi.org/10.48550/arXiv.2211.12588.\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao"
        }
    },
    {
        "id": "ea718f96a6f8a94cbf2e5f58cf07fb59a622c288",
        "content": "Medical Prediction Medical data such as electronic health records (EHR) is a rich and complex source of\ninformation about patients’ medical histories, treatments, and outcomes. It has more inherent complexity\nthan simple tabular data. It captures information about patients’ health over time, contains unstructured\ndata such as clinical notes, has high interconnection between variables, contains missing data and noisy",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Medical Prediction Medical data such as electronic health records (EHR) is a rich and complex source of\ninformation about patients’ medical histories, treatments, and outcomes. It has more inherent complexity\nthan simple tabular data. It captures information about patients’ health over time, contains unstructured\ndata such as clinical notes, has high interconnection between variables, contains missing data and noisy"
        }
    },
    {
        "id": "fc56087bfa6160e41d8c7e8a13539b14e3daf895",
        "content": "query generation. Finally, Liu et al. (2023c) proposes a no-code data analytics platform that uses LLMs\nto generate data summaries, including generating pertinent questions required for analysis, and queries into\nthe data parser. A survey by Zhang et al. (2023h) covers further concepts about natural language interfaces\nfor tabular data querying and visualization, diving deeper into recent advancements in Text-to-SQL and\nText-to-Vis domains.\n3 LLMs for predictions",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "query generation. Finally, Liu et al. (2023c) proposes a no-code data analytics platform that uses LLMs\nto generate data summaries, including generating pertinent questions required for analysis, and queries into\nthe data parser. A survey by Zhang et al. (2023h) covers further concepts about natural language interfaces\nfor tabular data querying and visualization, diving deeper into recent advancements in Text-to-SQL and\nText-to-Vis domains.\n3 LLMs for predictions"
        }
    },
    {
        "id": "c56d286b3ede3763c2cd916b332ad6de1eac92e8",
        "content": "Published in Transactions on Machine Learning Research (07/2024)\napproximation for s∗. Output queries are encoded into binary discrete syntax vectors. Narayan et al. (2022)\nexplored manually curated and random example selection.\n5.3.3 Multi-turn tasks\nSome papers design pipelines that call LLMs iteratively. We categorize the use-cases for doing so into three\nbuckets: (1) to decompose a challenging task into manageable sub-tasks, (2) to update the model outputs",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Published in Transactions on Machine Learning Research (07/2024)\napproximation for s∗. Output queries are encoded into binary discrete syntax vectors. Narayan et al. (2022)\nexplored manually curated and random example selection.\n5.3.3 Multi-turn tasks\nSome papers design pipelines that call LLMs iteratively. We categorize the use-cases for doing so into three\nbuckets: (1) to decompose a challenging task into manageable sub-tasks, (2) to update the model outputs"
        }
    },
    {
        "id": "1e37598560e04f7fb6bb16e7d787a220cd7e82e4",
        "content": "Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\n42",
        "metadata": {
            "source": "2402.17944v4.pdf",
            "text": "Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\n42"
        }
    },
    {
        "id": "94a844d952f06a525fa5dbb147c2b947fb48188d",
        "content": "likelihood based on network factors and the amount of \nconfirmation blocks that the vendor must view before deeming \nthe transaction irreversible have been devised. The related \nnumerical results are also provided for enhanced clarity. A \nsignificant characteristic of the obtained findings is that, in \naddition to the probability estimate, we can compute the number \nof confirmation blocks necessary to ensure the irreversibility of \nthe transaction with a probability as close to 1 as desired. \nThe KRaft algorithm was proposed by Wang et al. [26] in \n2019 which developed based on the Raft algorithm. By \nconstructing a new node connection in the consensus protocol, \nthe KRaft algorithm improves the election and consensus \nprocess. Scalability and transaction throughput improved by \nusing many candidate nodes to replicate logs simultaneously. \nBased on the Raft algorithm, Wang et al. developed the hhRaft \nalgorithm in 2021 [27]. This approach introduced a new monitor",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "likelihood based on network factors and the amount of \nconfirmation blocks that the vendor must view before deeming \nthe transaction irreversible have been devised. The related \nnumerical results are also provided for enhanced clarity. A \nsignificant characteristic of the obtained findings is that, in \naddition to the probability estimate, we can compute the number \nof confirmation blocks necessary to ensure the irreversibility of \nthe transaction with a probability as close to 1 as desired. \nThe KRaft algorithm was proposed by Wang et al. [26] in \n2019 which developed based on the Raft algorithm. By \nconstructing a new node connection in the consensus protocol, \nthe KRaft algorithm improves the election and consensus \nprocess. Scalability and transaction throughput improved by \nusing many candidate nodes to replicate logs simultaneously. \nBased on the Raft algorithm, Wang et al. developed the hhRaft \nalgorithm in 2021 [27]. This approach introduced a new monitor"
        }
    },
    {
        "id": "518a45b63769979e3873eb22a5292446938b493c",
        "content": "by Nir Bitansky [28] in 2020 to create more uncertainty in the \nvoting process by including virtual nodes into the VRF \nconsensus algorithm. The efficiency and low power \nconsumption of DPoS are retained, but the algorithm's \ndecentralization is also increased. Reijsbergen presented the \nLaKAS algorithm in 2021, which is a new form of PoS \nalgorithm that significantly increases transaction speed and \nminimizes the danger of long-range assaults [29]. Changes to the \naccounting rights competition mechanism in the PoW algorithm \nsuggested by Arjomandi-Nezhad [30] were made in 2021, and \nthis modification was based on node donation contributions. The \nright to accounting becomes more readily available the more \nnodes give to the network as a whole. Social taxation situations, \nsuch as educational funds or charitable organizations, are more \nsuited for this algorithm's use. Kara [31] proposed the CW-POW \nalgorithm in 2021, which turned the PoW algorithm's single-",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "by Nir Bitansky [28] in 2020 to create more uncertainty in the \nvoting process by including virtual nodes into the VRF \nconsensus algorithm. The efficiency and low power \nconsumption of DPoS are retained, but the algorithm's \ndecentralization is also increased. Reijsbergen presented the \nLaKAS algorithm in 2021, which is a new form of PoS \nalgorithm that significantly increases transaction speed and \nminimizes the danger of long-range assaults [29]. Changes to the \naccounting rights competition mechanism in the PoW algorithm \nsuggested by Arjomandi-Nezhad [30] were made in 2021, and \nthis modification was based on node donation contributions. The \nright to accounting becomes more readily available the more \nnodes give to the network as a whole. Social taxation situations, \nsuch as educational funds or charitable organizations, are more \nsuited for this algorithm's use. Kara [31] proposed the CW-POW \nalgorithm in 2021, which turned the PoW algorithm's single-"
        }
    },
    {
        "id": "8ffc3f702df660df1436ba2b5ba1a6c54186c368",
        "content": "use a game-theoretic method. The balance between blockchain \nnetwork protocols and trust is a considerable difficulty, and \nnumerous initiatives are being undertaken to progress certain \nareas on both sides. Block reward halving, in which miners' \nincentives are cut in half every four years, is not beneficial to \nparticipants since they want to rely on a network for longer than \na decade. This might be accomplished by drastically raising \ntransaction costs or lowering miners' security requirements (or \nboth). Another option is to create a cost-effective and \nsophisticated game-theory model with a low entry price, robust \nsecurity, and lengthy participant reliance. Dimitri [14] offers a \nblock size-based mining game theory. A high block size raises \nthe cost of computing resources used by a miner to generate a \nhash value indirectly. Additionally, the author establishes that \nthe bitcoin mining process is lucrative when fewer independent",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "use a game-theoretic method. The balance between blockchain \nnetwork protocols and trust is a considerable difficulty, and \nnumerous initiatives are being undertaken to progress certain \nareas on both sides. Block reward halving, in which miners' \nincentives are cut in half every four years, is not beneficial to \nparticipants since they want to rely on a network for longer than \na decade. This might be accomplished by drastically raising \ntransaction costs or lowering miners' security requirements (or \nboth). Another option is to create a cost-effective and \nsophisticated game-theory model with a low entry price, robust \nsecurity, and lengthy participant reliance. Dimitri [14] offers a \nblock size-based mining game theory. A high block size raises \nthe cost of computing resources used by a miner to generate a \nhash value indirectly. Additionally, the author establishes that \nthe bitcoin mining process is lucrative when fewer independent"
        }
    },
    {
        "id": "e8ac8f972bf1b9b00b3875e8a845ec34463ef266",
        "content": "and conclusion in section VI. \nII. RELATED WORKS \nExtensive research [7,8] has developed a consensus method \nin the Blockchain network. The authors in [9] compared \nblockchain consensus protocols. This study is conducted on \n2022 8th International Conference on Web Research (ICWR) | 978-1-6654-6626-4/22/$31.00 ©2022 IEEE | DOI: 10.1109/ICWR54782.2022.9786224\nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n83 \nvarious characteristics, including scalability, security, and \nrewarding validators issues. It has been shown that Proof of \nWork (PoW) is a widely recognized consensus method today. \nWhile blockchain technology has garnered great interest from \nthe fields of computer science and economics, its application of \ngame theory methodologies remains restricted [10]. In \nconsensus procedures, game theory is also generally recognized.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "and conclusion in section VI. \nII. RELATED WORKS \nExtensive research [7,8] has developed a consensus method \nin the Blockchain network. The authors in [9] compared \nblockchain consensus protocols. This study is conducted on \n2022 8th International Conference on Web Research (ICWR) | 978-1-6654-6626-4/22/$31.00 ©2022 IEEE | DOI: 10.1109/ICWR54782.2022.9786224\nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n83 \nvarious characteristics, including scalability, security, and \nrewarding validators issues. It has been shown that Proof of \nWork (PoW) is a widely recognized consensus method today. \nWhile blockchain technology has garnered great interest from \nthe fields of computer science and economics, its application of \ngame theory methodologies remains restricted [10]. In \nconsensus procedures, game theory is also generally recognized."
        }
    },
    {
        "id": "4c6885b004f7eba7979ca7a0c1654b24694bd301",
        "content": "block production process, an attacker must hold a significant \nportion of the total quantity of bitcoin created so far. In most \ncases, the cost of an attack using the PoA protocol would be \nsubstantially greater than with Bitcoin's pure PoW algorithm. \nFurthermore, the PoA protocol is anticipated to achieve other \nadvantageous qualities, such as enhanced network architecture, \nincentives for keeping fully operational nodes, minimal \ntransaction costs, and more efficient energy utilization [22]. \nIV. GAME THEORY \nA game is a tuple with a finite set of players  \u0001 = {1, … , \u0007}; \na finite set  \t\n of strategies for each player \u000b ∈\u0001 . The set of \nstrategy profiles of the game is    \t =\n def \n\t\r × \t\u000f × ⋯\t\u0011. The \npayoff functions of the players are defined as \u0012\n: \t →ℝ . The \nbest response correspondence br\n(\u0019)  as the set of all strategies \nthat maximizes the payoff for player k under profile \u0019 =\n(\u0019\r, … , \u0019\u0011) is shown at (1).  \n \nbr\n(\u0019) =\n def \n\u001b\n\u001c\u001d\u001e\u001f\u001c \n!∈\t\" \u0012\n(#; \u0019%",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "block production process, an attacker must hold a significant \nportion of the total quantity of bitcoin created so far. In most \ncases, the cost of an attack using the PoA protocol would be \nsubstantially greater than with Bitcoin's pure PoW algorithm. \nFurthermore, the PoA protocol is anticipated to achieve other \nadvantageous qualities, such as enhanced network architecture, \nincentives for keeping fully operational nodes, minimal \ntransaction costs, and more efficient energy utilization [22]. \nIV. GAME THEORY \nA game is a tuple with a finite set of players  \u0001 = {1, … , \u0007}; \na finite set  \t\n of strategies for each player \u000b ∈\u0001 . The set of \nstrategy profiles of the game is    \t =\n def \n\t\r × \t\u000f × ⋯\t\u0011. The \npayoff functions of the players are defined as \u0012\n: \t →ℝ . The \nbest response correspondence br\n(\u0019)  as the set of all strategies \nthat maximizes the payoff for player k under profile \u0019 =\n(\u0019\r, … , \u0019\u0011) is shown at (1).  \n \nbr\n(\u0019) =\n def \n\u001b\n\u001c\u001d\u001e\u001f\u001c \n!∈\t\" \u0012\n(#; \u0019%"
        }
    },
    {
        "id": "6c0b6bef33725b404ed3337199122b22e2a818dd",
        "content": "chained bottom topology, with the lowest unit of operation being \na block, ensuring that their algorithms are resistant to double \nspending. When it comes to fault tolerance, the proposed \nconsensus algorithms in [32], [33], and [34] are based on the \nmaximum fault tolerance of BFT. In comparison to the prior \nliterature, our algorithm is high concurrency also the scalability \nof suggested algorithm and resistance to double spending are \nquite strong, while the communication complexity is O(n). \nIn Table I., we compared the property of our proposed \nconsensus protocol to other common protocols. \nACKNOWLEDGMENT \nThe authors like to express their gratitude to the Editors and \nanonymous reviewers for their helpful remarks and ideas. \nTABLE I.  \nCOMPARISON OF OUR PROPOSED PROTOCOL WITH OTHER PROTOCOLS \n \nPOW \nPOS \nPOA \nDPOS \nPBFT \nProposed \nprotocol \nEnergy consumption \nVery \nhigh \nLow \n \nVery \nhigh \nVery \nlow \nVery \nlow \nVery low \nSelfish Mining and Majority-Attack \n \nYes \n \n \nYes",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "chained bottom topology, with the lowest unit of operation being \na block, ensuring that their algorithms are resistant to double \nspending. When it comes to fault tolerance, the proposed \nconsensus algorithms in [32], [33], and [34] are based on the \nmaximum fault tolerance of BFT. In comparison to the prior \nliterature, our algorithm is high concurrency also the scalability \nof suggested algorithm and resistance to double spending are \nquite strong, while the communication complexity is O(n). \nIn Table I., we compared the property of our proposed \nconsensus protocol to other common protocols. \nACKNOWLEDGMENT \nThe authors like to express their gratitude to the Editors and \nanonymous reviewers for their helpful remarks and ideas. \nTABLE I.  \nCOMPARISON OF OUR PROPOSED PROTOCOL WITH OTHER PROTOCOLS \n \nPOW \nPOS \nPOA \nDPOS \nPBFT \nProposed \nprotocol \nEnergy consumption \nVery \nhigh \nLow \n \nVery \nhigh \nVery \nlow \nVery \nlow \nVery low \nSelfish Mining and Majority-Attack \n \nYes \n \n \nYes"
        }
    },
    {
        "id": "215de355fb51f1dc5d2effe773a517d7a399c6a3",
        "content": "2022 8th International Conference on Web Research (ICWR) \n82 \n978-1-6654-6626-4/22/$31.00 ©2022 IEEE \nA Novel Consensus Protocol in Blockchain Network \nbased on Proof of Activity Protocol and Game Theory \nZahra Boreiri* \nDepartment of Engineering Science, College of Engineering \nUniversity of Tehran \nTehran, Iran \nzahra.boreiri@ut.ac.ir \n \nAlireza Norouzi Azad \nDepartment of Engineering Science, College of Engineering \nUniversity of Tehran \nTehran, Iran \nalireza.norouzi@ut.ac.ir\nAbstract— Blockchain networks are already extensively used \nin various applications because of their increased security. The \nunique characteristics of blockchain technology, such as \ndecentralized, peer-to-peer, and invariable distributed ledger \nqualities, make it appealing to researchers, academics, and \nindustry. The consensus protocol is a fundamental part of \nblockchain technology. PoW (Proof of Work) or fixed-validator \nconsensus protocols comprise most of the existing consensus",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "2022 8th International Conference on Web Research (ICWR) \n82 \n978-1-6654-6626-4/22/$31.00 ©2022 IEEE \nA Novel Consensus Protocol in Blockchain Network \nbased on Proof of Activity Protocol and Game Theory \nZahra Boreiri* \nDepartment of Engineering Science, College of Engineering \nUniversity of Tehran \nTehran, Iran \nzahra.boreiri@ut.ac.ir \n \nAlireza Norouzi Azad \nDepartment of Engineering Science, College of Engineering \nUniversity of Tehran \nTehran, Iran \nalireza.norouzi@ut.ac.ir\nAbstract— Blockchain networks are already extensively used \nin various applications because of their increased security. The \nunique characteristics of blockchain technology, such as \ndecentralized, peer-to-peer, and invariable distributed ledger \nqualities, make it appealing to researchers, academics, and \nindustry. The consensus protocol is a fundamental part of \nblockchain technology. PoW (Proof of Work) or fixed-validator \nconsensus protocols comprise most of the existing consensus"
        }
    },
    {
        "id": "51658680a6a3d7133c46842c547e608d9df5a52e",
        "content": "POW \nPOS \nPOA \nDPOS \nPBFT \nProposed \nprotocol \nEnergy consumption \nVery \nhigh \nLow \n \nVery \nhigh \nVery \nlow \nVery \nlow \nVery low \nSelfish Mining and Majority-Attack \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nNo \n \nPossibility of forming mining pools \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nNo \n \nBlock creation speed \nVery \nlow \nHigh \n \nVery \nlow \nVery \nhigh \nHigh \n \nVery high \nThe degree of decentralization \nVery \nhigh \nVery \nhigh \nHigh \n \nHigh \n \nVery \nlow \nVery low \nRequires advanced hardware \n \nYes \n \n \nNo \n \n \nYes \n \n \nNo \n \n \nNo \n \n \nNo \n \n \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n87 \nREFERENCES \n[1] D. Xu, L. Xiao, L. Sun and M. Lei, “Game theoretic study on blockchain \nbased secure edge networks,” 2017 IEEE/CIC International Conference \non Communications in China (ICCC), pp. 1–5, 2017. \n[2] Z. Zheng, S. Xie, H. Dai, X. Chen and H. Wang, “An Overview of",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "POW \nPOS \nPOA \nDPOS \nPBFT \nProposed \nprotocol \nEnergy consumption \nVery \nhigh \nLow \n \nVery \nhigh \nVery \nlow \nVery \nlow \nVery low \nSelfish Mining and Majority-Attack \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nNo \n \nPossibility of forming mining pools \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nYes \n \n \nNo \n \nBlock creation speed \nVery \nlow \nHigh \n \nVery \nlow \nVery \nhigh \nHigh \n \nVery high \nThe degree of decentralization \nVery \nhigh \nVery \nhigh \nHigh \n \nHigh \n \nVery \nlow \nVery low \nRequires advanced hardware \n \nYes \n \n \nNo \n \n \nYes \n \n \nNo \n \n \nNo \n \n \nNo \n \n \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n87 \nREFERENCES \n[1] D. Xu, L. Xiao, L. Sun and M. Lei, “Game theoretic study on blockchain \nbased secure edge networks,” 2017 IEEE/CIC International Conference \non Communications in China (ICCC), pp. 1–5, 2017. \n[2] Z. Zheng, S. Xie, H. Dai, X. Chen and H. Wang, “An Overview of"
        }
    },
    {
        "id": "707def3d0b73632d012c99e6e280fc40c07dfcf6",
        "content": "203-224, 2022. \n[26] R. Wang, L. Zhang, Q. Xu and H. Zhou, “K-Bucket Based Raft-Like \nConsensus Algorithm for Permissioned Blockchain,” 2019 IEEE 25th \nInternational Conference on Parallel and Distributed Systems (ICPADS), \npp. 996-999, 2019. \n[27] Y. Wang, S. Li, L. Xu, L. Xu, “Improved Raft Consensus Algorithm in \nHigh Real-Time and Highly Adversarial Environment,” International \nConference on Web Information Systems and Applications; Springer: \nCham, Switzerland, pp. 718–726, 2021. \n[28] B. Nir, “Verifiable Random Functions from Non-interactive Witness-\nIndistinguishable Proofs,” J. Cryptol, vol. 33, 459–493, 2020. \n[29] D. Reijsbergen et all., “A Probabilistic Proof-of-Stake Protocol,” \nProceedings \n2021, \nNetwork \nand \nDistributed \nSystem \nSecurity \nSymposium; Internet Society: Reston, VA, USA, 2021. \n[30] A. Arjomandi-Nezhad, M. Fotuhi-Firuzabad, A. Dorri, P. Dehghanian, \n“Proof of humanity: A tax-aware society-centric consensus algorithm for",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "203-224, 2022. \n[26] R. Wang, L. Zhang, Q. Xu and H. Zhou, “K-Bucket Based Raft-Like \nConsensus Algorithm for Permissioned Blockchain,” 2019 IEEE 25th \nInternational Conference on Parallel and Distributed Systems (ICPADS), \npp. 996-999, 2019. \n[27] Y. Wang, S. Li, L. Xu, L. Xu, “Improved Raft Consensus Algorithm in \nHigh Real-Time and Highly Adversarial Environment,” International \nConference on Web Information Systems and Applications; Springer: \nCham, Switzerland, pp. 718–726, 2021. \n[28] B. Nir, “Verifiable Random Functions from Non-interactive Witness-\nIndistinguishable Proofs,” J. Cryptol, vol. 33, 459–493, 2020. \n[29] D. Reijsbergen et all., “A Probabilistic Proof-of-Stake Protocol,” \nProceedings \n2021, \nNetwork \nand \nDistributed \nSystem \nSecurity \nSymposium; Internet Society: Reston, VA, USA, 2021. \n[30] A. Arjomandi-Nezhad, M. Fotuhi-Firuzabad, A. Dorri, P. Dehghanian, \n“Proof of humanity: A tax-aware society-centric consensus algorithm for"
        }
    },
    {
        "id": "67bca9726f90544bcf0019e5ed521d649f976f58",
        "content": "industry. The consensus protocol is a fundamental part of \nblockchain technology. PoW (Proof of Work) or fixed-validator \nconsensus protocols comprise most of the existing consensus \nmechanisms. However, the tremendous computational effort \nrequired for PoW leads to excessive energy and computing \nresource usage. On the other hand, Fixed-validator protocols \nvalidate new blocks by a fixed, static set of validators, allowing \nattackers to execute multiple attacks against these validators. In \nthis article, we proposed a novel consensus protocol base on the \nProof of Activity protocol and game theory. Our consensus \nprotocol is efficient in energy consumption and can deal with \nselfish mining and majority-attack. \nKeywords—Blockchain; Game theory; Consensus protocols; \nProof of Activity protocol; Selfish mining; Majority-attack; \nI. INTRODUCTION \nSatoshi Nakamoto first proposed the Blockchain concept in \n2008, an invariable timestamp ledger of blocks [1]. In a",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "industry. The consensus protocol is a fundamental part of \nblockchain technology. PoW (Proof of Work) or fixed-validator \nconsensus protocols comprise most of the existing consensus \nmechanisms. However, the tremendous computational effort \nrequired for PoW leads to excessive energy and computing \nresource usage. On the other hand, Fixed-validator protocols \nvalidate new blocks by a fixed, static set of validators, allowing \nattackers to execute multiple attacks against these validators. In \nthis article, we proposed a novel consensus protocol base on the \nProof of Activity protocol and game theory. Our consensus \nprotocol is efficient in energy consumption and can deal with \nselfish mining and majority-attack. \nKeywords—Blockchain; Game theory; Consensus protocols; \nProof of Activity protocol; Selfish mining; Majority-attack; \nI. INTRODUCTION \nSatoshi Nakamoto first proposed the Blockchain concept in \n2008, an invariable timestamp ledger of blocks [1]. In a"
        }
    },
    {
        "id": "4233f2de0369300cea2875d1d14ee901d04fd529",
        "content": "wants, the N-1 signatures of the other derived stakeholders, and \nher own signature for the hash of the whole block. \nIn the fifth step, The Nth stakeholder broadcasts the wrapped \nblock to the network, and when the other nodes discover that it \nis genuine according to the above, they consider it a legitimate \nextension of the blockchain. The nodes attempt to extend the \nmost extended branch of the blockchain they are aware of, where \n\"longest\" is measured in PoW difficulty, similar to Bitcoin. \nFinally, the miner and the N fortunate stakeholders split the \nfees from the transactions that the Nth stakeholder earned [22]. \nThe PoA protocol tries to decentralize the authority that \nsynchronizes transactions in a significant way. To dominate the \nblock production process, an attacker must hold a significant \nportion of the total quantity of bitcoin created so far. In most \ncases, the cost of an attack using the PoA protocol would be",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "wants, the N-1 signatures of the other derived stakeholders, and \nher own signature for the hash of the whole block. \nIn the fifth step, The Nth stakeholder broadcasts the wrapped \nblock to the network, and when the other nodes discover that it \nis genuine according to the above, they consider it a legitimate \nextension of the blockchain. The nodes attempt to extend the \nmost extended branch of the blockchain they are aware of, where \n\"longest\" is measured in PoW difficulty, similar to Bitcoin. \nFinally, the miner and the N fortunate stakeholders split the \nfees from the transactions that the Nth stakeholder earned [22]. \nThe PoA protocol tries to decentralize the authority that \nsynchronizes transactions in a significant way. To dominate the \nblock production process, an attacker must hold a significant \nportion of the total quantity of bitcoin created so far. In most \ncases, the cost of an attack using the PoA protocol would be"
        }
    },
    {
        "id": "36c7f1657cbfee1fa41989a118b69743a7e04842",
        "content": "Communications Letters, vol. 7, no. 5, pp. 760–763, Oct. 2018. \n[19] Y. Jiao, P. Wang, D. Niyato and Z. Xiong, “Social Welfare Maximization \nAuction in Edge Computing Resource Allocation for Mobile \nBlockchain,” 2018 IEEE International Conference on Communications \n(ICC), 2018. \n[20] S. Barber, X. Boyen, E. Shi, E. Uzun, “Bitter to Better — How to Make \nBitcoin a Better Currency,” In: Keromytis, A.D. (eds) Financial \nCryptography and Data Security, Computer Science, vol. 7397, pp. 399–\n414, 2012. \n[21] M. Rosenfeld, “Analysis of Hashrate-Based Double Spending,” arXiv, \n2014. \n[22] I. Bentov, Ch. Lee, A. Mizrahi, M. Rosenfeld, “Proof of Activity: \nExtending Bitcoin's Proof of Work via Proof of Stake [Extended \nAbstract]y,” SIGMETRICS Perform, Eval. Rev. 42, 3, pp. 34–37, 2014. \n[23] S. Durand, B. Gaujal, “Complexity and Optimality of the Best Response \nAlgorithm in Random Potential Games,” Symposium on Algorithmic \nGame Theory (SAGT), pp. 40–51, 2016.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Communications Letters, vol. 7, no. 5, pp. 760–763, Oct. 2018. \n[19] Y. Jiao, P. Wang, D. Niyato and Z. Xiong, “Social Welfare Maximization \nAuction in Edge Computing Resource Allocation for Mobile \nBlockchain,” 2018 IEEE International Conference on Communications \n(ICC), 2018. \n[20] S. Barber, X. Boyen, E. Shi, E. Uzun, “Bitter to Better — How to Make \nBitcoin a Better Currency,” In: Keromytis, A.D. (eds) Financial \nCryptography and Data Security, Computer Science, vol. 7397, pp. 399–\n414, 2012. \n[21] M. Rosenfeld, “Analysis of Hashrate-Based Double Spending,” arXiv, \n2014. \n[22] I. Bentov, Ch. Lee, A. Mizrahi, M. Rosenfeld, “Proof of Activity: \nExtending Bitcoin's Proof of Work via Proof of Stake [Extended \nAbstract]y,” SIGMETRICS Perform, Eval. Rev. 42, 3, pp. 34–37, 2014. \n[23] S. Durand, B. Gaujal, “Complexity and Optimality of the Best Response \nAlgorithm in Random Potential Games,” Symposium on Algorithmic \nGame Theory (SAGT), pp. 40–51, 2016."
        }
    },
    {
        "id": "78adb2155d2d69ead12da33730a6f9a2b9b0fd9e",
        "content": "Symposium; Internet Society: Reston, VA, USA, 2021. \n[30] A. Arjomandi-Nezhad, M. Fotuhi-Firuzabad, A. Dorri, P. Dehghanian, \n“Proof of humanity: A tax-aware society-centric consensus algorithm for \nBlockchains,” Peer—Peer Netw. Appl, vol. 14, pp. 3634–3646, 2021. \n[31] M. Kara et all., “A Compute and Wait in PoW (CW-PoW) Consensus \nAlgorithm for Preserving Energy Consumption,” Appl. Sci , 11, 6750, \n2021. \n[32] I. Abraham, D. Malkhi, and K. Nayak, “Sync HotStuff: simple and \npractical synchronous state machine replication,” in Proceedings of the \n41th Symposium on Security and Privacy, Piscataway, May 2020. \n[33] M. T. d. Oliveira, L. H. A. Reis, D. S. V. Medeiros, R. C. Carrano, S. D. \nOlabarriaga, and D. M. F. Mattos, “Blockchain reputation-based \nconsensus: a scalable and resilient mechanism for distributed mistrusting \napplications,” Computer Networks, vol. 179, no. 10, pp. 107367–107380, \n2020. \n[34] N. A. Lin, Z. H. Chen, and G. K. Liu, “Mechanism for proof-of-reputation",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Symposium; Internet Society: Reston, VA, USA, 2021. \n[30] A. Arjomandi-Nezhad, M. Fotuhi-Firuzabad, A. Dorri, P. Dehghanian, \n“Proof of humanity: A tax-aware society-centric consensus algorithm for \nBlockchains,” Peer—Peer Netw. Appl, vol. 14, pp. 3634–3646, 2021. \n[31] M. Kara et all., “A Compute and Wait in PoW (CW-PoW) Consensus \nAlgorithm for Preserving Energy Consumption,” Appl. Sci , 11, 6750, \n2021. \n[32] I. Abraham, D. Malkhi, and K. Nayak, “Sync HotStuff: simple and \npractical synchronous state machine replication,” in Proceedings of the \n41th Symposium on Security and Privacy, Piscataway, May 2020. \n[33] M. T. d. Oliveira, L. H. A. Reis, D. S. V. Medeiros, R. C. Carrano, S. D. \nOlabarriaga, and D. M. F. Mattos, “Blockchain reputation-based \nconsensus: a scalable and resilient mechanism for distributed mistrusting \napplications,” Computer Networks, vol. 179, no. 10, pp. 107367–107380, \n2020. \n[34] N. A. Lin, Z. H. Chen, and G. K. Liu, “Mechanism for proof-of-reputation"
        }
    },
    {
        "id": "1ae92db775e1282c9106d93fa0defa0f31a3aa49",
        "content": "based secure edge networks,” 2017 IEEE/CIC International Conference \non Communications in China (ICCC), pp. 1–5, 2017. \n[2] Z. Zheng, S. Xie, H. Dai, X. Chen and H. Wang, “An Overview of \nBlockchain Technology: Architecture, Consensus, and Future Trends,” \n2017 IEEE International Congress on Big Data (BigData Congress), pp. \n557–564, 2017. \n[3] M. Pilkington, “Blockchain Technology: Principles and Applications,” \nResearch Handbook on Digital Transformations, 2016. \n[4] B. Johnson, A. Laszka, J. Grossklags, M. Vasek, T. Moore, “Game-\ntheoretic analysis of DDoS attacks against bitcoin mining pools” \nFinancial Cryptography and Data Security - FC 2014 Workshops, \nBITCOIN and WAHC 2014, Springer Verlag, Vol. 8438, pp. 72–86, \n2014. \n[5] T. K. Mackey, G. Nayyar, “A review of existing and emerging digital \ntechnologies to combat the global trade in fake medicines,” Expert \nopinion on drug safety, 16(5), 587–602, 2017.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "based secure edge networks,” 2017 IEEE/CIC International Conference \non Communications in China (ICCC), pp. 1–5, 2017. \n[2] Z. Zheng, S. Xie, H. Dai, X. Chen and H. Wang, “An Overview of \nBlockchain Technology: Architecture, Consensus, and Future Trends,” \n2017 IEEE International Congress on Big Data (BigData Congress), pp. \n557–564, 2017. \n[3] M. Pilkington, “Blockchain Technology: Principles and Applications,” \nResearch Handbook on Digital Transformations, 2016. \n[4] B. Johnson, A. Laszka, J. Grossklags, M. Vasek, T. Moore, “Game-\ntheoretic analysis of DDoS attacks against bitcoin mining pools” \nFinancial Cryptography and Data Security - FC 2014 Workshops, \nBITCOIN and WAHC 2014, Springer Verlag, Vol. 8438, pp. 72–86, \n2014. \n[5] T. K. Mackey, G. Nayyar, “A review of existing and emerging digital \ntechnologies to combat the global trade in fake medicines,” Expert \nopinion on drug safety, 16(5), 587–602, 2017."
        }
    },
    {
        "id": "3e33c4de6c104a4ccede0cdfac709be4632654f7",
        "content": "Byzantine fault tolerance consensus algorithm in permission \nblockchain network which splits each node into trust nodes and \ndefective nodes. Voting is restricted to nodes with good \nreputations. In addition, the consensus process regards the node \nthat was used to identify the trust as having a high reputation. \nThe master node is chosen by a majority of voting values. This \nadaptive PBFT algorithm performs well in terms of long-term \nperiodicity, \ngreater \nscalability, \nand \ndecreased \ntotal \ncommunication costs. Kovalchuk et al. [25], suggested a non-\nstandard procedure for approving a transaction by a vendor to \nstrengthen its security which it must wait for a specified number \nof confirmation blocks to be built, so to speak, under its \nsupervision. Methods for calculating the upper estimates of this \nlikelihood based on network factors and the amount of \nconfirmation blocks that the vendor must view before deeming \nthe transaction irreversible have been devised. The related",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Byzantine fault tolerance consensus algorithm in permission \nblockchain network which splits each node into trust nodes and \ndefective nodes. Voting is restricted to nodes with good \nreputations. In addition, the consensus process regards the node \nthat was used to identify the trust as having a high reputation. \nThe master node is chosen by a majority of voting values. This \nadaptive PBFT algorithm performs well in terms of long-term \nperiodicity, \ngreater \nscalability, \nand \ndecreased \ntotal \ncommunication costs. Kovalchuk et al. [25], suggested a non-\nstandard procedure for approving a transaction by a vendor to \nstrengthen its security which it must wait for a specified number \nof confirmation blocks to be built, so to speak, under its \nsupervision. Methods for calculating the upper estimates of this \nlikelihood based on network factors and the amount of \nconfirmation blocks that the vendor must view before deeming \nthe transaction irreversible have been devised. The related"
        }
    },
    {
        "id": "6c62c35905a255f8f17e57e2ecbfec378d1075f0",
        "content": "Algorithm game theory and supervised machine learning \nalgorithms can be used to discover abnormalities as a result of \nthis monitoring. In order to recognize different types of assaults, \nsuch as majority population size attacks, DoS, DDoS, etc., \nsupervised machine learning is useful. Many alternative game \ntheory-based models are used [18,19]. Among the evolutionary \ngame models studied are hierarchical games and auctions. \nHowever, little research has been conducted on a single-game \nmodel for both resourceful and resource-constrained device \nnetworks. The primary difficulty in developing this model is the \nresource-dependent and challenge-variable technique of \nrewarding point-based game theory. \nIndra Navaroj et al. [24], suggested an adaptive practical \nByzantine fault tolerance consensus algorithm in permission \nblockchain network which splits each node into trust nodes and \ndefective nodes. Voting is restricted to nodes with good",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Algorithm game theory and supervised machine learning \nalgorithms can be used to discover abnormalities as a result of \nthis monitoring. In order to recognize different types of assaults, \nsuch as majority population size attacks, DoS, DDoS, etc., \nsupervised machine learning is useful. Many alternative game \ntheory-based models are used [18,19]. Among the evolutionary \ngame models studied are hierarchical games and auctions. \nHowever, little research has been conducted on a single-game \nmodel for both resourceful and resource-constrained device \nnetworks. The primary difficulty in developing this model is the \nresource-dependent and challenge-variable technique of \nrewarding point-based game theory. \nIndra Navaroj et al. [24], suggested an adaptive practical \nByzantine fault tolerance consensus algorithm in permission \nblockchain network which splits each node into trust nodes and \ndefective nodes. Voting is restricted to nodes with good"
        }
    },
    {
        "id": "8a962b4a8191d9b29c37068f2fef2666a399df4a",
        "content": "applications,” Computer Networks, vol. 179, no. 10, pp. 107367–107380, \n2020. \n[34] N. A. Lin, Z. H. Chen, and G. K. Liu, “Mechanism for proof-of-reputation \nconsensus for blockchain validator nodes,” Journal of Xidian UniverSity, \nvol. 47, no. 5, pp. 61–66, 2020. \n[35] H. Liu, S. Li, and W. Lv, “Master-slave multiple-blockchain consensus \nbased on credibility,” Journal of Nanjing University of Science and \nTechnology, vol. 44, no. 3, pp. 325–331, 2020.   \n \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "applications,” Computer Networks, vol. 179, no. 10, pp. 107367–107380, \n2020. \n[34] N. A. Lin, Z. H. Chen, and G. K. Liu, “Mechanism for proof-of-reputation \nconsensus for blockchain validator nodes,” Journal of Xidian UniverSity, \nvol. 47, no. 5, pp. 61–66, 2020. \n[35] H. Liu, S. Li, and W. Lv, “Master-slave multiple-blockchain consensus \nbased on credibility,” Journal of Nanjing University of Science and \nTechnology, vol. 44, no. 3, pp. 325–331, 2020.   \n \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply."
        }
    },
    {
        "id": "b78e1a03dbfb4b52ae36d1b0e94fec1c3f4e75dd",
        "content": "such as educational funds or charitable organizations, are more \nsuited for this algorithm's use. Kara [31] proposed the CW-POW \nalgorithm in 2021, which turned the PoW algorithm's single-\nround workload proof into a multi-round problem solution \ngame, removing nodes that failed to solve the solution round by \nround, improving the algorithm's resistance to attacks and \nlowering its energy consumption significantly. \nIII. PROOF OF ACTIVITY \nDirect attacks against Bitcoin's pure PoW system may be \ncarried out in a variety of ways. The infamous >50% hashpower \nattack, in which the attacker invests in hardware equipment \n(ASIC) to gain more PoW hashpower than all other Bitcoin \nminers combined [8,20,21], is one kind of those attack. Such an \nattacker may then double-spend by reversing the recent ledger \nhistory to defraud merchants, or launch a PoW-denial-of-service \n(PoW-DoS) assault by refusing to include transactions in the",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "such as educational funds or charitable organizations, are more \nsuited for this algorithm's use. Kara [31] proposed the CW-POW \nalgorithm in 2021, which turned the PoW algorithm's single-\nround workload proof into a multi-round problem solution \ngame, removing nodes that failed to solve the solution round by \nround, improving the algorithm's resistance to attacks and \nlowering its energy consumption significantly. \nIII. PROOF OF ACTIVITY \nDirect attacks against Bitcoin's pure PoW system may be \ncarried out in a variety of ways. The infamous >50% hashpower \nattack, in which the attacker invests in hardware equipment \n(ASIC) to gain more PoW hashpower than all other Bitcoin \nminers combined [8,20,21], is one kind of those attack. Such an \nattacker may then double-spend by reversing the recent ledger \nhistory to defraud merchants, or launch a PoW-denial-of-service \n(PoW-DoS) assault by refusing to include transactions in the"
        }
    },
    {
        "id": "51279a74b359be70ced20273fa2180143378c178",
        "content": "potential dangers [6]. \nExisting implementations of blockchain technology make \nuse of a variety of consensus techniques, including Proof of \nElapsed Time (PET), Proof of Stake Model (PSM), Stellar \nConsensus Protocol (SCP), Cross Fault Tolerance (XFT), \nPractical Byzantine Fault Tolerance Algorithm (PBFTA), \nFederated Byzantine Agreement (FBA), Ripple Consensus \nProtocol (RCP), Byzantine Fault Tolerance Algorithm (BFTA), \netc. \nThe rest of this paper is organized as follows. In section II, \nwe review some of the work related to the application of game \ntheory in blockchain networks. Section III discusses how the \nProof of Activity protocol works. Section IV presents the basic \nconcepts of game theory. In section V, the proposed confusing \nprotocol is discussed. Finally, the paper ends with summarizing \nand conclusion in section VI. \nII. RELATED WORKS \nExtensive research [7,8] has developed a consensus method \nin the Blockchain network. The authors in [9] compared",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "potential dangers [6]. \nExisting implementations of blockchain technology make \nuse of a variety of consensus techniques, including Proof of \nElapsed Time (PET), Proof of Stake Model (PSM), Stellar \nConsensus Protocol (SCP), Cross Fault Tolerance (XFT), \nPractical Byzantine Fault Tolerance Algorithm (PBFTA), \nFederated Byzantine Agreement (FBA), Ripple Consensus \nProtocol (RCP), Byzantine Fault Tolerance Algorithm (BFTA), \netc. \nThe rest of this paper is organized as follows. In section II, \nwe review some of the work related to the application of game \ntheory in blockchain networks. Section III discusses how the \nProof of Activity protocol works. Section IV presents the basic \nconcepts of game theory. In section V, the proposed confusing \nprotocol is discussed. Finally, the paper ends with summarizing \nand conclusion in section VI. \nII. RELATED WORKS \nExtensive research [7,8] has developed a consensus method \nin the Blockchain network. The authors in [9] compared"
        }
    },
    {
        "id": "ca6fe7b5878348cfa2b56913d8c86c92b4c832af",
        "content": "common is financial gain. The game theory can be a valuable \nweapon in the fight against such assaults and in strengthening \nblockchain security. \nA blockchain is a distributed, public ledger that consists of \nchained blocks, including several transactions. To ensure \nsecurity, these blocks are verified worldwide and publicly. This \nvalidation must be conducted independently of a central \nauthority. In order to verify, share, and synchronize the blocks \namong nodes, a decentralized, peer-to-peer, and distributed \nconsensus process is used [5]. The consensus protocol is a \nfundamental element of blockchain technology. Before a block \nis added to the public ledger, a consensus mechanism on the \nblockchain must ensure that the whole network nodes agree on \nthe validity of that block.  \nAdditionally, the consensus protocol ensures that the order \nof blocks in the chain of each node in the blockchain network is \nthe same. This guarantee is significant because blockchains are",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "common is financial gain. The game theory can be a valuable \nweapon in the fight against such assaults and in strengthening \nblockchain security. \nA blockchain is a distributed, public ledger that consists of \nchained blocks, including several transactions. To ensure \nsecurity, these blocks are verified worldwide and publicly. This \nvalidation must be conducted independently of a central \nauthority. In order to verify, share, and synchronize the blocks \namong nodes, a decentralized, peer-to-peer, and distributed \nconsensus process is used [5]. The consensus protocol is a \nfundamental element of blockchain technology. Before a block \nis added to the public ledger, a consensus mechanism on the \nblockchain must ensure that the whole network nodes agree on \nthe validity of that block.  \nAdditionally, the consensus protocol ensures that the order \nof blocks in the chain of each node in the blockchain network is \nthe same. This guarantee is significant because blockchains are"
        }
    },
    {
        "id": "538cd0eb67cb3bdb26c182a614e2868c22da9461",
        "content": "Proof of Activity protocol; Selfish mining; Majority-attack; \nI. INTRODUCTION \nSatoshi Nakamoto first proposed the Blockchain concept in \n2008, an invariable timestamp ledger of blocks [1]. In a \ndistributed way, these blocks include records of transactions. \nPersonal data, information regarding consensus protocols, \npayment history, and other information are all included in these \ntransaction records. Blockchain technology in recent years has \nreceived a great deal of attention in various industrial sectors. \nThe increasing number of cryptocurrencies being embraced \ndaily demonstrates their significance. As of right now, over 2200 \ncryptocurrencies exist [1,2]. \nBy eliminating centralized authority, every participant \ninvolved in the blockchain can share a distributed digital ledger \nof transactions. Users can contribute new blocks to the \nblockchain by completing computationally complex but readily \nverifiable challenges. The consensus method employed in the",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Proof of Activity protocol; Selfish mining; Majority-attack; \nI. INTRODUCTION \nSatoshi Nakamoto first proposed the Blockchain concept in \n2008, an invariable timestamp ledger of blocks [1]. In a \ndistributed way, these blocks include records of transactions. \nPersonal data, information regarding consensus protocols, \npayment history, and other information are all included in these \ntransaction records. Blockchain technology in recent years has \nreceived a great deal of attention in various industrial sectors. \nThe increasing number of cryptocurrencies being embraced \ndaily demonstrates their significance. As of right now, over 2200 \ncryptocurrencies exist [1,2]. \nBy eliminating centralized authority, every participant \ninvolved in the blockchain can share a distributed digital ledger \nof transactions. Users can contribute new blocks to the \nblockchain by completing computationally complex but readily \nverifiable challenges. The consensus method employed in the"
        }
    },
    {
        "id": "6b24b0326e8b37d302455cea9ea635a5e7b17a97",
        "content": "structure and there is no necessary sequence link between the \nproduction of blocks. Because the algorithms proposed in [33] \nand [34] are based on standard BFT, there is no scalability \nincrease. Point-to-point propagation is used in all algorithms in \n[33], [34], and [35], hence the communication complexity is \nO(n2). The communication complexity in the literature [32] is \nO(n) since it utilizes a star topology. Concurrency is low in [32], \n[33], and [34], which all employ the classic chain structure. In \ncomparison, the proposed protocol in [32] uses a pipeline \napproach and does not impose a time constraint on the \nproduction of the front and back blocks, allowing for more \nconcurrency. The authors of [32], [33], [35], and [34] utilized a \nchained bottom topology, with the lowest unit of operation being \na block, ensuring that their algorithms are resistant to double \nspending. When it comes to fault tolerance, the proposed",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "structure and there is no necessary sequence link between the \nproduction of blocks. Because the algorithms proposed in [33] \nand [34] are based on standard BFT, there is no scalability \nincrease. Point-to-point propagation is used in all algorithms in \n[33], [34], and [35], hence the communication complexity is \nO(n2). The communication complexity in the literature [32] is \nO(n) since it utilizes a star topology. Concurrency is low in [32], \n[33], and [34], which all employ the classic chain structure. In \ncomparison, the proposed protocol in [32] uses a pipeline \napproach and does not impose a time constraint on the \nproduction of the front and back blocks, allowing for more \nconcurrency. The authors of [32], [33], [35], and [34] utilized a \nchained bottom topology, with the lowest unit of operation being \na block, ensuring that their algorithms are resistant to double \nspending. When it comes to fault tolerance, the proposed"
        }
    },
    {
        "id": "8d414c904d4b9706a1b38b422b65c6d8b19eb161",
        "content": "[23] S. Durand, B. Gaujal, “Complexity and Optimality of the Best Response \nAlgorithm in Random Potential Games,” Symposium on Algorithmic \nGame Theory (SAGT), pp. 40–51, 2016. \n[24] G. Indra Navaroj, E. Golden Julie, Y. Harold Robinson, “Adaptive \npractical Byzantine fault tolerance consensus algorithm in permission \nblockchain network,” International Journal of Web and Grid Services, \nVol. 18, No. 1, 2021. \n[25] L. Kovalchuk, R. Oliynykov, Y. Bespalov, M. Rodinko, “Comparative \nAnalysis of Consensus Algorithms Using a Directed Acyclic Graph \nInstead of a Blockchain, and the Construction of Security Estimates of \nSpectre Protocol Against Double Spend Attack,” Information Security \nTechnologies in the Decentralized Distributed Networks. Lecture Notes \non Data Engineering and Communications Technologies, vol 115, pp. \n203-224, 2022. \n[26] R. Wang, L. Zhang, Q. Xu and H. Zhou, “K-Bucket Based Raft-Like \nConsensus Algorithm for Permissioned Blockchain,” 2019 IEEE 25th",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "[23] S. Durand, B. Gaujal, “Complexity and Optimality of the Best Response \nAlgorithm in Random Potential Games,” Symposium on Algorithmic \nGame Theory (SAGT), pp. 40–51, 2016. \n[24] G. Indra Navaroj, E. Golden Julie, Y. Harold Robinson, “Adaptive \npractical Byzantine fault tolerance consensus algorithm in permission \nblockchain network,” International Journal of Web and Grid Services, \nVol. 18, No. 1, 2021. \n[25] L. Kovalchuk, R. Oliynykov, Y. Bespalov, M. Rodinko, “Comparative \nAnalysis of Consensus Algorithms Using a Directed Acyclic Graph \nInstead of a Blockchain, and the Construction of Security Estimates of \nSpectre Protocol Against Double Spend Attack,” Information Security \nTechnologies in the Decentralized Distributed Networks. Lecture Notes \non Data Engineering and Communications Technologies, vol 115, pp. \n203-224, 2022. \n[26] R. Wang, L. Zhang, Q. Xu and H. Zhou, “K-Bucket Based Raft-Like \nConsensus Algorithm for Permissioned Blockchain,” 2019 IEEE 25th"
        }
    },
    {
        "id": "6d7b079d00b9845ce8c0079ca7312e99bf1a0ee4",
        "content": "input to the follow-the-Satoshi function. \nIn the fourth step, every online stakeholder examines \nwhether the miner is broadcasted empty block header is \nlegitimate, which means it includes the hash of the previous \nblock and satisfies the current difficulty. Following validation, \nthe stakeholder determines if she is one of the N fortunate \nstakeholders for this block. When the first N-1 fortunate \nstakeholders realize that the block is derived from them, they \nsign the hash of this empty block header with the private key \nthat controls their derived Satoshi and broadcast their signature \nto the network. When the Nth stakeholder notices that the block \nderives her, she constructs a wrapped block that expands the \nempty block header by containing as many transactions as she \nwants, the N-1 signatures of the other derived stakeholders, and \nher own signature for the hash of the whole block. \nIn the fifth step, The Nth stakeholder broadcasts the wrapped",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "input to the follow-the-Satoshi function. \nIn the fourth step, every online stakeholder examines \nwhether the miner is broadcasted empty block header is \nlegitimate, which means it includes the hash of the previous \nblock and satisfies the current difficulty. Following validation, \nthe stakeholder determines if she is one of the N fortunate \nstakeholders for this block. When the first N-1 fortunate \nstakeholders realize that the block is derived from them, they \nsign the hash of this empty block header with the private key \nthat controls their derived Satoshi and broadcast their signature \nto the network. When the Nth stakeholder notices that the block \nderives her, she constructs a wrapped block that expands the \nempty block header by containing as many transactions as she \nwants, the N-1 signatures of the other derived stakeholders, and \nher own signature for the hash of the whole block. \nIn the fifth step, The Nth stakeholder broadcasts the wrapped"
        }
    },
    {
        "id": "b3bffad9c7d57399635da1eacf86cc895fa52fe4",
        "content": "best response correspondence br\n(\u0019)  as the set of all strategies \nthat maximizes the payoff for player k under profile \u0019 =\n(\u0019\r, … , \u0019\u0011) is shown at (1).  \n \nbr\n(\u0019) =\n def \n\u001b\n\u001c\u001d\u001e\u001f\u001c \n!∈\t\" \u0012\n(#; \u0019%\n)&.                                (1) \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n85 \nA Nash equilibrium (NE) is a profile  \u0019∗ such that \u0019\n∗∈\nbr\n(\u0019∗) for every player k [23]. \nV. THE PROPOSED CONSENSUS PROTOCOL \nIn the POA algorithm, the node which wants to add a block \nmust solve the same problem as in the POW, which requires \nmassive computational effort, high cost, and special hardware. \nIn this paper, we modify the POA algorithm so that nodes do not \ntend to violate the protocol without having to solve a difficult \nproblem based on a model in game theory. \nLike the Proof of Stake (PoS) protocol, each node can be \nselected as the leader based on the amount of cryptocurrency it",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "best response correspondence br\n(\u0019)  as the set of all strategies \nthat maximizes the payoff for player k under profile \u0019 =\n(\u0019\r, … , \u0019\u0011) is shown at (1).  \n \nbr\n(\u0019) =\n def \n\u001b\n\u001c\u001d\u001e\u001f\u001c \n!∈\t\" \u0012\n(#; \u0019%\n)&.                                (1) \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n85 \nA Nash equilibrium (NE) is a profile  \u0019∗ such that \u0019\n∗∈\nbr\n(\u0019∗) for every player k [23]. \nV. THE PROPOSED CONSENSUS PROTOCOL \nIn the POA algorithm, the node which wants to add a block \nmust solve the same problem as in the POW, which requires \nmassive computational effort, high cost, and special hardware. \nIn this paper, we modify the POA algorithm so that nodes do not \ntend to violate the protocol without having to solve a difficult \nproblem based on a model in game theory. \nLike the Proof of Stake (PoS) protocol, each node can be \nselected as the leader based on the amount of cryptocurrency it"
        }
    },
    {
        "id": "96bd9e56f8cee04e7635f96b57e99a681f9cdb0d",
        "content": "of transactions. Users can contribute new blocks to the \nblockchain by completing computationally complex but readily \nverifiable challenges. The consensus method employed in the \nBlockchain network is based on this challenge, which prevents \nusers from engaging in malicious activity. Since blockchain \ntechnology enables security, transparency, and decentralization \nin global peer-to-peer transactions, Blockchain technology has \nemerged as a viable option for a wide range of sectors in the past \nfew years [3]. Regardless of whether it has the potential to \nimprove security, new technologies always come with security \nconcerns [4]. Blockchain technology has also enabled the \ndevelopment of block withholding attacks and selfish mining \nassaults. Many reasons drive these types of assaults, but the most \ncommon is financial gain. The game theory can be a valuable \nweapon in the fight against such assaults and in strengthening \nblockchain security.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "of transactions. Users can contribute new blocks to the \nblockchain by completing computationally complex but readily \nverifiable challenges. The consensus method employed in the \nBlockchain network is based on this challenge, which prevents \nusers from engaging in malicious activity. Since blockchain \ntechnology enables security, transparency, and decentralization \nin global peer-to-peer transactions, Blockchain technology has \nemerged as a viable option for a wide range of sectors in the past \nfew years [3]. Regardless of whether it has the potential to \nimprove security, new technologies always come with security \nconcerns [4]. Blockchain technology has also enabled the \ndevelopment of block withholding attacks and selfish mining \nassaults. Many reasons drive these types of assaults, but the most \ncommon is financial gain. The game theory can be a valuable \nweapon in the fight against such assaults and in strengthening \nblockchain security."
        }
    },
    {
        "id": "f14dcda454c101b3bcbdc28fa2f7892bf9254292",
        "content": "acquires a significant proportion of the overall hashpower may \nundertake double-spending attacks [8,21]. The nodes in the \nnetwork of Proof of Activity protocol (PoA) do more \ncomplicated verifications in comparison to the nodes in the \nnetwork of Proof of Work protocol (PoW). Follow-the-satoshi, \nthe principal PoA subroutine, picks a single satoshi (the smallest \nunit of bitcoin) at random from a pool of all the Satoshis that \nhave ever been created. Once the pseudorandom index is \nselected, it is followed by analyzing the block where this Satoshi \nwas created, and then following each subsequent transaction that \nmoved the coin to the next address until it reaches the current \nowner of the Satoshi. We specify how blocks are generated in \nthe PoA network as follows: \nFirstly, each miner puts their computing resources into \ncreating an empty block header, which is made up of just the \nhash of the previous block and a nonce as well as their own",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "acquires a significant proportion of the overall hashpower may \nundertake double-spending attacks [8,21]. The nodes in the \nnetwork of Proof of Activity protocol (PoA) do more \ncomplicated verifications in comparison to the nodes in the \nnetwork of Proof of Work protocol (PoW). Follow-the-satoshi, \nthe principal PoA subroutine, picks a single satoshi (the smallest \nunit of bitcoin) at random from a pool of all the Satoshis that \nhave ever been created. Once the pseudorandom index is \nselected, it is followed by analyzing the block where this Satoshi \nwas created, and then following each subsequent transaction that \nmoved the coin to the next address until it reaches the current \nowner of the Satoshi. We specify how blocks are generated in \nthe PoA network as follows: \nFirstly, each miner puts their computing resources into \ncreating an empty block header, which is made up of just the \nhash of the previous block and a nonce as well as their own"
        }
    },
    {
        "id": "7c8710219a11d46a5e163b17bec4ee9548ae81b9",
        "content": "attacker may then double-spend by reversing the recent ledger \nhistory to defraud merchants, or launch a PoW-denial-of-service \n(PoW-DoS) assault by refusing to include transactions in the \nblocks she creates, unless the transactions comply with the rules \nof attacker. This would allow such an attacker to either double-\nspend or carry out PoW-DoS attacks. Because double-spending \nor PoW-DoS attacks may undermine trust in the Bitcoin system, \nthe attacker may succeed in her goal if she intends to do damage. \nUsing PoW-DoS to extort others and raise transaction fees is an \noption for a selfish self-interested attacker who wants to profit \nfrom double-spending. \nPoW-DoS attacks would increase if the attacker has at least \n50% of the total hashpower. Additionally, an attacker who \nacquires a significant proportion of the overall hashpower may \nundertake double-spending attacks [8,21]. The nodes in the \nnetwork of Proof of Activity protocol (PoA) do more",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "attacker may then double-spend by reversing the recent ledger \nhistory to defraud merchants, or launch a PoW-denial-of-service \n(PoW-DoS) assault by refusing to include transactions in the \nblocks she creates, unless the transactions comply with the rules \nof attacker. This would allow such an attacker to either double-\nspend or carry out PoW-DoS attacks. Because double-spending \nor PoW-DoS attacks may undermine trust in the Bitcoin system, \nthe attacker may succeed in her goal if she intends to do damage. \nUsing PoW-DoS to extort others and raise transaction fees is an \noption for a selfish self-interested attacker who wants to profit \nfrom double-spending. \nPoW-DoS attacks would increase if the attacker has at least \n50% of the total hashpower. Additionally, an attacker who \nacquires a significant proportion of the overall hashpower may \nundertake double-spending attacks [8,21]. The nodes in the \nnetwork of Proof of Activity protocol (PoA) do more"
        }
    },
    {
        "id": "022901766dfb89a7622715642bbc4ce0944af67a",
        "content": "problem based on a model in game theory. \nLike the Proof of Stake (PoS) protocol, each node can be \nselected as the leader based on the amount of cryptocurrency it \nhas, and its validity rate but this is done randomly, so no node \nknows when this will be possible. In this algorithm, the nodes \nthat want to be elected as leaders block some of their \ncryptocurrencies. In fact, from the nodes that have the most \nvalidity rate, the one that has blocked the most cryptocurrencies \nwill be selected as the leader in the next step. In order not to \nknow in advance which node is the winner, the choice of leader \nis taken out of the final state and is done to some extent \nrandomly. It can be very effective in reducing the likelihood of \nfraud. Here we use a combination of hash code and stock. \nTherefore, the lower the node hash code and the larger the stock, \nthe higher the chance of selection. This algorithm is performed \nin the following steps:",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "problem based on a model in game theory. \nLike the Proof of Stake (PoS) protocol, each node can be \nselected as the leader based on the amount of cryptocurrency it \nhas, and its validity rate but this is done randomly, so no node \nknows when this will be possible. In this algorithm, the nodes \nthat want to be elected as leaders block some of their \ncryptocurrencies. In fact, from the nodes that have the most \nvalidity rate, the one that has blocked the most cryptocurrencies \nwill be selected as the leader in the next step. In order not to \nknow in advance which node is the winner, the choice of leader \nis taken out of the final state and is done to some extent \nrandomly. It can be very effective in reducing the likelihood of \nfraud. Here we use a combination of hash code and stock. \nTherefore, the lower the node hash code and the larger the stock, \nthe higher the chance of selection. This algorithm is performed \nin the following steps:"
        }
    },
    {
        "id": "901c935d8ac397fa0c70247c170af9153a595738",
        "content": "Therefore, the lower the node hash code and the larger the stock, \nthe higher the chance of selection. This algorithm is performed \nin the following steps: \n1. If a node wants to be elected leader in the next step, based \non its validity rate it blocks part of its cryptocurrencies and \nnotifies the network. \n2. From among the candidate nodes, the node that has \nblocked most cryptocurrencies is selected as the leader. \n3. The leader randomly selects N nodes, where N is a random \nnumber. In this way, N random numbers are generated between \none and the maximum number of Satoshis in the network. Since \na Satoshi may have been traded many times, a node is \nconsidered to be the owner of the Satoshi who is the last owner. \nThen the leader node sends the set of selected Satoshis to all \nnetwork nodes. We call this set of nodes, acceptors. \n4. Whenever a node in the network receives this message, it \nchecks whether it owns one of the selected Satoshis. Satoshi",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Therefore, the lower the node hash code and the larger the stock, \nthe higher the chance of selection. This algorithm is performed \nin the following steps: \n1. If a node wants to be elected leader in the next step, based \non its validity rate it blocks part of its cryptocurrencies and \nnotifies the network. \n2. From among the candidate nodes, the node that has \nblocked most cryptocurrencies is selected as the leader. \n3. The leader randomly selects N nodes, where N is a random \nnumber. In this way, N random numbers are generated between \none and the maximum number of Satoshis in the network. Since \na Satoshi may have been traded many times, a node is \nconsidered to be the owner of the Satoshi who is the last owner. \nThen the leader node sends the set of selected Satoshis to all \nnetwork nodes. We call this set of nodes, acceptors. \n4. Whenever a node in the network receives this message, it \nchecks whether it owns one of the selected Satoshis. Satoshi"
        }
    },
    {
        "id": "b6a21c0a5506c2ef6d321a4c8fb2e2631a4d8060",
        "content": "blockchain consensus algorithms,” 41st International Convention on \nInformation \nand \nCommunication \nTechnology, \nElectronics \nand \nMicroelectronics (MIPRO), pp. 1545–1550, 2018. \n[10] M. Nojoumian, A. Golchubian, L. Njilla, K. Kwiat, C. Kamhoua, \n“Incentivizing blockchain miners to avoid dishonest mining strategies by \na reputation-based paradigm,” IEEE Computing Conference (CC). IEEE, \nLondon (2018)  \n[11] I. Eyal, “The Miner's Dilemma,” 2015 IEEE Symposium on Security and \nPrivacy, pp. 89–103, 2015. \n[12] A. Stone, “An examination of single transaction blocks and their effect on \nnetwork throughput and block size,” Self-published Paper, 2015.  \n[13] T. Swanson, “Consensus-as-a-service: a brief report on the emergence of \npermissioned, distributed ledger systems, ” Report, 2015. \n[14] N. Dimitri, “Bitcoin Mining as a Contest”, ledger, vol. 2, pp. 31–37, Sep. \n2017. \n[15] Z. Liu et al., “A survey on applications of game theory in blockchain.” \narXiv, 2019.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "blockchain consensus algorithms,” 41st International Convention on \nInformation \nand \nCommunication \nTechnology, \nElectronics \nand \nMicroelectronics (MIPRO), pp. 1545–1550, 2018. \n[10] M. Nojoumian, A. Golchubian, L. Njilla, K. Kwiat, C. Kamhoua, \n“Incentivizing blockchain miners to avoid dishonest mining strategies by \na reputation-based paradigm,” IEEE Computing Conference (CC). IEEE, \nLondon (2018)  \n[11] I. Eyal, “The Miner's Dilemma,” 2015 IEEE Symposium on Security and \nPrivacy, pp. 89–103, 2015. \n[12] A. Stone, “An examination of single transaction blocks and their effect on \nnetwork throughput and block size,” Self-published Paper, 2015.  \n[13] T. Swanson, “Consensus-as-a-service: a brief report on the emergence of \npermissioned, distributed ledger systems, ” Report, 2015. \n[14] N. Dimitri, “Bitcoin Mining as a Contest”, ledger, vol. 2, pp. 31–37, Sep. \n2017. \n[15] Z. Liu et al., “A survey on applications of game theory in blockchain.” \narXiv, 2019."
        }
    },
    {
        "id": "06ded38cc9a6e8fe9271be332e87ea13307ba19f",
        "content": "and the block mining node. Therefore, the nodes do not tend to \nform mining pools and attack the network because they lose the \nchance to be an addition and a leader and can not get a reward. \nIn general, consensus algorithms are assessed based on three \ncriteria: \nperformance, \nsecurity, \nand \nthe \ndegree \nof \ndecentralization. The most critical considerations in determining \nsystem performance are scalability, complexity, throughput, and \nalgorithm activity. For the most part, anti-Byzantine ability and \nresistance to double-spending are responsible for security. Also, \nthe scale of nodes involved in the consensus process determines \nthe decentralization level. The protocol proposed in [32] \nimproved the scalability since it uses a pipelined block topology \nstructure and there is no necessary sequence link between the \nproduction of blocks. Because the algorithms proposed in [33] \nand [34] are based on standard BFT, there is no scalability",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "and the block mining node. Therefore, the nodes do not tend to \nform mining pools and attack the network because they lose the \nchance to be an addition and a leader and can not get a reward. \nIn general, consensus algorithms are assessed based on three \ncriteria: \nperformance, \nsecurity, \nand \nthe \ndegree \nof \ndecentralization. The most critical considerations in determining \nsystem performance are scalability, complexity, throughput, and \nalgorithm activity. For the most part, anti-Byzantine ability and \nresistance to double-spending are responsible for security. Also, \nthe scale of nodes involved in the consensus process determines \nthe decentralization level. The protocol proposed in [32] \nimproved the scalability since it uses a pipelined block topology \nstructure and there is no necessary sequence link between the \nproduction of blocks. Because the algorithms proposed in [33] \nand [34] are based on standard BFT, there is no scalability"
        }
    },
    {
        "id": "c2771d8e671cc5c6c320bb25408fd631b40999fa",
        "content": "Additionally, the consensus protocol ensures that the order \nof blocks in the chain of each node in the blockchain network is \nthe same. This guarantee is significant because blockchains are \na network of decentralized nodes that need a way to synchronize \ntheir copies of information. The validators or miners are the \nnodes that are performing consensus protocols. There are several \nestablished consensus protocols. In a decentralized consensus \nprotocol, an unknown, changeable number of validators must \nvalidate blocks to increase the stability of the protocol. \nHowever, all of the existing consensus protocols do not provide \ndecentralization. Rather than that, they depend on predefined, \nwell-known validators. This creates an opening for a variety of \npotential dangers [6]. \nExisting implementations of blockchain technology make \nuse of a variety of consensus techniques, including Proof of \nElapsed Time (PET), Proof of Stake Model (PSM), Stellar",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Additionally, the consensus protocol ensures that the order \nof blocks in the chain of each node in the blockchain network is \nthe same. This guarantee is significant because blockchains are \na network of decentralized nodes that need a way to synchronize \ntheir copies of information. The validators or miners are the \nnodes that are performing consensus protocols. There are several \nestablished consensus protocols. In a decentralized consensus \nprotocol, an unknown, changeable number of validators must \nvalidate blocks to increase the stability of the protocol. \nHowever, all of the existing consensus protocols do not provide \ndecentralization. Rather than that, they depend on predefined, \nwell-known validators. This creates an opening for a variety of \npotential dangers [6]. \nExisting implementations of blockchain technology make \nuse of a variety of consensus techniques, including Proof of \nElapsed Time (PET), Proof of Stake Model (PSM), Stellar"
        }
    },
    {
        "id": "33123a95c1d5e0e6529471fb549df87ebd1ab89b",
        "content": "Anyone in a blockchain network may employ token supply and \ntransaction tipping while performing the role of blockchain \nmining, according to the Nakamoto protocol [16]. With the \nappropriate use of game theory, this method instills the profit \nprocess and optimizes the return. Game theory may also be used \nwith two different types of mining management: individual \nmining and pool mining. Each miner's dominating tactic in the \nindividual mining process is either processing power or forking \nthe longest chain. Other activities in the pool mining-based \ngame theory model for payout include block size setting, pool \nselection, and reward allocation. \nAccording to Dey [17], there is a way that uses software \nagents to keep track of blockchain network participants. \nAlgorithm game theory and supervised machine learning \nalgorithms can be used to discover abnormalities as a result of \nthis monitoring. In order to recognize different types of assaults,",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Anyone in a blockchain network may employ token supply and \ntransaction tipping while performing the role of blockchain \nmining, according to the Nakamoto protocol [16]. With the \nappropriate use of game theory, this method instills the profit \nprocess and optimizes the return. Game theory may also be used \nwith two different types of mining management: individual \nmining and pool mining. Each miner's dominating tactic in the \nindividual mining process is either processing power or forking \nthe longest chain. Other activities in the pool mining-based \ngame theory model for payout include block size setting, pool \nselection, and reward allocation. \nAccording to Dey [17], there is a way that uses software \nagents to keep track of blockchain network participants. \nAlgorithm game theory and supervised machine learning \nalgorithms can be used to discover abnormalities as a result of \nthis monitoring. In order to recognize different types of assaults,"
        }
    },
    {
        "id": "78ca76a76990400c8d340d8650f2229480b2f54b",
        "content": "using many candidate nodes to replicate logs simultaneously. \nBased on the Raft algorithm, Wang et al. developed the hhRaft \nalgorithm in 2021 [27]. This approach introduced a new monitor \nrole to watch candidate and leader nodes, optimized the leader \nnode election and log replication phases, increased fault \ntolerance, decreased consensus latency, and improved \ntransaction throughput. In terms of anti-Byzantine failure \ncapabilities, the hhRaft algorithm outperforms the Raft method, \nand it is better suited for high real-time and highly hostile \nsituations. \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n84 \nDPoS, a highly centralized consensus method, was modified \nby Nir Bitansky [28] in 2020 to create more uncertainty in the \nvoting process by including virtual nodes into the VRF \nconsensus algorithm. The efficiency and low power",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "using many candidate nodes to replicate logs simultaneously. \nBased on the Raft algorithm, Wang et al. developed the hhRaft \nalgorithm in 2021 [27]. This approach introduced a new monitor \nrole to watch candidate and leader nodes, optimized the leader \nnode election and log replication phases, increased fault \ntolerance, decreased consensus latency, and improved \ntransaction throughput. In terms of anti-Byzantine failure \ncapabilities, the hhRaft algorithm outperforms the Raft method, \nand it is better suited for high real-time and highly hostile \nsituations. \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n84 \nDPoS, a highly centralized consensus method, was modified \nby Nir Bitansky [28] in 2020 to create more uncertainty in the \nvoting process by including virtual nodes into the VRF \nconsensus algorithm. The efficiency and low power"
        }
    },
    {
        "id": "c46f1f8466b7fdd04d5f04a639c17fd57f6568a8",
        "content": "the fields of computer science and economics, its application of \ngame theory methodologies remains restricted [10]. In \nconsensus procedures, game theory is also generally recognized. \nNumerous game theory-based consensus techniques are \ninvestigated, including the following: \nThe authors in [11] investigate what happens when pools \nbegin to attack each other. Infiltrated miners from other pools \nmay enter open pools and undertake withholding attacks. Open \npools indicate pools of miners, which let any miner participate \nin mining operations. The mentioned article describes a game in \nwhich mining pools recruit members to enter other pools to \nreduce their mining power. There are two pools of players in the \nminer's dilemma game, and their strategies revolve around \nwhether they attack each other or not. As this article points out, \nthe most common tactic used by each player is attacking. Stone \n[12] provides a game-theoretic model with a block-size-",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "the fields of computer science and economics, its application of \ngame theory methodologies remains restricted [10]. In \nconsensus procedures, game theory is also generally recognized. \nNumerous game theory-based consensus techniques are \ninvestigated, including the following: \nThe authors in [11] investigate what happens when pools \nbegin to attack each other. Infiltrated miners from other pools \nmay enter open pools and undertake withholding attacks. Open \npools indicate pools of miners, which let any miner participate \nin mining operations. The mentioned article describes a game in \nwhich mining pools recruit members to enter other pools to \nreduce their mining power. There are two pools of players in the \nminer's dilemma game, and their strategies revolve around \nwhether they attack each other or not. As this article points out, \nthe most common tactic used by each player is attacking. Stone \n[12] provides a game-theoretic model with a block-size-"
        }
    },
    {
        "id": "3a88f85e3246a40283004be1560228fcba82225b",
        "content": "2014. \n[5] T. K. Mackey, G. Nayyar, “A review of existing and emerging digital \ntechnologies to combat the global trade in fake medicines,” Expert \nopinion on drug safety, 16(5), 587–602, 2017. \n[6] N. Alzahrani, N. Bulusu, “Towards True Decentralization: A Blockchain \nConsensus Protocol Based on Game Theory and Randomness,” \nInternational Conference on Decision and Game Theory for Security, \nGameSec 2018: Decision and Game Theory for Security, pp. 465–485, \nvol. 11199, 2018. \n[7] W. Wang et al., “A Survey on Consensus Mechanisms and Mining \nStrategy Management in Blockchain Networks,” in IEEE Access, vol. 7, \npp. 22328–22370, 2019. \n[8] N. Satoshi, “Bitcoin: A Peer-to-Peer Electronic Cash System,” 2009. \n[9] L. M. Bach, B. Mihaljevic and M. Zagar, “Comparative analysis of \nblockchain consensus algorithms,” 41st International Convention on \nInformation \nand \nCommunication \nTechnology, \nElectronics \nand \nMicroelectronics (MIPRO), pp. 1545–1550, 2018.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "2014. \n[5] T. K. Mackey, G. Nayyar, “A review of existing and emerging digital \ntechnologies to combat the global trade in fake medicines,” Expert \nopinion on drug safety, 16(5), 587–602, 2017. \n[6] N. Alzahrani, N. Bulusu, “Towards True Decentralization: A Blockchain \nConsensus Protocol Based on Game Theory and Randomness,” \nInternational Conference on Decision and Game Theory for Security, \nGameSec 2018: Decision and Game Theory for Security, pp. 465–485, \nvol. 11199, 2018. \n[7] W. Wang et al., “A Survey on Consensus Mechanisms and Mining \nStrategy Management in Blockchain Networks,” in IEEE Access, vol. 7, \npp. 22328–22370, 2019. \n[8] N. Satoshi, “Bitcoin: A Peer-to-Peer Electronic Cash System,” 2009. \n[9] L. M. Bach, B. Mihaljevic and M. Zagar, “Comparative analysis of \nblockchain consensus algorithms,” 41st International Convention on \nInformation \nand \nCommunication \nTechnology, \nElectronics \nand \nMicroelectronics (MIPRO), pp. 1545–1550, 2018."
        }
    },
    {
        "id": "460fbfa4c0f61635710d2dc5675cdc4e76e13dda",
        "content": "whether they attack each other or not. As this article points out, \nthe most common tactic used by each player is attacking. Stone \n[12] provides a game-theoretic model with a block-size-\ndependent reward strategy. A big block size has a high payout \ncost. A threshold block size is explored in this section for a real-\nworld situation. A greater block size introduces more latency, \nwhich isolates the block. Isolation is a significant contributing \nfactor to Denial of Service (DoS) and Distributed Denial of \nService (DDoS) attacks. Additionally, it is noticed that the \nblockchain network's physical nature precludes a high gas price \nfor a big block size. \nSwanson [13] contends that in order to be secure, a \npermissionless collection of nodes in a blockchain network must \nuse a game-theoretic method. The balance between blockchain \nnetwork protocols and trust is a considerable difficulty, and \nnumerous initiatives are being undertaken to progress certain",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "whether they attack each other or not. As this article points out, \nthe most common tactic used by each player is attacking. Stone \n[12] provides a game-theoretic model with a block-size-\ndependent reward strategy. A big block size has a high payout \ncost. A threshold block size is explored in this section for a real-\nworld situation. A greater block size introduces more latency, \nwhich isolates the block. Isolation is a significant contributing \nfactor to Denial of Service (DoS) and Distributed Denial of \nService (DDoS) attacks. Additionally, it is noticed that the \nblockchain network's physical nature precludes a high gas price \nfor a big block size. \nSwanson [13] contends that in order to be secure, a \npermissionless collection of nodes in a blockchain network must \nuse a game-theoretic method. The balance between blockchain \nnetwork protocols and trust is a considerable difficulty, and \nnumerous initiatives are being undertaken to progress certain"
        }
    },
    {
        "id": "8a1975584acf5149deb01363762439ffaab6b289",
        "content": "network nodes. We call this set of nodes, acceptors. \n4. Whenever a node in the network receives this message, it \nchecks whether it owns one of the selected Satoshis. Satoshi \nowner's node then participates in the process as one of the nodes \nselected by the leader. \n5. The leader node creates a new non-transactional block that \ncontains the hash code of the previous block, the general address \nof the leader, and the distance from the first block of the chain, \nthen sends it to all nodes in the network. Whenever an acceptor \nreceives this message, it first checks that the block has been \ncreated correctly and has complied with the rules. In fact, they \nexamine the header of the empty block formed by the leader. If \nit was valid and had registered the hash code of the previous \nblock correctly, then they sign the blank block hash code and \nnotifies the leader. Since the leader places the block header and \nit blocks the most stocks and has the most validity rate, it is very",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "network nodes. We call this set of nodes, acceptors. \n4. Whenever a node in the network receives this message, it \nchecks whether it owns one of the selected Satoshis. Satoshi \nowner's node then participates in the process as one of the nodes \nselected by the leader. \n5. The leader node creates a new non-transactional block that \ncontains the hash code of the previous block, the general address \nof the leader, and the distance from the first block of the chain, \nthen sends it to all nodes in the network. Whenever an acceptor \nreceives this message, it first checks that the block has been \ncreated correctly and has complied with the rules. In fact, they \nexamine the header of the empty block formed by the leader. If \nit was valid and had registered the hash code of the previous \nblock correctly, then they sign the blank block hash code and \nnotifies the leader. Since the leader places the block header and \nit blocks the most stocks and has the most validity rate, it is very"
        }
    },
    {
        "id": "8b6fb5315274709f641108cfa32e3b0384bcd143",
        "content": "is really wrong and should not be verified, and also the attacker \nnodes that form the mining pool do not approve it. The strategy \nprofile (Approve, Approve) means the added empty block is \nreally valid and should be verified, and also the attacker nodes \nthat form the mining pool approve it. The strategy profile \n(Disapprove, Approve) means the added empty block is really \nvalid and should be verified, but the attacker nodes that form the \nmining pool do not approve it. The strategy profile (Approve, \nDisapprove) means the added empty block is really wrong and \nshould not be verified, but the attacker nodes that form the \nmining pool approve it. Now we determine payoff functions or \nutility function for each players. \nAs it can be seen in (2) and (3) the uM and ul demonstrate the \nutility function for Mining pool member nodes and Individual \nnodes, respectively. Let A and D illustrate approval and \ndisapprove, respectively.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "is really wrong and should not be verified, and also the attacker \nnodes that form the mining pool do not approve it. The strategy \nprofile (Approve, Approve) means the added empty block is \nreally valid and should be verified, and also the attacker nodes \nthat form the mining pool approve it. The strategy profile \n(Disapprove, Approve) means the added empty block is really \nvalid and should be verified, but the attacker nodes that form the \nmining pool do not approve it. The strategy profile (Approve, \nDisapprove) means the added empty block is really wrong and \nshould not be verified, but the attacker nodes that form the \nmining pool approve it. Now we determine payoff functions or \nutility function for each players. \nAs it can be seen in (2) and (3) the uM and ul demonstrate the \nutility function for Mining pool member nodes and Individual \nnodes, respectively. Let A and D illustrate approval and \ndisapprove, respectively."
        }
    },
    {
        "id": "b0a9c002976b881168e3f60deef83a1f45efa4d0",
        "content": "[14] N. Dimitri, “Bitcoin Mining as a Contest”, ledger, vol. 2, pp. 31–37, Sep. \n2017. \n[15] Z. Liu et al., “A survey on applications of game theory in blockchain.” \narXiv, 2019. \n[16] A. Kumar, S. Jain, “Proof of Game (PoG): A Game Theory Based \nConsensus Model,” In: Karrupusamy, Sustainable Communication \nNetworks and Application(ICSCN), Lecture Notes on Data Engineering \nand Communications Technologies, vol. 39, 2020. \n[17] S. Dey, “Securing Majority-Attack in Blockchain Using Machine \nLearning and Algorithmic Game Theory: A Proof of Work,” 10th \nComputer Science and Electronic Engineering (CEEC), pp. 7–10, 2018. \n[18] X. Liu, W. Wang, D. Niyato, N. Zhao and P. Wang, “Evolutionary Game \nfor Mining Pool Selection in Blockchain Networks,” in IEEE Wireless \nCommunications Letters, vol. 7, no. 5, pp. 760–763, Oct. 2018. \n[19] Y. Jiao, P. Wang, D. Niyato and Z. Xiong, “Social Welfare Maximization \nAuction in Edge Computing Resource Allocation for Mobile",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "[14] N. Dimitri, “Bitcoin Mining as a Contest”, ledger, vol. 2, pp. 31–37, Sep. \n2017. \n[15] Z. Liu et al., “A survey on applications of game theory in blockchain.” \narXiv, 2019. \n[16] A. Kumar, S. Jain, “Proof of Game (PoG): A Game Theory Based \nConsensus Model,” In: Karrupusamy, Sustainable Communication \nNetworks and Application(ICSCN), Lecture Notes on Data Engineering \nand Communications Technologies, vol. 39, 2020. \n[17] S. Dey, “Securing Majority-Attack in Blockchain Using Machine \nLearning and Algorithmic Game Theory: A Proof of Work,” 10th \nComputer Science and Electronic Engineering (CEEC), pp. 7–10, 2018. \n[18] X. Liu, W. Wang, D. Niyato, N. Zhao and P. Wang, “Evolutionary Game \nfor Mining Pool Selection in Blockchain Networks,” in IEEE Wireless \nCommunications Letters, vol. 7, no. 5, pp. 760–763, Oct. 2018. \n[19] Y. Jiao, P. Wang, D. Niyato and Z. Xiong, “Social Welfare Maximization \nAuction in Edge Computing Resource Allocation for Mobile"
        }
    },
    {
        "id": "ba8dc0836e4770c10657cc2529d7f6131d6a35bf",
        "content": "signs the blank block hash code and notifies the leader. Since the \nleader places the block header and blocks the most stocks, and \nhas the most validity rate, it is improbable that the node is an \nattacker and mistakenly adds the previous block hash code and \nother information to the new empty block. Acceptor nodes may \nbe members of mining pools, so they play a strategic game. \nAccording to Nash equilibrium, we showed that the whole nodes \nin the networks do not tend to create mining pools. Then one of \nthe acceptor nodes, which is not in the blocked nodes list, which \nwe call the block mining node, adds the transactions to the block \nand creates a new block, then sends that throughout the network. \nFinally, the block mining reward is divided between the leader \nand the block mining node. Therefore, the nodes do not tend to \nform mining pools and attack the network because they lose the \nchance to be an addition and a leader and can not get a reward.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "signs the blank block hash code and notifies the leader. Since the \nleader places the block header and blocks the most stocks, and \nhas the most validity rate, it is improbable that the node is an \nattacker and mistakenly adds the previous block hash code and \nother information to the new empty block. Acceptor nodes may \nbe members of mining pools, so they play a strategic game. \nAccording to Nash equilibrium, we showed that the whole nodes \nin the networks do not tend to create mining pools. Then one of \nthe acceptor nodes, which is not in the blocked nodes list, which \nwe call the block mining node, adds the transactions to the block \nand creates a new block, then sends that throughout the network. \nFinally, the block mining reward is divided between the leader \nand the block mining node. Therefore, the nodes do not tend to \nform mining pools and attack the network because they lose the \nchance to be an addition and a leader and can not get a reward."
        }
    },
    {
        "id": "18ea1135f385e745380b6f9ca6236271f8ae3d03",
        "content": "9. In the next step, block mining node, adds the desired \ntransactions to the block and practically forms a new block then \nsends that throughout the network. \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n86 \n10. In the last step, the reward regarding block mining is \ndivided between the leader node and the block mining node in \nthe last step. Therefore, the nodes do not tend to form mining \npools and attack the network because they lose the chance to be \nan addition and a leader and can not get a reward. \nVI. CONCLUTION \nIn this paper, we introduce a novel consensus protocol in \nblockchain networks which Its Energy consumption is very low, \nBlock creation speed is very high and not Requires advanced \nhardware. In this protocol no nodes tend to forming mining \npools. Any node that has blocked most cryptocurrencies and not",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "9. In the next step, block mining node, adds the desired \ntransactions to the block and practically forms a new block then \nsends that throughout the network. \nAuthorized licensed use limited to: KLE Technological University. Downloaded on October 01,2023 at 06:04:52 UTC from IEEE Xplore.  Restrictions apply. \n86 \n10. In the last step, the reward regarding block mining is \ndivided between the leader node and the block mining node in \nthe last step. Therefore, the nodes do not tend to form mining \npools and attack the network because they lose the chance to be \nan addition and a leader and can not get a reward. \nVI. CONCLUTION \nIn this paper, we introduce a novel consensus protocol in \nblockchain networks which Its Energy consumption is very low, \nBlock creation speed is very high and not Requires advanced \nhardware. In this protocol no nodes tend to forming mining \npools. Any node that has blocked most cryptocurrencies and not"
        }
    },
    {
        "id": "6f1e96a73ec98dbbad6bf9068bc26cdd7d64aaed",
        "content": "the cost of computing resources used by a miner to generate a \nhash value indirectly. Additionally, the author establishes that \nthe bitcoin mining process is lucrative when fewer independent \nindividuals participate in mining, and that miners' rationality \nbenefits everyone since big payoffs encourage miners to acquire \nextra computer capacity. \nLiu et al. [15] conducted a thorough review of game theory \napplications on the blockchain. Non-cooperative games, \nextensive-form games, Stackelberg games, and stochastic games \nare some of the game theory models accessible, according to \n[15]. Selfish mining assaults, majority attacks, Denial of Service \nattacks, fraudulent data sharing, untrustworthy goods selling, \nand cyber-insurance may all be detected using game theory. \nAnyone in a blockchain network may employ token supply and \ntransaction tipping while performing the role of blockchain \nmining, according to the Nakamoto protocol [16]. With the",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "the cost of computing resources used by a miner to generate a \nhash value indirectly. Additionally, the author establishes that \nthe bitcoin mining process is lucrative when fewer independent \nindividuals participate in mining, and that miners' rationality \nbenefits everyone since big payoffs encourage miners to acquire \nextra computer capacity. \nLiu et al. [15] conducted a thorough review of game theory \napplications on the blockchain. Non-cooperative games, \nextensive-form games, Stackelberg games, and stochastic games \nare some of the game theory models accessible, according to \n[15]. Selfish mining assaults, majority attacks, Denial of Service \nattacks, fraudulent data sharing, untrustworthy goods selling, \nand cyber-insurance may all be detected using game theory. \nAnyone in a blockchain network may employ token supply and \ntransaction tipping while performing the role of blockchain \nmining, according to the Nakamoto protocol [16]. With the"
        }
    },
    {
        "id": "0355301c7f1573b7162a74bcc3ee62e222560d4c",
        "content": "Firstly, each miner puts their computing resources into \ncreating an empty block header, which is made up of just the \nhash of the previous block and a nonce as well as their own \npublic IP address and height concerning the genesis block. This \nheader has no connection to any transactions. \nSecondly, when a miner has generated an empty block \nheader indicating that the hash of a miner's data is lower than the \ncurrent difficulty goal, the miner broadcasts the block header in \nthe network. \nIn the third step, the hash of this block header is used as data \nby all network nodes, which generates N pseudorandom \nstakeholders deterministically. Each combination of the \npreceding block's hash and N fixed suffix values are \nconcatenated with this one and hashed before being used as \ninput to the follow-the-Satoshi function. \nIn the fourth step, every online stakeholder examines \nwhether the miner is broadcasted empty block header is",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Firstly, each miner puts their computing resources into \ncreating an empty block header, which is made up of just the \nhash of the previous block and a nonce as well as their own \npublic IP address and height concerning the genesis block. This \nheader has no connection to any transactions. \nSecondly, when a miner has generated an empty block \nheader indicating that the hash of a miner's data is lower than the \ncurrent difficulty goal, the miner broadcasts the block header in \nthe network. \nIn the third step, the hash of this block header is used as data \nby all network nodes, which generates N pseudorandom \nstakeholders deterministically. Each combination of the \npreceding block's hash and N fixed suffix values are \nconcatenated with this one and hashed before being used as \ninput to the follow-the-Satoshi function. \nIn the fourth step, every online stakeholder examines \nwhether the miner is broadcasted empty block header is"
        }
    },
    {
        "id": "32449b5f71a65a01af1a3f5b7af6a2e86a27b0f1",
        "content": "utility function for Mining pool member nodes and Individual \nnodes, respectively. Let A and D illustrate approval and \ndisapprove, respectively. \nuM(D,D) > uM(A,D) > uM(A,A) > uM(A,D)                    (2) \nul(D,D) = ul(A,A) > ul(D,A) > ul(A,D)                         (3) \nAs it can seen, the worst-case scenario for any group of \nnodes is when that group disapproves the new block while the \nrival group approves the block. The Nash equilibriums are  \n(Disapprove, Disapprove)  and (Approve, Approve). This \nhappens when both groups have an honest strategy; that is, if the \nnew block is added wrongly, they disapprove the block, and if it \nis completely correct, they approve it. \n7. When a disagreement occurs between nodes, the leader \nnode puts the nodes that disapproved the block in a list called \nthe blocked nodes list and adds the block. The leader then sends \nthe set of blocked nodes to all the nodes in the network. In this",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "utility function for Mining pool member nodes and Individual \nnodes, respectively. Let A and D illustrate approval and \ndisapprove, respectively. \nuM(D,D) > uM(A,D) > uM(A,A) > uM(A,D)                    (2) \nul(D,D) = ul(A,A) > ul(D,A) > ul(A,D)                         (3) \nAs it can seen, the worst-case scenario for any group of \nnodes is when that group disapproves the new block while the \nrival group approves the block. The Nash equilibriums are  \n(Disapprove, Disapprove)  and (Approve, Approve). This \nhappens when both groups have an honest strategy; that is, if the \nnew block is added wrongly, they disapprove the block, and if it \nis completely correct, they approve it. \n7. When a disagreement occurs between nodes, the leader \nnode puts the nodes that disapproved the block in a list called \nthe blocked nodes list and adds the block. The leader then sends \nthe set of blocked nodes to all the nodes in the network. In this"
        }
    },
    {
        "id": "5ab09d2f1d5ef38457a0448b8ba4ba18a78949de",
        "content": "Block creation speed is very high and not Requires advanced \nhardware. In this protocol no nodes tend to forming mining \npools. Any node that has blocked most cryptocurrencies and not \nbeing in the block node list is selected as the leader. The leader \nselects N random numbers between one and the maximum \nnumber of Satoshis in the network. The leader sends a set of \nSatoshis to network. Whenever a node in the network receives \nthe set of Satoshis, it checks whether it owns one of the selected \nSatoshis. We call the owner of the Satoshis acceptors. The leader \ncreates a new non-transactional block and sends it to all nodes \nin the network. Whenever an acceptor receives the empty block, \nit first checks that the block has been created correctly, then it \nsigns the blank block hash code and notifies the leader. Since the \nleader places the block header and blocks the most stocks, and \nhas the most validity rate, it is improbable that the node is an",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "Block creation speed is very high and not Requires advanced \nhardware. In this protocol no nodes tend to forming mining \npools. Any node that has blocked most cryptocurrencies and not \nbeing in the block node list is selected as the leader. The leader \nselects N random numbers between one and the maximum \nnumber of Satoshis in the network. The leader sends a set of \nSatoshis to network. Whenever a node in the network receives \nthe set of Satoshis, it checks whether it owns one of the selected \nSatoshis. We call the owner of the Satoshis acceptors. The leader \ncreates a new non-transactional block and sends it to all nodes \nin the network. Whenever an acceptor receives the empty block, \nit first checks that the block has been created correctly, then it \nsigns the blank block hash code and notifies the leader. Since the \nleader places the block header and blocks the most stocks, and \nhas the most validity rate, it is improbable that the node is an"
        }
    },
    {
        "id": "b3649f29d31390ff761d9c3687ae4111fc8ec1ad",
        "content": "node puts the nodes that disapproved the block in a list called \nthe blocked nodes list and adds the block. The leader then sends \nthe set of blocked nodes to all the nodes in the network. In this \ncase, the validity rate of these nodes decreases, which means that \nother nodes no longer trust these blocked nodes to be selected as \nthe leader. \n8. Now one of the acceptor nodes which is not in the blocked \nnodes list, can be taken as the block mining node. According to \n(4), a node is the block mining node whose hash function value \nis on the sum of the previous block's hash code and the general \nnode address of the maximum value.     \nmax{hash(hash(previous block) + the general node address)}         (4) \n9. In the next step, block mining node, adds the desired \ntransactions to the block and practically forms a new block then \nsends that throughout the network.",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "node puts the nodes that disapproved the block in a list called \nthe blocked nodes list and adds the block. The leader then sends \nthe set of blocked nodes to all the nodes in the network. In this \ncase, the validity rate of these nodes decreases, which means that \nother nodes no longer trust these blocked nodes to be selected as \nthe leader. \n8. Now one of the acceptor nodes which is not in the blocked \nnodes list, can be taken as the block mining node. According to \n(4), a node is the block mining node whose hash function value \nis on the sum of the previous block's hash code and the general \nnode address of the maximum value.     \nmax{hash(hash(previous block) + the general node address)}         (4) \n9. In the next step, block mining node, adds the desired \ntransactions to the block and practically forms a new block then \nsends that throughout the network."
        }
    },
    {
        "id": "5013ba5e1c566d5e59c9c0979023c4f4b61b79b8",
        "content": "block correctly, then they sign the blank block hash code and \nnotifies the leader. Since the leader places the block header and \nit blocks the most stocks and has the most validity rate, it is very \nunlikely that the node is an attacker and mistakenly adds the \nprevious block hash code and other information to the new \nempty block. \n6. Acceptor nodes may be members of mining pools, so they \nplay a strategic game modeled as follows. \nPlayers = {Mining pool member nodes, Individual nodes} \nStrategies for each player = {Approve, Disapprove} \nIn the following, we show each strategy profiles as (A, B) in \nwhich A represents the action of the mining pool member nodes \nand B represents the action of the individual nodes. The strategy \nprofile (Disapprove, Disapprove) means the added empty block \nis really wrong and should not be verified, and also the attacker \nnodes that form the mining pool do not approve it. The strategy \nprofile (Approve, Approve) means the added empty block is",
        "metadata": {
            "source": "A_Novel_Consensus_Protocol_in_Blockchain_Network_based_on_Proof_of_Activity_Protocol_and_Game_Theory.pdf",
            "text": "block correctly, then they sign the blank block hash code and \nnotifies the leader. Since the leader places the block header and \nit blocks the most stocks and has the most validity rate, it is very \nunlikely that the node is an attacker and mistakenly adds the \nprevious block hash code and other information to the new \nempty block. \n6. Acceptor nodes may be members of mining pools, so they \nplay a strategic game modeled as follows. \nPlayers = {Mining pool member nodes, Individual nodes} \nStrategies for each player = {Approve, Disapprove} \nIn the following, we show each strategy profiles as (A, B) in \nwhich A represents the action of the mining pool member nodes \nand B represents the action of the individual nodes. The strategy \nprofile (Disapprove, Disapprove) means the added empty block \nis really wrong and should not be verified, and also the attacker \nnodes that form the mining pool do not approve it. The strategy \nprofile (Approve, Approve) means the added empty block is"
        }
    },
    {
        "id": "7bf57897a7f74218cd24957e5ddedb3321e98b9c",
        "content": "limits are too tight and messages are not guaranteed to be delivered before the beginning\nof the next phase.\nRationality Assumption. We proved that any rational party or parties, i.e. a party/parties\ninterested only in maximizing their own payoff, would play uniformly at random in an\nRIG game and would therefore be reliable. This is because playing uniformly at random\nis the only alliance-resistant equilibrium in the game. Moreover, the uniformity of the\noutput random number depends on having at least one reliable player. Therefore, we\nmust assume that at least one player is rational and our approach would not work if\nnone of the players are rational. However, this case is unlikely to happen in practice as\nwe would normally expect all parties to be rational.\nComputation and Communication Complexity. Our RIG can be implemented using a\ncommit-reveal scheme or a PVSS scheme, which is typical in random number genera-",
        "metadata": {
            "source": "game thereotic randomness.pdf",
            "text": "limits are too tight and messages are not guaranteed to be delivered before the beginning\nof the next phase.\nRationality Assumption. We proved that any rational party or parties, i.e. a party/parties\ninterested only in maximizing their own payoff, would play uniformly at random in an\nRIG game and would therefore be reliable. This is because playing uniformly at random\nis the only alliance-resistant equilibrium in the game. Moreover, the uniformity of the\noutput random number depends on having at least one reliable player. Therefore, we\nmust assume that at least one player is rational and our approach would not work if\nnone of the players are rational. However, this case is unlikely to happen in practice as\nwe would normally expect all parties to be rational.\nComputation and Communication Complexity. Our RIG can be implemented using a\ncommit-reveal scheme or a PVSS scheme, which is typical in random number genera-"
        }
    }
]
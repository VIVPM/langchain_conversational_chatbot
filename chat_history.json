[{"role": "user", "content": "how many trainable parameters are used in training deepseek model?"}, {"role": "assistant", "content": "**Answer (from web search):**\nAccording to the search results, the DeepSeek-V3 model has a total of 671 billion parameters, with 37 billion parameters actively engaged during inference. However, the number of trainable parameters used in training is not explicitly stated as 671 billion. \n\nA related paper on arxiv.org mentions training a baseline MoE model with 228.7B total parameters, but this is not the DeepSeek-V3 model. The DeepSeek-V3 model's training details are mentioned in a report on wandb.ai, which states that the model was trained on a massive dataset of 14.8 trillion tokens, but it does not explicitly mention the number of trainable parameters.\n\nTherefore, based on the available information, it can be inferred that the total number of parameters in the DeepSeek-V3 model is 671 billion, but the exact number of trainable parameters used in training is not specified. \n\nKey sources: \n- https://wandb.ai/byyoung3/ml-news/reports/DeepSeek-V3-Training-671-billion-parameters-with-a-6-million-dollar-budget--VmlldzoxMDczNTI2Ng\n- https://arxiv.org/html/2412.19437v1\n- https://huggingface.co/deepseek-ai/DeepSeek-V3\n\n**Web Sources used:**\n- https://wandb.ai/byyoung3/ml-news/reports/DeepSeek-V3-Training-671-billion-parameters-with-a-6-million-dollar-budget--VmlldzoxMDczNTI2Ng\n- https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1\n- https://huggingface.co/deepseek-ai/DeepSeek-V3\n- https://arxiv.org/html/2412.19437v1\n- https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond"}]